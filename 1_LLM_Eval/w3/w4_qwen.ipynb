{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "479420e3",
      "metadata": {
        "id": "479420e3"
      },
      "source": [
        "# Week 4\n",
        "**Deliverables**\n",
        "* Refine prompts for Binary & Multitype"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1138d41b",
      "metadata": {
        "id": "1138d41b"
      },
      "source": [
        "## 1. Prompt Library\n",
        "Binary and multitype templates that feed every evaluation routine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1ce217a3",
      "metadata": {
        "id": "1ce217a3"
      },
      "outputs": [],
      "source": [
        "# BINARY PROMPTS\n",
        "# (1) Zero-Shot — final (no inline JSON examples)\n",
        "prompt_binary_zeroshot = \"\"\"\n",
        "You are analyzing whether a sentence describes Intimate Partner Violence (IPV).\n",
        "\n",
        "Definition:\n",
        "Intimate Partner Violence includes any form of physical, emotional, or sexual harm\n",
        "inflicted by a partner. This may involve control, manipulation, threats, humiliation,\n",
        "or physical aggression.\n",
        "\n",
        "Decide if the following sentence involves IPV or not.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: a single key \"label\" with value \"IPV\" or \"NOT_IPV\".\n",
        "Print nothing else.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (2) Few-Shot — final (no inline JSON examples)\n",
        "prompt_binary_fewshot = \"\"\"\n",
        "You are classifying sentences as describing Intimate Partner Violence (IPV) or NOT_IPV.\n",
        "\n",
        "Definition:\n",
        "IPV occurs when one partner harms or controls another physically, emotionally, or sexually.\n",
        "\n",
        "Here are examples (for guidance only):\n",
        "1. \"He pushed me against the wall and shouted that I’m worthless.\" → IPV\n",
        "2. \"She often insults me and forbids me from seeing my friends.\" → IPV\n",
        "3. \"We argued but both apologized and moved on peacefully.\" → NOT_IPV\n",
        "4. \"He surprised me with flowers after work.\" → NOT_IPV\n",
        "\n",
        "Now analyze this sentence:\n",
        "\"{text}\"\n",
        "\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (3) Chain-of-Thought — final (no inline JSON examples)\n",
        "prompt_binary_cot = \"\"\"\n",
        "You are reasoning step-by-step to decide if a sentence describes Intimate Partner Violence (IPV).\n",
        "\n",
        "Guidelines:\n",
        "- IPV includes threats, coercion, physical harm, humiliation, or emotional manipulation.\n",
        "- NOT_IPV describes healthy, neutral, or unrelated situations.\n",
        "\n",
        "Think internally (do NOT show your reasoning) about:\n",
        "1. Does the sentence show any behavior that causes harm, fear, or control?\n",
        "2. Is there a partner/relationship context?\n",
        "3. Does it express affection or support instead of harm?\n",
        "\n",
        "After thinking silently, return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (4) Meta — final (no inline JSON examples)\n",
        "prompt_binary_meta = \"\"\"\n",
        "You are a social-behavioral analyst evaluating sentences for signs of Intimate Partner Violence (IPV).\n",
        "\n",
        "Your objective is to be accurate but cautious.\n",
        "- If the sentence clearly involves harm, coercion, or control → label as IPV.\n",
        "- If the sentence shows affection, neutrality, or uncertainty → label as NOT_IPV.\n",
        "\n",
        "Reflect internally before answering; do NOT print your reasoning.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (5) Self-Consistency — final (no inline JSON examples)\n",
        "prompt_binary_selfconsistency = \"\"\"\n",
        "You will internally evaluate the sentence for Intimate Partner Violence (IPV) multiple times\n",
        "and choose the majority label as your final answer.\n",
        "\n",
        "Guidelines:\n",
        "- IPV → signs of physical, emotional, or sexual harm, threats, or coercion.\n",
        "- NOT_IPV → supportive, neutral, or unrelated content.\n",
        "\n",
        "Do NOT reveal thoughts or votes.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1210e000",
      "metadata": {
        "id": "1210e000"
      },
      "source": [
        "### 1.2 Multitype Prompts\n",
        "These templates mirror the binary ones but collect subtype labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c09576f0",
      "metadata": {
        "id": "c09576f0"
      },
      "outputs": [],
      "source": [
        "# MULTITYPE PROMPTS\n",
        "prompt_multilabel_zeroshot = \"\"\"\n",
        "You are identifying which forms of Intimate Partner Violence (IPV) appear in a sentence.\n",
        "\n",
        "Decide independently for emotional, physical, and sexual abuse.\n",
        "Return ONLY one JSON object enclosed between <json> and </json> with the keys\n",
        "'id', 'emotional', 'physical', and 'sexual'.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_fewshot = \"\"\"\n",
        "You are labeling sentences for types of Intimate Partner Violence (IPV).\n",
        "Use the examples to stay calibrated. For each category (emotional, physical, sexual), output 1 if it is clearly present, else 0.\n",
        "\n",
        "Examples:\n",
        "1. \"He insults me daily and forbids me from leaving the house.\" -> emotional: 1, physical: 0, sexual: 0\n",
        "2. \"She slapped me when I disagreed with her.\" -> emotional: 0, physical: 1, sexual: 0\n",
        "3. \"They pressured me into intimacy when I said no.\" -> emotional: 0, physical: 0, sexual: 1\n",
        "4. \"We spent the evening cooking together peacefully.\" -> emotional: 0, physical: 0, sexual: 0\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_cot = \"\"\"\n",
        "Reason silently about whether emotional, physical, or sexual IPV occurs in the sentence.\n",
        "Use relationship context, threats, coercion, and bodily harm cues.\n",
        "After your hidden reasoning, output ONLY the JSON block specified below.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_meta = \"\"\"\n",
        "Act as a cautious social-behavioral analyst.\n",
        "Label a subtype as 1 only when the text clearly shows that form of IPV; otherwise return 0.\n",
        "Favor precision to avoid false positives.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_selfconsistency = \"\"\"\n",
        "Independently evaluate the sentence multiple times to reduce uncertainty.\n",
        "After internal self-consistency voting, output the majority decision for each subtype in the JSON schema below.\n",
        "Do not reveal the intermediate thoughts.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb692028",
      "metadata": {
        "id": "cb692028",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 2. System & Model Setup\n",
        "Shared imports, paths, and model objects. Execute once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "84f65f46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "fd31b3a79961493f8e6de35f86a2a896",
            "774696fc0e09452aa74fd1d7ecc0e631",
            "2647645796aa41a6b0fa63dddd9a39da",
            "f937ee2499214f7592fe534d5b07900e",
            "71c2f44a3500435d93ff924e76e12b01",
            "9569bbc1c1d04d4e9666363c097a5e83",
            "c1f39be0116d4a39a6b3f1dbb34899ff",
            "35243a2b7e944b4cb96ef74b18caf2ea",
            "71a90bfa6ad640a9b08c988bb1e1b474",
            "d50b5ea7641149fea1b356e7eb0868b7",
            "4612038745274cc1a09637108b08680d"
          ]
        },
        "id": "84f65f46",
        "outputId": "0756f068-dc75-42ee-9abe-4ea87e376e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Qwen/Qwen2.5-7B-Instruct ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd31b3a79961493f8e6de35f86a2a896",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DATASET_PATH = Path(\"../Dataset/reddut_data.csv\")\n",
        "RESULTS_ROOT = Path(\"test_results_raw\")\n",
        "BINARY_RESULTS_DIR = RESULTS_ROOT / \"binary\"\n",
        "MULTITYPE_RESULTS_DIR = RESULTS_ROOT / \"multitype\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME} ...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Model ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251e43c8",
      "metadata": {
        "id": "251e43c8"
      },
      "source": [
        "## 3. Prediction Generation\n",
        "Run the Colab cloning cell first, then load the dataset and choose either binary or multitype generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b18ed7b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b18ed7b4",
        "outputId": "8aed309e-20e2-4e46-eb55-47017eb50771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ipvresearch25'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (174/174), done.\u001b[K\n",
            "remote: Total 225 (delta 110), reused 148 (delta 51), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (225/225), 7.94 MiB | 39.07 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "/content/ipvresearch25/1_LLM_Eval\n"
          ]
        }
      ],
      "source": [
        "#Clone from git\n",
        "!git clone https://github.com/zelaneroz/ipvresearch25\n",
        "%cd ipvresearch25/1_LLM_Eval\n",
        "#Load dataset\n",
        "filename = \"../Dataset/reddit_data.csv\"\n",
        "df = pd.read_csv(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3947348",
      "metadata": {
        "id": "e3947348"
      },
      "source": [
        "### 3.0 Data Access & Directories\n",
        "Loads the dataset and prepares local folders for saving model outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2e8905ea",
      "metadata": {
        "id": "2e8905ea"
      },
      "outputs": [],
      "source": [
        "# Dataset + result folders\n",
        "BINARY_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MULTITYPE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# if not filename.exists():\n",
        "#     raise FileNotFoundError(f\"Dataset not found at {filename}. Run the git clone cell above or update DATASET_PATH.\")\n",
        "\n",
        "# df = pd.read_csv(filename)\n",
        "# print(f\"Loaded {len(df)} rows from {filename}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c36d68b9",
      "metadata": {
        "id": "c36d68b9"
      },
      "outputs": [],
      "source": [
        "# Prompt registries for downstream loops\n",
        "binary_prompts = {\n",
        "    \"zeroshot\": prompt_binary_zeroshot,\n",
        "    \"fewshot\": prompt_binary_fewshot,\n",
        "    \"cot\": prompt_binary_cot,\n",
        "    \"meta\": prompt_binary_meta,\n",
        "    \"selfconsistency\": prompt_binary_selfconsistency,\n",
        "}\n",
        "\n",
        "multilabel_prompts = {\n",
        "    \"zeroshot\": prompt_multilabel_zeroshot,\n",
        "    \"fewshot\": prompt_multilabel_fewshot,\n",
        "    \"cot\": prompt_multilabel_cot,\n",
        "    \"meta\": prompt_multilabel_meta,\n",
        "    \"selfconsistency\": prompt_multilabel_selfconsistency,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ba5ebf",
      "metadata": {
        "id": "f4ba5ebf"
      },
      "source": [
        "### 3.1 Binary Prediction Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1700430e",
      "metadata": {
        "id": "1700430e"
      },
      "outputs": [],
      "source": [
        "def test_binary_prompts(df: pd.DataFrame, path: Path, n_samples: int = 3) -> None:\n",
        "    \"\"\"Run all binary prompt types on the first `n_samples` rows and persist outputs.\"\"\"\n",
        "    import re\n",
        "\n",
        "    df_subset = df.head(n_samples)\n",
        "    results_dir = Path(path)\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Running binary classification tests...\")\n",
        "    print(f\"Number of samples: {len(df_subset)}\")\n",
        "    print(f\"Results will be saved in: {results_dir}\")\n",
        "\n",
        "    for prompt_type, template in binary_prompts.items():\n",
        "        print(f\"Testing prompt type: {prompt_type}\")\n",
        "        records = []\n",
        "\n",
        "        for i, row in df_subset.iterrows():\n",
        "            text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
        "            prompt_text = template.replace(\"{text}\", text)\n",
        "\n",
        "            try:\n",
        "                inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "                output = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=128,\n",
        "                    temperature=0.0,\n",
        "                    do_sample=False,\n",
        "                )\n",
        "                gen_tokens = output[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "                result_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
        "            except Exception as exc:\n",
        "                result_text = f\"ERROR: {exc}\"\n",
        "\n",
        "            label = None\n",
        "            match = re.search(r\"<json[^>]*>\\s*(.*?)\\s*</json>\", result_text, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                block = match.group(1).strip()\n",
        "                try:\n",
        "                    parsed = json.loads(block)\n",
        "                    if isinstance(parsed, dict):\n",
        "                        label = parsed.get(\"label\") or parsed.get(\"labels\")\n",
        "                    elif isinstance(parsed, list) and parsed:\n",
        "                        label = parsed[0]\n",
        "                    elif isinstance(parsed, str):\n",
        "                        label = parsed.strip()\n",
        "                except json.JSONDecodeError:\n",
        "                    if \"NOT_IPV\" in block.upper():\n",
        "                        label = \"NOT_IPV\"\n",
        "                    elif \"IPV\" in block.upper():\n",
        "                        label = \"IPV\"\n",
        "            else:\n",
        "                if \"NOT_IPV\" in result_text.upper():\n",
        "                    label = \"NOT_IPV\"\n",
        "                elif \"IPV\" in result_text.upper():\n",
        "                    label = \"IPV\"\n",
        "\n",
        "            if label is None:\n",
        "                label = \"UNKNOWN\"\n",
        "\n",
        "            records.append(\n",
        "                {\n",
        "                    \"id\": int(i),\n",
        "                    \"prompt_type\": prompt_type,\n",
        "                    \"extracted_label\": label,\n",
        "                    \"raw_response\": result_text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        output_path = results_dir / f\"binary_{prompt_type}.json\"\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "            json.dump(records, fp, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved results for '{prompt_type}' to {output_path}\")\n",
        "\n",
        "    print(\"All binary prompt tests completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3f75e5e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f75e5e9",
        "outputId": "341bc90b-a58a-44d7-8072-712c0b4d27e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary generation skipped. Set RUN_BINARY = True to execute.\n"
          ]
        }
      ],
      "source": [
        "# Binary generation runner (toggle RUN_BINARY to execute)\n",
        "RUN_BINARY = False\n",
        "BINARY_SAMPLE_COUNT = 5\n",
        "binary_run_dir = BINARY_RESULTS_DIR / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "if RUN_BINARY:\n",
        "    if 'df' not in globals():\n",
        "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
        "    test_binary_prompts(df, path=binary_run_dir, n_samples=BINARY_SAMPLE_COUNT)\n",
        "else:\n",
        "    print(\"Binary generation skipped. Set RUN_BINARY = True to execute.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4db5db",
      "metadata": {
        "id": "8e4db5db"
      },
      "source": [
        "### 3.2 Multitype Prediction Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "528df897",
      "metadata": {
        "id": "528df897"
      },
      "outputs": [],
      "source": [
        "# ---------- Stage 1: Classification ----------\n",
        "def multitype_predict(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, int]:\n",
        "    template = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot)\n",
        "    prompt = template.format(text=sentence, sample_id=sample_id or 0)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
        "    try:\n",
        "        return json.loads(decoded)\n",
        "    except Exception:\n",
        "        import re\n",
        "        match = re.search(r\"\\{.*\\}\", decoded, re.DOTALL)\n",
        "        return json.loads(match.group()) if match else {\"emotional\": 0, \"physical\": 0, \"sexual\": 0}\n",
        "\n",
        "\n",
        "# ---------- Stage 2: Confidence ----------\n",
        "def logprob_confidence(prompt: str, generated_text: str) -> float:\n",
        "    tokens = tokenizer(prompt + generated_text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
        "    input_ids = tokens[\"input_ids\"][0]\n",
        "\n",
        "    logp_sum = 0.0\n",
        "    count = 0\n",
        "    for idx in range(1, len(input_ids)):\n",
        "        token_id = input_ids[idx]\n",
        "        logp_sum += log_probs[0, idx - 1, token_id].item()\n",
        "        count += 1\n",
        "    avg_logp = logp_sum / max(1, count)\n",
        "    confidence = math.exp(avg_logp)\n",
        "    return float(max(0.0, min(1.0, confidence)))\n",
        "\n",
        "\n",
        "# ---------- Combined Function ----------\n",
        "def multitype_classify(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, float]:\n",
        "    pred = multitype_predict(sentence, sample_id=sample_id, prompt_key=prompt_key)\n",
        "    classification_prompt = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot).format(\n",
        "        text=sentence,\n",
        "        sample_id=sample_id or 0,\n",
        "    )\n",
        "\n",
        "    output_str = json.dumps(pred)\n",
        "    conf = logprob_confidence(classification_prompt, output_str)\n",
        "    pred[\"confidence_score\"] = round(conf, 4)\n",
        "    pred[\"id\"] = sample_id or 0\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "042f8c45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "042f8c45",
        "outputId": "cb67ee27-f5b8-4834-e8dc-e31900a6e33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multitype generation skipped. Set RUN_MULTITPE = True to execute.\n"
          ]
        }
      ],
      "source": [
        "# Multitype generation runner (toggle RUN_MULTITYPE to execute)\n",
        "RUN_MULTITYPE = False\n",
        "MULTITYPE_SAMPLE_COUNT = 5\n",
        "MULTITYPE_PROMPT_KEY = \"zeroshot\"\n",
        "multitype_outputs = []\n",
        "\n",
        "if RUN_MULTITYPE:\n",
        "    if 'df' not in globals():\n",
        "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
        "    subset = df.head(MULTITYPE_SAMPLE_COUNT)\n",
        "    for idx, row in subset.iterrows():\n",
        "        text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
        "        multitype_outputs.append(\n",
        "            multitype_classify(text, sample_id=int(idx), prompt_key=MULTITYPE_PROMPT_KEY)\n",
        "        )\n",
        "    print(f\"Generated {len(multitype_outputs)} multitype predictions using '{MULTITYPE_PROMPT_KEY}'.\")\n",
        "else:\n",
        "    print(\"Multitype generation skipped. Set RUN_MULTITPE = True to execute.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61de86ab",
      "metadata": {
        "id": "61de86ab"
      },
      "source": [
        "## 4. Results\n",
        "Summaries, metrics, and visual diagnostics. Load the JSON artifacts generated above and feed them into the eval pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "NXIFY8_iKQua",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXIFY8_iKQua",
        "outputId": "cb29bae0-e28b-4399-bfb8-e376926616fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running binary classification tests...\n",
            "Number of samples: 618\n",
            "Results will be saved in: 1_LLM_Eval/test_results_raw/w4\n",
            "Testing prompt type: zeroshot\n",
            "Saved results for 'zeroshot' to 1_LLM_Eval/test_results_raw/w4/binary_zeroshot.json\n",
            "Testing prompt type: fewshot\n",
            "Saved results for 'fewshot' to 1_LLM_Eval/test_results_raw/w4/binary_fewshot.json\n",
            "Testing prompt type: cot\n",
            "Saved results for 'cot' to 1_LLM_Eval/test_results_raw/w4/binary_cot.json\n",
            "Testing prompt type: meta\n",
            "Saved results for 'meta' to 1_LLM_Eval/test_results_raw/w4/binary_meta.json\n",
            "Testing prompt type: selfconsistency\n",
            "Saved results for 'selfconsistency' to 1_LLM_Eval/test_results_raw/w4/binary_selfconsistency.json\n",
            "All binary prompt tests completed.\n"
          ]
        }
      ],
      "source": [
        "test_binary_prompts(df=df,path=\"1_LLM_Eval/test_results_raw/w4\",n_samples=618)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "CZuVdnbfKQln",
      "metadata": {
        "id": "CZuVdnbfKQln"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ecff4bbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecff4bbc",
        "outputId": "6c7360b6-75fb-40ae-fe7f-9a8b131136b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recent binary result files:\n",
            "No files found under test_results_raw/binary\n",
            "\n",
            "Recent multitype result files:\n",
            "No files found under test_results_raw/multitype\n"
          ]
        }
      ],
      "source": [
        "# Utility: list recently generated result files\n",
        "from glob import glob\n",
        "\n",
        "def list_result_files(root: Path, pattern: str = \"*.json\", limit: int = 10):\n",
        "    files = sorted(root.rglob(pattern))[-limit:]\n",
        "    if not files:\n",
        "        print(f\"No files found under {root}\")\n",
        "        return\n",
        "    for file in files:\n",
        "        print(f\"- {file.relative_to(Path.cwd())}\")\n",
        "\n",
        "print(\"Recent binary result files:\")\n",
        "list_result_files(BINARY_RESULTS_DIR)\n",
        "print(\"\\nRecent multitype result files:\")\n",
        "list_result_files(MULTITYPE_RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84c3a4d6",
      "metadata": {
        "id": "84c3a4d6"
      },
      "source": [
        "### 4.1 Import Evaluation Functions\n",
        "Import the evaluation and visualization functions from eval_llm_pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "77bfedea",
      "metadata": {
        "id": "77bfedea"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'eval_llm_pipeline'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meval_llm_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     compute_binary_metrics_detailed,\n\u001b[1;32m      4\u001b[0m     compute_multitype_metrics_per_subgroup,\n\u001b[1;32m      5\u001b[0m     plot_confusion_matrix,\n\u001b[1;32m      6\u001b[0m     plot_roc_curve_binary,\n\u001b[1;32m      7\u001b[0m     plot_precision_recall_curve_binary,\n\u001b[1;32m      8\u001b[0m     plot_per_class_f1_bar_chart,\n\u001b[1;32m      9\u001b[0m     append_binary_results_to_json,\n\u001b[1;32m     10\u001b[0m     append_multitype_results_to_json,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Paths for JSON results\u001b[39;00m\n\u001b[1;32m     14\u001b[0m BINARY_JSON_PATH \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/binary_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eval_llm_pipeline'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from eval_llm_pipeline import (\n",
        "    compute_binary_metrics_detailed,\n",
        "    compute_multitype_metrics_per_subgroup,\n",
        "    plot_confusion_matrix,\n",
        "    plot_roc_curve_binary,\n",
        "    plot_precision_recall_curve_binary,\n",
        "    plot_per_class_f1_bar_chart,\n",
        "    append_binary_results_to_json,\n",
        "    append_multitype_results_to_json,\n",
        ")\n",
        "\n",
        "# Paths for JSON results\n",
        "BINARY_JSON_PATH = Path(\"results/binary_results.json\")\n",
        "MULTITYPE_JSON_PATH = Path(\"results/multitype_results.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "368e5d67",
      "metadata": {
        "id": "368e5d67"
      },
      "source": [
        "### 4.2 Binary Evaluation\n",
        "Evaluate binary predictions with detailed metrics and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5cK5Z-cnY4sN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cK5Z-cnY4sN",
        "outputId": "7b843cb8-6720-4e0b-d8d7-ce5e5caa7ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found files: ['binary_cot.json', 'binary_fewshot.json', 'binary_meta.json', 'binary_selfconsistency.json', 'binary_zeroshot.json']\n",
            "\n",
            "=== Evaluating binary_cot.json (cot) ===\n",
            "{'Accuracy': 0.8058252427184466, 'F1': 0.7818181818181819, 'ROC_AUC': 0.8143436423228843, 'PR_AUC': 0.812622715340827}\n",
            "Saved plot → test_results_raw/w4/cot_eval.png\n",
            "\n",
            "=== Evaluating binary_fewshot.json (fewshot) ===\n",
            "{'Accuracy': 0.8932038834951457, 'F1': 0.89, 'ROC_AUC': 0.8975229010841247, 'PR_AUC': 0.8935638327087934}\n",
            "Saved plot → test_results_raw/w4/fewshot_eval.png\n",
            "\n",
            "=== Evaluating binary_meta.json (meta) ===\n",
            "{'Accuracy': 0.8527508090614887, 'F1': 0.8428324697754751, 'ROC_AUC': 0.8588221699302463, 'PR_AUC': 0.8545270599177531}\n",
            "Saved plot → test_results_raw/w4/meta_eval.png\n",
            "\n",
            "=== Evaluating binary_selfconsistency.json (selfconsistency) ===\n",
            "{'Accuracy': 0.8559870550161812, 'F1': 0.8462867012089809, 'ROC_AUC': 0.8620682410286579, 'PR_AUC': 0.8588377906953475}\n",
            "Saved plot → test_results_raw/w4/selfconsistency_eval.png\n",
            "\n",
            "=== Evaluating binary_zeroshot.json (zeroshot) ===\n",
            "{'Accuracy': 0.7281553398058253, 'F1': 0.6557377049180327, 'ROC_AUC': 0.7419741154718884, 'PR_AUC': 0.7533467312886377}\n",
            "Saved plot → test_results_raw/w4/zeroshot_eval.png\n",
            "\n",
            "All results saved to: results/binary_results.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Paths\n",
        "# =====================================\n",
        "GT_PATH = \"../Dataset/reddit_data.csv\"\n",
        "PLOT_DIR=RESULTS_DIR = \"test_results_raw/w4\"\n",
        "BINARY_RESULTS_JSON = \"results/binary_results.json\"\n",
        "os.makedirs(PLOT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"qwen2.5-7b\"\n",
        "\n",
        "# =====================================\n",
        "# Helper Functions\n",
        "# =====================================\n",
        "def compute_binary_metrics_detailed(y_true, y_pred):\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"ROC_AUC\": roc_auc_score(y_true, y_pred),\n",
        "        \"PR_AUC\": average_precision_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def append_binary_results_to_json(json_path, model_name, prompt_version, metrics, notes=\"\"):\n",
        "    now_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    entry = {\n",
        "        \"model_name\": model_name,\n",
        "        \"prompt_version\": prompt_version,\n",
        "        \"metrics\": metrics,\n",
        "        \"notes\": notes,\n",
        "        \"date_tested\": now_str,\n",
        "    }\n",
        "\n",
        "    if Path(json_path).exists():\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    else:\n",
        "        data = []\n",
        "\n",
        "    data.append(entry)\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, ax, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    im = ax.imshow(cm, cmap=\"Blues\")\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "\n",
        "    ax.set_xticks([0,1])\n",
        "    ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels([\"NOT_IPV\", \"IPV\"])\n",
        "    ax.set_yticklabels([\"NOT_IPV\", \"IPV\"])\n",
        "\n",
        "    for (i,j), val in np.ndenumerate(cm):\n",
        "        ax.text(j, i, val, ha=\"center\", va=\"center\")\n",
        "\n",
        "\n",
        "def plot_roc_curve_binary(y_true, y_pred, ax, label):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "    ax.plot(fpr, tpr, lw=2, label=label)\n",
        "    ax.plot([0,1], [0,1], \"--\", color=\"gray\")\n",
        "    ax.set_title(\"ROC Curve\")\n",
        "    ax.set_xlabel(\"False Positive Rate\")\n",
        "    ax.set_ylabel(\"True Positive Rate\")\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "def plot_precision_recall_curve_binary(y_true, y_pred, ax, label):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "    ax.plot(recall, precision, lw=2, label=label)\n",
        "    ax.set_title(\"Precision-Recall Curve\")\n",
        "    ax.set_xlabel(\"Recall\")\n",
        "    ax.set_ylabel(\"Precision\")\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Load Ground Truth\n",
        "# =====================================\n",
        "df_gt = pd.read_csv(GT_PATH)\n",
        "df_gt[\"label_true\"] = (\n",
        "    df_gt[[\"Physical Abuse\", \"Emotional Abuse\", \"Sexual Abuse\"]].any(axis=1)\n",
        ").astype(int)\n",
        "df_gt = df_gt.reset_index().rename(columns={\"index\": \"id\"})\n",
        "\n",
        "# =====================================\n",
        "# Process ALL binary JSON files\n",
        "# =====================================\n",
        "binary_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.startswith(\"binary_\") and f.endswith(\".json\")])\n",
        "print(\"Found files:\", binary_files)\n",
        "\n",
        "for file in binary_files:\n",
        "    json_path = os.path.join(RESULTS_DIR, file)\n",
        "\n",
        "    # Extract prompt_type properly\n",
        "    prompt_type = file.replace(\"binary_\", \"\").replace(\".json\", \"\")\n",
        "\n",
        "    print(f\"\\n=== Evaluating {file} ({prompt_type}) ===\")\n",
        "\n",
        "    # Load predictions\n",
        "    with open(json_path, \"r\") as f:\n",
        "        preds = json.load(f)\n",
        "    df_pred = pd.DataFrame(preds)\n",
        "\n",
        "    if \"id\" not in df_pred:\n",
        "        df_pred = df_pred.reset_index().rename(columns={\"index\": \"id\"})\n",
        "\n",
        "    merged = df_gt.merge(df_pred, on=\"id\", how=\"inner\")\n",
        "\n",
        "    merged[\"y_true\"] = merged[\"label_true\"]\n",
        "    merged[\"y_pred\"] = merged[\"extracted_label\"].str.upper().eq(\"IPV\").astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_binary_metrics_detailed(merged[\"y_true\"], merged[\"y_pred\"])\n",
        "    print(metrics)\n",
        "\n",
        "    # Save results (append)\n",
        "    append_binary_results_to_json(\n",
        "        json_path=BINARY_RESULTS_JSON,\n",
        "        model_name=MODEL_NAME,\n",
        "        prompt_version=prompt_type,\n",
        "        metrics=metrics,\n",
        "        notes=f\"Evaluation of {prompt_type} prompt\"\n",
        "    )\n",
        "\n",
        "    # ===== Plotting =====\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    plot_confusion_matrix(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[0],\n",
        "                          title=f\"{prompt_type} — Confusion Matrix\")\n",
        "\n",
        "    plot_roc_curve_binary(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[1],\n",
        "                          label=prompt_type)\n",
        "\n",
        "    plot_precision_recall_curve_binary(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[2],\n",
        "                                       label=prompt_type)\n",
        "\n",
        "    plt.suptitle(f\"Binary Evaluation — {prompt_type}\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    save_path = os.path.join(PLOT_DIR, f\"{prompt_type}_eval.png\")\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Saved plot → {save_path}\")\n",
        "\n",
        "\n",
        "print(\"\\nAll results saved to:\", BINARY_RESULTS_JSON)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dddb680a",
      "metadata": {
        "id": "dddb680a"
      },
      "source": [
        "### 4.3 Multitype Evaluation\n",
        "Evaluate multitype predictions per subgroup with detailed metrics and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8d2e4302",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d2e4302",
        "outputId": "75d88f1f-eede-4689-a8e4-fbc2a21c3547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set MULTITYPE_RESULT_FILE to a valid path to run multitype evaluation.\n"
          ]
        }
      ],
      "source": [
        "# Example: Load multitype predictions and ground truth\n",
        "# Replace with your actual data loading logic\n",
        "# This assumes you have:\n",
        "# - predictions_df: DataFrame with 'id', 'emotional', 'physical', 'sexual' columns\n",
        "# - ground_truth_df: DataFrame with 'id', 'Physical', 'Emotional', 'Sexual' columns\n",
        "# - Optional: subgroup column (e.g., 'gender', 'age_group')\n",
        "\n",
        "# Example setup (modify based on your actual data structure):\n",
        "MULTITYPE_RESULT_FILE = None  # Set to your multitype result JSON file path\n",
        "MULTITYPE_PROMPT_TYPE = \"zeroshot\"  # Set to the prompt type you want to evaluate\n",
        "MULTITYPE_MODEL_NAME = \"qwen2.5-7b\"\n",
        "MULTITYPE_PROMPT_VERSION = MULTITYPE_PROMPT_TYPE\n",
        "\n",
        "# If you have a result file, load it:\n",
        "if MULTITYPE_RESULT_FILE and Path(MULTITYPE_RESULT_FILE).exists():\n",
        "    with open(MULTITYPE_RESULT_FILE, 'r') as f:\n",
        "        predictions_data = json.load(f)\n",
        "    predictions_df = pd.DataFrame(predictions_data)\n",
        "\n",
        "    # Merge with ground truth\n",
        "    # Assuming df has 'Physical', 'Emotional', 'Sexual' columns (or 'Physical Abuse', etc.)\n",
        "    merged_df = df.merge(predictions_df, on='id', how='inner')\n",
        "\n",
        "    # Map column names if needed\n",
        "    true_cols = []\n",
        "    pred_cols = []\n",
        "    for label in ['Physical', 'Emotional', 'Sexual']:\n",
        "        # Try different possible column names\n",
        "        true_col = None\n",
        "        for col in [label, f\"{label} Abuse\"]:\n",
        "            if col in merged_df.columns:\n",
        "                true_col = col\n",
        "                break\n",
        "        if true_col:\n",
        "            true_cols.append(true_col)\n",
        "            # Prediction columns might be lowercase\n",
        "            pred_col = label.lower() if label.lower() in merged_df.columns else label\n",
        "            if pred_col in merged_df.columns:\n",
        "                pred_cols.append(pred_col)\n",
        "            else:\n",
        "                pred_cols.append(None)\n",
        "\n",
        "    # Filter out None columns\n",
        "    valid_pairs = [(t, p) for t, p in zip(true_cols, pred_cols) if p is not None]\n",
        "    if valid_pairs:\n",
        "        true_cols, pred_cols = zip(*valid_pairs)\n",
        "        true_cols, pred_cols = list(true_cols), list(pred_cols)\n",
        "\n",
        "        # Optional: subgroup column (e.g., 'gender', 'age_group')\n",
        "        subgroup_col = None  # Set to column name if you want per-subgroup metrics\n",
        "\n",
        "        # Compute multitype metrics per subgroup\n",
        "        multitype_metrics = compute_multitype_metrics_per_subgroup(\n",
        "            df=merged_df,\n",
        "            y_true_cols=true_cols,\n",
        "            y_pred_cols=pred_cols,\n",
        "            subgroup_col=subgroup_col\n",
        "        )\n",
        "\n",
        "        # Display metrics table\n",
        "        print(\"=== Multitype Classification Metrics (Per Subgroup) ===\")\n",
        "        for subgroup, classes in multitype_metrics.items():\n",
        "            print(f\"\\n--- Subgroup: {subgroup} ---\")\n",
        "            metrics_list = []\n",
        "            for class_name, metrics in classes.items():\n",
        "                row = {'Class': class_name, **metrics}\n",
        "                metrics_list.append(row)\n",
        "            metrics_table = pd.DataFrame(metrics_list)\n",
        "            print(metrics_table.to_string(index=False))\n",
        "\n",
        "        # Visualizations\n",
        "        # Get overall metrics for visualization (or first subgroup)\n",
        "        overall_key = \"overall\" if \"overall\" in multitype_metrics else list(multitype_metrics.keys())[0]\n",
        "        overall_metrics = multitype_metrics[overall_key]\n",
        "\n",
        "        # Create figure with subplots for confusion matrices\n",
        "        n_classes = len(overall_metrics)\n",
        "        fig, axes = plt.subplots(1, n_classes, figsize=(6 * n_classes, 5))\n",
        "        if n_classes == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        # Confusion matrices for each class\n",
        "        for idx, (class_name, class_metrics) in enumerate(overall_metrics.items()):\n",
        "            # Get true and predicted for this class\n",
        "            y_true_class = merged_df[true_cols[idx]].astype(int).values\n",
        "            y_pred_class = merged_df[pred_cols[idx]].astype(int).values\n",
        "\n",
        "            plot_confusion_matrix(\n",
        "                y_true_class, y_pred_class,\n",
        "                ax=axes[idx],\n",
        "                title=f\"Confusion Matrix - {class_name}\",\n",
        "                xlabel=\"Predicted\",\n",
        "                ylabel=\"Actual\",\n",
        "                class_names=[\"Not Present\", \"Present\"]\n",
        "            )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Per-Class F1 Bar Chart (separate figure)\n",
        "        fig_f1, ax_f1 = plt.subplots(figsize=(8, 5))\n",
        "        plot_per_class_f1_bar_chart(\n",
        "            overall_metrics,\n",
        "            ax=ax_f1,\n",
        "            title=\"Per-Class F1 Scores (Multitype)\",\n",
        "            xlabel=\"Class\",\n",
        "            ylabel=\"F1 Score\"\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Append to JSON\n",
        "        append_multitype_results_to_json(\n",
        "            json_path=MULTITYPE_JSON_PATH,\n",
        "            model_name=MULTITYPE_MODEL_NAME,\n",
        "            prompt_version=MULTITYPE_PROMPT_VERSION,\n",
        "            metrics_per_subgroup=multitype_metrics,\n",
        "            notes=f\"Evaluation of {MULTITYPE_PROMPT_TYPE} prompt\"\n",
        "        )\n",
        "        print(f\"\\nResults appended to {MULTITYPE_JSON_PATH}\")\n",
        "    else:\n",
        "        print(\"Could not find matching true/prediction columns. Check column names.\")\n",
        "else:\n",
        "    print(\"Set MULTITYPE_RESULT_FILE to a valid path to run multitype evaluation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bz4AnhrMeVoC",
      "metadata": {
        "id": "bz4AnhrMeVoC"
      },
      "source": [
        "# SAVE TO GITHUB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pTYsmVH-f2IK",
      "metadata": {
        "id": "pTYsmVH-f2IK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2647645796aa41a6b0fa63dddd9a39da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35243a2b7e944b4cb96ef74b18caf2ea",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71a90bfa6ad640a9b08c988bb1e1b474",
            "value": 4
          }
        },
        "35243a2b7e944b4cb96ef74b18caf2ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4612038745274cc1a09637108b08680d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a90bfa6ad640a9b08c988bb1e1b474": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71c2f44a3500435d93ff924e76e12b01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774696fc0e09452aa74fd1d7ecc0e631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9569bbc1c1d04d4e9666363c097a5e83",
            "placeholder": "​",
            "style": "IPY_MODEL_c1f39be0116d4a39a6b3f1dbb34899ff",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9569bbc1c1d04d4e9666363c097a5e83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1f39be0116d4a39a6b3f1dbb34899ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d50b5ea7641149fea1b356e7eb0868b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f937ee2499214f7592fe534d5b07900e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50b5ea7641149fea1b356e7eb0868b7",
            "placeholder": "​",
            "style": "IPY_MODEL_4612038745274cc1a09637108b08680d",
            "value": " 4/4 [00:05&lt;00:00,  1.25s/it]"
          }
        },
        "fd31b3a79961493f8e6de35f86a2a896": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_774696fc0e09452aa74fd1d7ecc0e631",
              "IPY_MODEL_2647645796aa41a6b0fa63dddd9a39da",
              "IPY_MODEL_f937ee2499214f7592fe534d5b07900e"
            ],
            "layout": "IPY_MODEL_71c2f44a3500435d93ff924e76e12b01"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
