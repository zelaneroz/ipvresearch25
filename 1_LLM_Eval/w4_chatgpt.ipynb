{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6076ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "from eval_llm_pipeline import (\n",
    "    compute_binary_metrics_detailed,\n",
    "    compute_multitype_metrics_per_subgroup,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve_binary,\n",
    "    plot_precision_recall_curve_binary,\n",
    "    plot_per_class_f1_bar_chart,\n",
    "    append_binary_results_to_json,\n",
    "    append_multitype_results_to_json,\n",
    ")\n",
    "\n",
    "# Paths for JSON results\n",
    "BINARY_JSON_PATH = Path(\"results/binary_results.json\")\n",
    "MULTITYPE_JSON_PATH = Path(\"results/multitype_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584c6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['binary_fewshot.json', 'binary_zeroshot.json']\n",
      "\n",
      "=== Evaluating binary_fewshot.json (fewshot) ===\n",
      "{'Accuracy': 0.6456310679611651, 'F1': 0.5249457700650759, 'ROC_AUC': 0.6616102193461636, 'PR_AUC': 0.6643896025395817}\n",
      "Saved plot → w4/chatgpt/fewshot_eval.png\n",
      "\n",
      "=== Evaluating binary_zeroshot.json (zeroshot) ===\n",
      "{'Accuracy': 0.6456310679611651, 'F1': 0.5249457700650759, 'ROC_AUC': 0.6616102193461636, 'PR_AUC': 0.6643896025395817}\n",
      "Saved plot → w4/chatgpt/zeroshot_eval.png\n",
      "\n",
      "All results saved to: results/binary_results.json\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Paths\n",
    "# =====================================\n",
    "GT_PATH = \"../Dataset/reddit_data.csv\"\n",
    "PLOT_DIR=RESULTS_DIR = \"w4/chatgpt\"\n",
    "BINARY_RESULTS_JSON = \"results/binary_results.json\"\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"ChatGPT 5.1\"\n",
    "\n",
    "# =====================================\n",
    "# Helper Functions\n",
    "# =====================================\n",
    "def compute_binary_metrics_detailed(y_true, y_pred):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1\": f1_score(y_true, y_pred),\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, y_pred),\n",
    "        \"PR_AUC\": average_precision_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def append_binary_results_to_json(json_path, model_name, prompt_version, metrics, notes=\"\"):\n",
    "    now_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    entry = {\n",
    "        \"model_name\": model_name,\n",
    "        \"prompt_version\": prompt_version,\n",
    "        \"metrics\": metrics,\n",
    "        \"notes\": notes,\n",
    "        \"date_tested\": now_str,\n",
    "    }\n",
    "\n",
    "    if Path(json_path).exists():\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(entry)\n",
    "\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "\n",
    "    ax.set_xticks([0,1])\n",
    "    ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels([\"NOT_IPV\", \"IPV\"])\n",
    "    ax.set_yticklabels([\"NOT_IPV\", \"IPV\"])\n",
    "\n",
    "    for (i,j), val in np.ndenumerate(cm):\n",
    "        ax.text(j, i, val, ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "def plot_roc_curve_binary(y_true, y_pred, ax, label):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    ax.plot(fpr, tpr, lw=2, label=label)\n",
    "    ax.plot([0,1], [0,1], \"--\", color=\"gray\")\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve_binary(y_true, y_pred, ax, label):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    ax.plot(recall, precision, lw=2, label=label)\n",
    "    ax.set_title(\"Precision-Recall Curve\")\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Load Ground Truth\n",
    "# =====================================\n",
    "df_gt = pd.read_csv(GT_PATH)\n",
    "df_gt[\"label_true\"] = (\n",
    "    df_gt[[\"Physical Abuse\", \"Emotional Abuse\", \"Sexual Abuse\"]].any(axis=1)\n",
    ").astype(int)\n",
    "df_gt = df_gt.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "# =====================================\n",
    "# Process ALL binary JSON files\n",
    "# =====================================\n",
    "binary_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.startswith(\"binary_\") and f.endswith(\".json\")])\n",
    "print(\"Found files:\", binary_files)\n",
    "\n",
    "for file in binary_files:\n",
    "    json_path = os.path.join(RESULTS_DIR, file)\n",
    "\n",
    "    # Extract prompt_type properly\n",
    "    prompt_type = file.replace(\"binary_\", \"\").replace(\".json\", \"\")\n",
    "\n",
    "    print(f\"\\n=== Evaluating {file} ({prompt_type}) ===\")\n",
    "\n",
    "    # Load predictions\n",
    "    with open(json_path, \"r\") as f:\n",
    "        preds = json.load(f)\n",
    "    df_pred = pd.DataFrame(preds)\n",
    "\n",
    "    if \"id\" not in df_pred:\n",
    "        df_pred = df_pred.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "    merged = df_gt.merge(df_pred, on=\"id\", how=\"inner\")\n",
    "\n",
    "    merged[\"y_true\"] = merged[\"label_true\"]\n",
    "    merged[\"y_pred\"] = merged[\"extracted_label\"].str.upper().eq(\"IPV\").astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_binary_metrics_detailed(merged[\"y_true\"], merged[\"y_pred\"])\n",
    "    print(metrics)\n",
    "\n",
    "    # Save results (append)\n",
    "    append_binary_results_to_json(\n",
    "        json_path=BINARY_RESULTS_JSON,\n",
    "        model_name=MODEL_NAME,\n",
    "        prompt_version=prompt_type,\n",
    "        metrics=metrics,\n",
    "        notes=f\"Evaluation of {prompt_type} prompt\"\n",
    "    )\n",
    "\n",
    "    # ===== Plotting =====\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    plot_confusion_matrix(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[0],\n",
    "                          title=f\"{prompt_type} — Confusion Matrix\")\n",
    "\n",
    "    plot_roc_curve_binary(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[1],\n",
    "                          label=prompt_type)\n",
    "\n",
    "    plot_precision_recall_curve_binary(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[2],\n",
    "                                       label=prompt_type)\n",
    "\n",
    "    plt.suptitle(f\"Binary Evaluation — {prompt_type}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    save_path = os.path.join(PLOT_DIR, f\"{prompt_type}_eval.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved plot → {save_path}\")\n",
    "\n",
    "\n",
    "print(\"\\nAll results saved to:\", BINARY_RESULTS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543a712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
