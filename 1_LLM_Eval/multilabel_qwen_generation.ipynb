{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multilabel IPV Prediction Generation with Qwen\n",
        "\n",
        "This notebook generates multilabel predictions for IPV classification using Qwen model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Edit the settings below before running the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !git clone https://github.com/zelaneroz/ipvresearch25\n",
        "# %cd ipvresearch25/1_LLM_Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== CONFIGURATION - EDIT THESE SETTINGS ==========\n",
        "\n",
        "# Prompt template (use {text} for sentence and {sample_id} for ID)\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are identifying which forms of Intimate Partner Violence (IPV) appear in a sentence.\n",
        "\n",
        "Decide independently for emotional, physical, and sexual abuse. If it is a particular type of IPV, set emotional, physical, or sexual to 1, otherwise set it to 0. Multiple IPV types can be true or none at all.\n",
        "\n",
        "Return ONLY one JSON object enclosed between <json> and </json> with the keys 'id', 'emotional', 'physical', and 'sexual'.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "# Results directory (will be created if it doesn't exist)\n",
        "RESULTS_DIR = \"w4/qwen\"\n",
        "\n",
        "# Dataset path\n",
        "DATASET_PATH = \"../Dataset/reddit_data_fortesting.csv\"\n",
        "\n",
        "# Model name\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# Number of samples to process (set to None to process all)\n",
        "NUM_SAMPLES = None\n",
        "\n",
        "# Output filename (without extension)\n",
        "OUTPUT_FILENAME = \"multilabel_predictions\"\n",
        "\n",
        "# =========================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers torch accelerate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"Dataset loaded: {len(df)} rows\")\n",
        "\n",
        "# Limit to NUM_SAMPLES if specified\n",
        "if NUM_SAMPLES is not None:\n",
        "    df = df.head(NUM_SAMPLES)\n",
        "    print(f\"Processing {len(df)} samples\")\n",
        "\n",
        "# Display first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Qwen Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Model loaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "def extract_json_from_response(response: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Extract JSON from model response, handling various formats.\"\"\"\n",
        "    # Try to find JSON between <json> tags\n",
        "    match = re.search(r\"<json[^>]*>\\s*(.*?)\\s*</json>\", response, re.DOTALL | re.IGNORECASE)\n",
        "    if match:\n",
        "        json_str = match.group(1).strip()\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    \n",
        "    # Try to find any JSON object in the response\n",
        "    match = re.search(r\"\\{[^{}]*\\\"(id|emotional|physical|sexual)\\\"[^{}]*\\}\", response, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(0))\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    \n",
        "    return None\n",
        "\n",
        "def make_prediction(text: str, sample_id: int) -> Dict[str, Any]:\n",
        "    \"\"\"Make a prediction for a single text sample.\"\"\"\n",
        "    # Format the prompt\n",
        "    prompt = PROMPT_TEMPLATE.format(text=text, sample_id=sample_id)\n",
        "    \n",
        "    try:\n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                temperature=0.0,\n",
        "                do_sample=False,\n",
        "            )\n",
        "        \n",
        "        # Decode the response\n",
        "        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "        \n",
        "        # Extract JSON from response\n",
        "        prediction = extract_json_from_response(response)\n",
        "        \n",
        "        if prediction is None:\n",
        "            # Fallback: try to extract values from text\n",
        "            prediction = {\n",
        "                \"id\": sample_id,\n",
        "                \"emotional\": 1 if \"emotional\" in response.lower() and (\"1\" in response or \"true\" in response.lower()) else 0,\n",
        "                \"physical\": 1 if \"physical\" in response.lower() and (\"1\" in response or \"true\" in response.lower()) else 0,\n",
        "                \"sexual\": 1 if \"sexual\" in response.lower() and (\"1\" in response or \"true\" in response.lower()) else 0,\n",
        "            }\n",
        "        \n",
        "        # Ensure ID is set\n",
        "        prediction[\"id\"] = sample_id\n",
        "        \n",
        "        # Ensure binary values are integers (0 or 1)\n",
        "        for key in [\"emotional\", \"physical\", \"sexual\"]:\n",
        "            if key in prediction:\n",
        "                val = prediction[key]\n",
        "                if isinstance(val, bool):\n",
        "                    prediction[key] = 1 if val else 0\n",
        "                elif isinstance(val, (int, float)):\n",
        "                    prediction[key] = 1 if val > 0 else 0\n",
        "                else:\n",
        "                    prediction[key] = 0\n",
        "        \n",
        "        return {\n",
        "            \"id\": sample_id,\n",
        "            \"text\": text,\n",
        "            \"emotional\": prediction.get(\"emotional\", 0),\n",
        "            \"physical\": prediction.get(\"physical\", 0),\n",
        "            \"sexual\": prediction.get(\"sexual\", 0),\n",
        "            \"raw_response\": response,\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"id\": sample_id,\n",
        "            \"text\": text,\n",
        "            \"emotional\": 0,\n",
        "            \"physical\": 0,\n",
        "            \"sexual\": 0,\n",
        "            \"raw_response\": f\"ERROR: {str(e)}\",\n",
        "        }\n",
        "\n",
        "print(\"Starting prediction generation...\")\n",
        "predictions = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
        "    sample_id = int(idx)\n",
        "    \n",
        "    print(f\"Processing sample {sample_id}...\", end=\" \")\n",
        "    pred = make_prediction(text, sample_id)\n",
        "    predictions.append(pred)\n",
        "    print(f\"Done (emotional={pred['emotional']}, physical={pred['physical']}, sexual={pred['sexual']})\")\n",
        "\n",
        "print(f\"\\nGenerated {len(predictions)} predictions!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results directory if it doesn't exist\n",
        "results_path = Path(RESULTS_DIR)\n",
        "results_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Generate output filename with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "output_file = results_path / f\"{OUTPUT_FILENAME}_{timestamp}.json\"\n",
        "\n",
        "# Save predictions to JSON\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "print(f\"Total predictions: {len(predictions)}\")\n",
        "\n",
        "# Display summary statistics\n",
        "emotional_count = sum(1 for p in predictions if p[\"emotional\"] == 1)\n",
        "physical_count = sum(1 for p in predictions if p[\"physical\"] == 1)\n",
        "sexual_count = sum(1 for p in predictions if p[\"sexual\"] == 1)\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Emotional abuse: {emotional_count} ({100*emotional_count/len(predictions):.1f}%)\")\n",
        "print(f\"  Physical abuse: {physical_count} ({100*physical_count/len(predictions):.1f}%)\")\n",
        "print(f\"  Sexual abuse: {sexual_count} ({100*sexual_count/len(predictions):.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Predictions with Chain-of-Thought (COT) / Meta Prompts\n",
        "\n",
        "This section is specifically for prompts that include reasoning/chain-of-thought output, such as COT or Meta prompt styles. The reasoning steps will be extracted and saved along with the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== CONFIGURATION FOR COT/META PROMPTS - EDIT THESE SETTINGS ==========\n",
        "\n",
        "# Prompt template with reasoning steps (use {text} for sentence and {sample_id} for ID)\n",
        "PROMPT_TEMPLATE_COT = \"\"\"\n",
        "You are tasked to determine whether the sentence contains emotional abuse, physical abuse, sexual abuse, or none. A sentence may contain multiple types or none at all.\n",
        "\n",
        "Your chain-of-thought should check for:\n",
        "• Physical IPV cues: hitting, kicking, choking, slapping, pushing, grabbing, burning, using weapons, restraining, threats of physical harm.\n",
        "• Sexual IPV cues: coercion, pressuring for sex, unwanted touching, sexual name-calling, lack of consent, intoxicated/incapacitated scenarios, forced penetration, sexual intimidation.\n",
        "• Emotional IPV cues: manipulation, humiliation, gaslighting, isolation, jealousy, threatening abandonment, unpredictable anger, degradation, economic control, insults.\n",
        "\n",
        "Other cues involve:\n",
        "• relationship context (partner, ex, spouse, boyfriend/girlfriend)\n",
        "• threats, coercion, domination, intimidation\n",
        "• whether harm or risk is implied even without explicit force\n",
        "• whether the described act is consensual or not\n",
        "\n",
        "After your reasoning, output the JSON block below.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1,\n",
        "  \"reasoning_steps\": \"One short, high-level explanation of why each label was chosen, describing a summary of your chain-of-thought.\"\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "# Results directory (will be created if it doesn't exist)\n",
        "RESULTS_DIR_COT = \"w4/qwen\"\n",
        "\n",
        "# Dataset path (same as above, or can be different)\n",
        "DATASET_PATH_COT = \"../Dataset/reddit_data_fortesting.csv\"\n",
        "\n",
        "# Model name (same as above)\n",
        "MODEL_NAME_COT = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# Number of samples to process (set to None to process all)\n",
        "NUM_SAMPLES_COT = None\n",
        "\n",
        "# Output filename (without extension)\n",
        "OUTPUT_FILENAME_COT = \"multi_cot\"\n",
        "\n",
        "# =========================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "def extract_json_from_response_cot(response: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Extract JSON from model response, handling various formats. Also extracts reasoning_steps.\"\"\"\n",
        "    # Try to find JSON between <json> tags\n",
        "    match = re.search(r\"<json[^>]*>\\s*(.*?)\\s*</json>\", response, re.DOTALL | re.IGNORECASE)\n",
        "    if match:\n",
        "        json_str = match.group(1).strip()\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    \n",
        "    # Try to find any JSON object in the response\n",
        "    match = re.search(r\"\\{[^{}]*\\\"(id|emotional|physical|sexual|reasoning_steps)\\\"[^{}]*\\}\", response, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(0))\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    \n",
        "    return None\n",
        "\n",
        "def make_prediction_with_reasoning(text: str, sample_id: int) -> Dict[str, Any]:\n",
        "    \"\"\"Make a prediction for a single text sample and extract reasoning steps.\"\"\"\n",
        "    # Format the prompt\n",
        "    prompt = PROMPT_TEMPLATE_COT.format(text=text, sample_id=sample_id)\n",
        "    \n",
        "    try:\n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,  # Increased for reasoning steps\n",
        "                temperature=0.0,\n",
        "                do_sample=False,\n",
        "            )\n",
        "        \n",
        "        # Decode the response\n",
        "        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "        \n",
        "        # Extract JSON from response\n",
        "        prediction = extract_json_from_response_cot(response)\n",
        "        \n",
        "        # Extract reasoning_steps if available (either from JSON or from raw response)\n",
        "        reasoning_steps = \"\"\n",
        "        if prediction and \"reasoning_steps\" in prediction:\n",
        "            reasoning_steps = str(prediction.get(\"reasoning_steps\", \"\"))\n",
        "        elif not prediction:\n",
        "            # Try to extract reasoning from raw response if JSON extraction failed\n",
        "            # Look for text before <json> tags or after reasoning markers\n",
        "            reasoning_match = re.search(r\"reasoning[:\\s]*(.*?)(?:<json>|$)\", response, re.DOTALL | re.IGNORECASE)\n",
        "            if reasoning_match:\n",
        "                reasoning_steps = reasoning_match.group(1).strip()\n",
        "        \n",
        "        if prediction is None:\n",
        "            # Fallback: try to extract values from text\n",
        "            prediction = {\n",
        "                \"id\": sample_id,\n",
        "                \"emotional\": 1 if \"emotional\" in response.lower() and (\"1\" in response or \"true\" in response.lower()) else 0,\n",
        "                \"physical\": 1 if \"physical\" in response.lower() and (\"1\" in response or \"true\" in response.lower()) else 0,\n",
        "                \"sexual\": 1 if \"sexual\" in response.lower() and (\"1\" in response or \"true\" in response.lower()) else 0,\n",
        "            }\n",
        "        \n",
        "        # Ensure ID is set\n",
        "        prediction[\"id\"] = sample_id\n",
        "        \n",
        "        # Ensure binary values are integers (0 or 1)\n",
        "        for key in [\"emotional\", \"physical\", \"sexual\"]:\n",
        "            if key in prediction:\n",
        "                val = prediction[key]\n",
        "                if isinstance(val, bool):\n",
        "                    prediction[key] = 1 if val else 0\n",
        "                elif isinstance(val, (int, float)):\n",
        "                    prediction[key] = 1 if val > 0 else 0\n",
        "                else:\n",
        "                    prediction[key] = 0\n",
        "        \n",
        "        return {\n",
        "            \"id\": sample_id,\n",
        "            \"text\": text,\n",
        "            \"emotional\": prediction.get(\"emotional\", 0),\n",
        "            \"physical\": prediction.get(\"physical\", 0),\n",
        "            \"sexual\": prediction.get(\"sexual\", 0),\n",
        "            \"reasoning_steps\": reasoning_steps,\n",
        "            \"raw_response\": response,\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"id\": sample_id,\n",
        "            \"text\": text,\n",
        "            \"emotional\": 0,\n",
        "            \"physical\": 0,\n",
        "            \"sexual\": 0,\n",
        "            \"reasoning_steps\": f\"ERROR: {str(e)}\",\n",
        "            \"raw_response\": f\"ERROR: {str(e)}\",\n",
        "        }\n",
        "\n",
        "print(\"Starting prediction generation with COT/Meta reasoning...\")\n",
        "predictions_cot = []\n",
        "\n",
        "# Load dataset for COT if different path, otherwise use existing df\n",
        "if DATASET_PATH_COT != DATASET_PATH or 'df_cot' not in locals():\n",
        "    df_cot = pd.read_csv(DATASET_PATH_COT)\n",
        "    if NUM_SAMPLES_COT is not None:\n",
        "        df_cot = df_cot.head(NUM_SAMPLES_COT)\n",
        "    print(f\"Dataset loaded for COT: {len(df_cot)} rows\")\n",
        "else:\n",
        "    df_cot = df.copy()\n",
        "    if NUM_SAMPLES_COT is not None:\n",
        "        df_cot = df_cot.head(NUM_SAMPLES_COT)\n",
        "\n",
        "for idx, row in df_cot.iterrows():\n",
        "    text = row[\"items\"] if \"items\" in df_cot.columns else str(row.iloc[0])\n",
        "    sample_id = int(idx)\n",
        "    \n",
        "    print(f\"Processing sample {sample_id}...\", end=\" \")\n",
        "    pred = make_prediction_with_reasoning(text, sample_id)\n",
        "    predictions_cot.append(pred)\n",
        "    has_reasoning = len(pred.get('reasoning_steps', '')) > 0\n",
        "    print(f\"Done (emotional={pred['emotional']}, physical={pred['physical']}, sexual={pred['sexual']}, reasoning={has_reasoning})\")\n",
        "\n",
        "print(f\"\\nGenerated {len(predictions_cot)} predictions with reasoning steps!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results directory if it doesn't exist\n",
        "results_path = Path(RESULTS_DIR_COT)\n",
        "results_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Generate output filename with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "output_file = results_path / f\"{OUTPUT_FILENAME_COT}_{timestamp}.json\"\n",
        "\n",
        "# Save predictions to JSON (including reasoning_steps)\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(predictions_cot, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "print(f\"Total predictions: {len(predictions_cot)}\")\n",
        "\n",
        "# Display summary statistics\n",
        "emotional_count = sum(1 for p in predictions_cot if p[\"emotional\"] == 1)\n",
        "physical_count = sum(1 for p in predictions_cot if p[\"physical\"] == 1)\n",
        "sexual_count = sum(1 for p in predictions_cot if p[\"sexual\"] == 1)\n",
        "reasoning_count = sum(1 for p in predictions_cot if p.get(\"reasoning_steps\") and len(str(p.get(\"reasoning_steps\", \"\"))) > 0)\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Emotional abuse: {emotional_count} ({100*emotional_count/len(predictions_cot):.1f}%)\")\n",
        "print(f\"  Physical abuse: {physical_count} ({100*physical_count/len(predictions_cot):.1f}%)\")\n",
        "print(f\"  Sexual abuse: {sexual_count} ({100*sexual_count/len(predictions_cot):.1f}%)\")\n",
        "print(f\"  Predictions with reasoning steps: {reasoning_count} ({100*reasoning_count/len(predictions_cot):.1f}%)\")\n",
        "\n",
        "# Show example with reasoning\n",
        "if reasoning_count > 0:\n",
        "    print(f\"\\nExample prediction with reasoning:\")\n",
        "    example = next((p for p in predictions_cot if p.get(\"reasoning_steps\") and len(str(p.get(\"reasoning_steps\", \"\"))) > 0), None)\n",
        "    if example:\n",
        "        print(f\"  ID: {example['id']}\")\n",
        "        print(f\"  Text: {example['text'][:100]}...\")\n",
        "        print(f\"  Labels: emotional={example['emotional']}, physical={example['physical']}, sexual={example['sexual']}\")\n",
        "        print(f\"  Reasoning: {example['reasoning_steps'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate COT/Meta Predictions and Save Metrics to multitype_result.json\n",
        "\n",
        "This cell evaluates the COT predictions against ground truth and saves metrics to multitype_result.json, including reasoning_steps metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate COT/Meta predictions and save metrics to multitype_result.json\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Configuration\n",
        "GT_PATH_COT = \"../Dataset/reddit_data.csv\"  # Ground truth CSV path\n",
        "MULTITYPE_RESULT_JSON_PATH = \"../results/multitype_result.json\"  # Path to results JSON\n",
        "PROMPT_TYPE_COT = \"cot\"  # Update this to match your prompt type (e.g., \"cot\", \"meta\")\n",
        "\n",
        "# Load ground truth\n",
        "print(\"Loading ground truth...\")\n",
        "df_gt_cot = pd.read_csv(GT_PATH_COT)\n",
        "\n",
        "# Ensure ID column exists\n",
        "if \"id\" not in df_gt_cot.columns:\n",
        "    df_gt_cot = df_gt_cot.reset_index().rename(columns={\"index\": \"id\"})\n",
        "else:\n",
        "    df_gt_cot[\"id\"] = df_gt_cot[\"id\"].astype(int)\n",
        "\n",
        "print(f\"Ground truth loaded: {len(df_gt_cot)} samples\")\n",
        "\n",
        "# Convert predictions to DataFrame\n",
        "if len(predictions_cot) == 0:\n",
        "    print(\"ERROR: No predictions found. Please run the prediction generation cell first.\")\n",
        "else:\n",
        "    print(f\"Predictions loaded: {len(predictions_cot)} samples\")\n",
        "    \n",
        "    # Create predictions DataFrame\n",
        "    preds_df = pd.DataFrame(predictions_cot)\n",
        "    preds_df[\"id\"] = preds_df[\"id\"].astype(int)\n",
        "    \n",
        "    # Ensure binary values (0/1) in predictions\n",
        "    for col in [\"emotional\", \"physical\", \"sexual\"]:\n",
        "        if col in preds_df.columns:\n",
        "            preds_df[col] = preds_df[col].astype(int).clip(0, 1)\n",
        "    \n",
        "    # Merge with ground truth\n",
        "    merged_df = df_gt_cot.merge(\n",
        "        preds_df[[\"id\", \"emotional\", \"physical\", \"sexual\", \"reasoning_steps\"]],\n",
        "        on='id',\n",
        "        how='inner',\n",
        "        suffixes=('_true', '_pred')\n",
        "    )\n",
        "    \n",
        "    print(f\"Merged dataset: {len(merged_df)} samples\")\n",
        "    \n",
        "    # Prepare ground truth columns (convert boolean to int)\n",
        "    merged_df[\"emotional_true\"] = merged_df[\"Emotional Abuse\"].astype(int)\n",
        "    merged_df[\"physical_true\"] = merged_df[\"Physical Abuse\"].astype(int)\n",
        "    merged_df[\"sexual_true\"] = merged_df[\"Sexual Abuse\"].astype(int)\n",
        "    \n",
        "    # Prepare prediction columns\n",
        "    merged_df[\"emotional_pred\"] = merged_df[\"emotional\"].astype(int)\n",
        "    merged_df[\"physical_pred\"] = merged_df[\"physical\"].astype(int)\n",
        "    merged_df[\"sexual_pred\"] = merged_df[\"sexual\"].astype(int)\n",
        "    \n",
        "    # Compute metrics for each label\n",
        "    def compute_binary_metrics(y_true, y_pred):\n",
        "        \"\"\"Compute detailed binary classification metrics.\"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        \n",
        "        # Extract TP, FP, TN, FN from confusion matrix\n",
        "        if cm.shape == (2, 2):\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "        else:\n",
        "            # Handle edge cases\n",
        "            if len(cm) == 1:\n",
        "                if y_true.sum() == 0:\n",
        "                    tn, fp, fn, tp = len(y_true), 0, 0, 0\n",
        "                else:\n",
        "                    tn, fp, fn, tp = 0, 0, 0, len(y_true)\n",
        "            else:\n",
        "                tn, fp, fn, tp = 0, 0, 0, 0\n",
        "        \n",
        "        return {\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"precision\": float(precision),\n",
        "            \"recall\": float(recall),\n",
        "            \"f1\": float(f1),\n",
        "            \"true_positives\": int(tp),\n",
        "            \"false_positives\": int(fp),\n",
        "            \"true_negatives\": int(tn),\n",
        "            \"false_negatives\": int(fn),\n",
        "        }\n",
        "    \n",
        "    # Compute metrics for each abuse type\n",
        "    physical_metrics = compute_binary_metrics(merged_df[\"physical_true\"], merged_df[\"physical_pred\"])\n",
        "    emotional_metrics = compute_binary_metrics(merged_df[\"emotional_true\"], merged_df[\"emotional_pred\"])\n",
        "    sexual_metrics = compute_binary_metrics(merged_df[\"sexual_true\"], merged_df[\"sexual_pred\"])\n",
        "    \n",
        "    # Compute reasoning_steps statistics\n",
        "    reasoning_stats = {}\n",
        "    reasoning_steps_list = merged_df[\"reasoning_steps\"].fillna(\"\").astype(str)\n",
        "    valid_reasoning = reasoning_steps_list[reasoning_steps_list.str.len() > 0]\n",
        "    \n",
        "    reasoning_stats = {\n",
        "        \"total_samples\": len(merged_df),\n",
        "        \"samples_with_reasoning\": int(valid_reasoning.count()),\n",
        "        \"reasoning_coverage_percent\": float(100 * valid_reasoning.count() / len(merged_df)),\n",
        "        \"avg_reasoning_length\": float(valid_reasoning.str.len().mean()) if len(valid_reasoning) > 0 else 0.0,\n",
        "        \"min_reasoning_length\": int(valid_reasoning.str.len().min()) if len(valid_reasoning) > 0 else 0,\n",
        "        \"max_reasoning_length\": int(valid_reasoning.str.len().max()) if len(valid_reasoning) > 0 else 0,\n",
        "    }\n",
        "    \n",
        "    # Format the results entry\n",
        "    result_entry = {\n",
        "        \"model\": \"qwen2.5-7b\",\n",
        "        \"prompt_type\": PROMPT_TYPE_COT,\n",
        "        \"date_tested\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"total_samples\": len(merged_df),\n",
        "        \"metrics\": {\n",
        "            \"physical\": physical_metrics,\n",
        "            \"emotional\": emotional_metrics,\n",
        "            \"sexual\": sexual_metrics,\n",
        "        },\n",
        "        \"reasoning_steps_stats\": reasoning_stats,\n",
        "        \"notes\": f\"Evaluation of {PROMPT_TYPE_COT} prompt with chain-of-thought reasoning\"\n",
        "    }\n",
        "    \n",
        "    # Load existing results or create new list\n",
        "    result_json_path = Path(MULTITYPE_RESULT_JSON_PATH)\n",
        "    result_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    if result_json_path.exists():\n",
        "        with open(result_json_path, 'r', encoding='utf-8') as f:\n",
        "            existing_results = json.load(f)\n",
        "        # Ensure it's a list\n",
        "        if not isinstance(existing_results, list):\n",
        "            existing_results = [existing_results] if existing_results else []\n",
        "    else:\n",
        "        existing_results = []\n",
        "    \n",
        "    # Append new result\n",
        "    existing_results.append(result_entry)\n",
        "    \n",
        "    # Save to JSON file\n",
        "    with open(result_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(existing_results, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Results saved to: {result_json_path.resolve()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Entry added: qwen2.5-7b - {PROMPT_TYPE_COT}\")\n",
        "    print(f\"\\nMetrics Summary:\")\n",
        "    print(f\"  Physical: Accuracy={physical_metrics['accuracy']:.4f}, F1={physical_metrics['f1']:.4f}\")\n",
        "    print(f\"  Emotional: Accuracy={emotional_metrics['accuracy']:.4f}, F1={emotional_metrics['f1']:.4f}\")\n",
        "    print(f\"  Sexual: Accuracy={sexual_metrics['accuracy']:.4f}, F1={sexual_metrics['f1']:.4f}\")\n",
        "    print(f\"\\nReasoning Steps Statistics:\")\n",
        "    print(f\"  Samples with reasoning: {reasoning_stats['samples_with_reasoning']}/{reasoning_stats['total_samples']} ({reasoning_stats['reasoning_coverage_percent']:.1f}%)\")\n",
        "    print(f\"  Average reasoning length: {reasoning_stats['avg_reasoning_length']:.0f} characters\")\n",
        "    print(f\"  Min/Max reasoning length: {reasoning_stats['min_reasoning_length']}/{reasoning_stats['max_reasoning_length']} characters\")\n",
        "    print(f\"{'='*80}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
