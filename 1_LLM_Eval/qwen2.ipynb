{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16168294",
   "metadata": {},
   "source": [
    "# Qwen2.5-7B-Instruct to predict IPV and utilize DetectIPV Paper Evaluation Pipeline\n",
    "Key tasks:\n",
    "* Binary IPV detection (IPV vs NOT_IPV)\n",
    "* Multi-label subtype detection (Physical, Emotional, Sexual)\n",
    "\n",
    "**Key Evaluation Components from the Paper**\n",
    "| Figure/Table | What it Shows |\n",
    "|---------------|----------------|\n",
    "| **Fig. 2(a)** | ROC curve for *General Violence Model* |\n",
    "| **Fig. 2(b–c)** | “Waterfall” plot: sentences sorted by predicted confidence; color = ground-truth subtype |\n",
    "| **Fig. 3(a)** | AUROC of Type-Specific vs General Models |\n",
    "| **Fig. 3(b–e)** | More waterfall plots for type-specific models |\n",
    "| **Fig. 4** | Radial visualization of confidence in 3-D type space |\n",
    "| **Table 4** | AUROC for models trained on different negative examples |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394d82e",
   "metadata": {},
   "source": [
    "## 1. Prompt Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221224c",
   "metadata": {},
   "source": [
    "## 2. System & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9833a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ca8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILENAMES\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "filename=\"617points.csv\"\n",
    "\n",
    "#Load Model & Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",      \n",
    "    torch_dtype=torch.bfloat16  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256833ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "031b83e9",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(name):\n",
    "    with open(Path(\"prompts\") / f\"{name}.txt\", \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "binary_prompt = load_prompt(\"binary_v1\")\n",
    "prompt = binary_prompt.format(text=row[\"items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e081dbc",
   "metadata": {},
   "source": [
    "## 4. Evaluation Pipeline\n",
    "\n",
    "1. **Prompt Comparison Phase**\n",
    "   * Design multiple prompt variants for each task:\n",
    "     - **Binary Prompts:** Different phrasings for IPV vs NOT_IPV classification.  \n",
    "     - **Multi-label Prompts:** Different phrasings for type-specific labeling (Physical, Emotional, Sexual, None).\n",
    "   * Run all prompts on the same dataset.\n",
    "   * Compute for each prompt:\n",
    "     - Accuracy, F1-score, ROC-AUC (per task or per subtype)\n",
    "     - Average confidence calibration (mean predicted probability for positives/negatives)\n",
    "   * Visualize:\n",
    "     - **Prompt Comparison Table or Bar Plot** showing AUC/F1 across prompts\n",
    "     - Identify the **best-performing prompt** for each task (Binary, Multi-label)\n",
    "\n",
    "2. **Final Evaluation with Best Prompt**\n",
    "   * **Binary (General IPV Model):**\n",
    "     - Use the best binary prompt to compute final **ROC curve**, **AUC**, **Precision-Recall**, **Accuracy**, and **F1-score**.\n",
    "     - Plot **ROC Curve** (replicating Fig. 2a).\n",
    "     - Generate **Waterfall Plot** — sentences sorted by prediction confidence, color-coded by true label (Physical, Emotional, Sexual, Negative).\n",
    "\n",
    "   * **Multi-label (Type-Specific Models):**\n",
    "     - Use the best multi-label prompt to compute **per-type ROC/AUC** for Physical, Emotional, and Sexual abuse.\n",
    "     - Compare type-specific vs. general model performance (bar chart similar to Fig. 3a).\n",
    "     - Produce **Waterfall Plots** for each subtype showing confidence distribution and overlap between true labels and predictions.\n",
    "\n",
    "3. **(Optional) Extended Visualization**\n",
    "   * Create a **3D or Radial Plot** showing confidence magnitudes of all three type-specific predictions for interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f45f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
