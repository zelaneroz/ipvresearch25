{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9vAhtXFRJA3"
      },
      "source": [
        "# Evaluation Pipeline\n",
        "\n",
        "The evaluation system is divided into **three main stages**:\n",
        "\n",
        "| Stage | Purpose | Core Functions |\n",
        "|--------|----------|----------------|\n",
        "| **A. Inference / Test Phase** | Generate model predictions for each prompt type and save JSON results | `test_binary_prompts()` / `test_multilabel_prompts()` |\n",
        "| **B. Processing & Comparison Phase** | Read JSON results, merge with ground truth, compute metrics, and compare prompts | `process_results_file()`, `_binary_metrics()`, `_multilabel_metrics()`, `compare_prompts()` |\n",
        "| **C. Final Evaluation & Visualization Phase** | Evaluate best-performing prompts in depth and visualize results | `final_evaluation_binary()`, `final_evaluation_multilabel()`, `plot_prompt_comparison()`, `run_prompt_comparison_and_final()` |\n",
        "\n",
        "Each stage produces intermediate outputs (JSONs, metrics, plots) that can be reused without recomputation.\n",
        "\n",
        "\n",
        "## 3. Stage B — Processing & Comparison Phase\n",
        "This phase processes saved results and computes metrics per prompt.\n",
        "`process_results_file(file_path, df_truth, task)`\n",
        "Reads one JSON results file and merges it with ground truth.\n",
        "Duties:\n",
        "* Load JSON (binary_*.json or multilabel_*.json).\n",
        "* Merge with ground-truth DataFrame on id.\n",
        "* Convert into a clean numeric format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LPE60AmtHcTI"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUATION PIPELINE: Prompt Comparison + Final Evaluation\n",
        "# ============================================================\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "# ---------- Utilities ---------------------------------------------------------\n",
        "def _ensure_list(x):\n",
        "    if x is None or (isinstance(x, float) and math.isnan(x)):\n",
        "        return []\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    if isinstance(x, str):\n",
        "        # split comma-joined strings like \"Emotional, Physical\"\n",
        "        parts = [p.strip() for p in re.split(r\"[;,]\", x) if p.strip()]\n",
        "        return parts if parts else [x]\n",
        "    return [x]\n",
        "\n",
        "def _try_extract_scores(record: dict) -> Optional[Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Try to extract per-class probabilities/scores if your model emitted them\n",
        "    inside raw_response JSON (e.g., {\"probs\": {\"IPV\":0.81,\"NOT_IPV\":0.19}} or\n",
        "    {\"scores\":{\"Physical\":0.4,...}}). If not found, return None.\n",
        "    \"\"\"\n",
        "    raw = record.get(\"raw_response\", \"\")\n",
        "    # Try to locate JSON block between <json>...</json> first\n",
        "    m = re.search(r\"<json[^>]*>\\s*(\\{.*?\\})\\s*</json>\", raw, re.DOTALL | re.IGNORECASE)\n",
        "    text_blocks = []\n",
        "    if m:\n",
        "        text_blocks.append(m.group(1))\n",
        "    else:\n",
        "        # fallback: any JSON-like object\n",
        "        m2 = re.search(r\"(\\{.*\\})\", raw, re.DOTALL)\n",
        "        if m2:\n",
        "            text_blocks.append(m2.group(1))\n",
        "\n",
        "    for block in text_blocks:\n",
        "        try:\n",
        "            parsed = json.loads(block)\n",
        "            # Common keys people use\n",
        "            for key in (\"probs\", \"probabilities\", \"scores\", \"confidences\"):\n",
        "                if key in parsed and isinstance(parsed[key], dict):\n",
        "                    # normalize keys to str, vals to float\n",
        "                    return {str(k): float(v) for k, v in parsed[key].items()}\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def _binarize_labels(labels: List[str], positive=\"IPV\") -> int:\n",
        "    \"\"\"Return 1 if positive present, else 0.\"\"\"\n",
        "    s = {lbl.strip() for lbl in _ensure_list(labels)}\n",
        "    return 1 if positive in s else 0\n",
        "\n",
        "def _one_vs_rest_columns() -> List[str]:\n",
        "    return [\"Physical\", \"Emotional\", \"Sexual\"]\n",
        "\n",
        "# ---------- Helper: process a results file ------------------------------------\n",
        "\n",
        "def process_results_file(\n",
        "    file_path: str | Path,\n",
        "    df_truth: pd.DataFrame,\n",
        "    task: str,  # \"binary\" or \"multilabel\"\n",
        "    cache_dir: str | Path = \"../1_LLM_Eval/eval_cache\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Load one result JSON file, merge with ground truth, and produce a tidy frame:\n",
        "    Returns dict with:\n",
        "      - 'file': file_path\n",
        "      - 'prompt_type': inferred from file name\n",
        "      - 'task': task\n",
        "      - 'df': DataFrame with columns:\n",
        "          id, y_true, y_pred, y_score (binary)\n",
        "          ...or for multilabel: id, and for each class C in {Physical, Emotional, Sexual}:\n",
        "              y_true_C, y_pred_C, y_score_C (if available)\n",
        "      - 'has_scores': whether per-sample probabilities were found\n",
        "    Caching: saves a compact .parquet keyed by mtime so future calls reuse.\n",
        "    \"\"\"\n",
        "    file_path = Path(file_path)\n",
        "    cache_dir = Path(cache_dir)\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "    stamp = f\"{file_path.stat().st_mtime_ns}\"\n",
        "    cache_key = cache_dir / f\"{file_path.stem}_{stamp}.parquet\"\n",
        "\n",
        "    if cache_key.exists():\n",
        "        out = pd.read_parquet(cache_key)\n",
        "        # Rehydrate metadata saved in footer\n",
        "        meta_path = cache_dir / f\"{file_path.stem}_{stamp}.meta.json\"\n",
        "        meta = json.loads(Path(meta_path).read_text()) if meta_path.exists() else {}\n",
        "        return {\n",
        "            \"file\": str(file_path),\n",
        "            \"prompt_type\": meta.get(\"prompt_type\", file_path.stem),\n",
        "            \"task\": task,\n",
        "            \"df\": out.copy(),\n",
        "            \"has_scores\": bool(meta.get(\"has_scores\", False)),\n",
        "        }\n",
        "\n",
        "    # Read results JSON\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        records = json.load(f)\n",
        "\n",
        "    df_res = pd.DataFrame(records)\n",
        "    # Infer prompt_type if not carried\n",
        "    prompt_type = df_res[\"prompt_type\"].iloc[0] if \"prompt_type\" in df_res.columns else file_path.stem\n",
        "\n",
        "    # Merge with ground truth on id (left join to keep only evaluated rows)\n",
        "    key = \"id\"\n",
        "    df_truth = df_truth.copy()\n",
        "    if key not in df_truth.columns:\n",
        "        raise ValueError(\"df_truth must include an 'id' column.\")\n",
        "\n",
        "    # Build tidy frame\n",
        "    if task == \"binary\":\n",
        "        if \"label\" not in df_truth.columns:\n",
        "            raise ValueError(\"df_truth for binary must include a 'label' column with values in {'IPV','NOT_IPV'}\")\n",
        "        # y_true\n",
        "        merged = df_res.merge(df_truth[[\"id\", \"label\"]], on=\"id\", how=\"inner\")\n",
        "        merged[\"y_true\"] = merged[\"label\"].map({\"IPV\": 1, \"NOT_IPV\": 0})\n",
        "        # y_pred from extracted_label or response\n",
        "        if \"extracted_label\" in merged.columns:\n",
        "            pred_str = merged[\"extracted_label\"]\n",
        "        elif \"response\" in merged.columns:\n",
        "            pred_str = merged[\"response\"]\n",
        "        else:\n",
        "            raise ValueError(\"Binary results must contain 'extracted_label' or 'response'.\")\n",
        "\n",
        "        merged[\"y_pred\"] = (pred_str.astype(str).str.upper() == \"IPV\").astype(int)\n",
        "\n",
        "        # Optional scores\n",
        "        scores = []\n",
        "        has_scores = False\n",
        "        for _, r in merged.iterrows():\n",
        "            s = _try_extract_scores(r.to_dict())\n",
        "            if s and any(k.upper() in (\"IPV\", \"NOT_IPV\") for k in s.keys()):\n",
        "                has_scores = True\n",
        "                # prefer IPV score if present; else invert NOT_IPV\n",
        "                if \"IPV\" in s:\n",
        "                    scores.append(float(s[\"IPV\"]))\n",
        "                elif \"NOT_IPV\" in s:\n",
        "                    scores.append(1.0 - float(s[\"NOT_IPV\"]))\n",
        "                else:\n",
        "                    scores.append(np.nan)\n",
        "            else:\n",
        "                scores.append(np.nan)\n",
        "        merged[\"y_score\"] = scores\n",
        "\n",
        "        tidy = merged[[\"id\", \"y_true\", \"y_pred\", \"y_score\"]].copy()\n",
        "\n",
        "    elif task == \"multilabel\":\n",
        "        # Ground truth expects boolean columns for each subtype\n",
        "        need_cols = _one_vs_rest_columns()\n",
        "        for c in need_cols:\n",
        "            if c not in df_truth.columns:\n",
        "                raise ValueError(f\"df_truth for multilabel must include boolean column '{c}'\")\n",
        "\n",
        "        merged = df_res.merge(df_truth[[\"id\"] + need_cols], on=\"id\", how=\"inner\")\n",
        "\n",
        "        # Predictions: list of strings in 'extracted_labels' or 'response'\n",
        "        if \"extracted_labels\" in merged.columns:\n",
        "            preds = merged[\"extracted_labels\"].apply(_ensure_list)\n",
        "        elif \"response\" in merged.columns:\n",
        "            preds = merged[\"response\"].apply(_ensure_list)\n",
        "        else:\n",
        "            raise ValueError(\"Multilabel results must contain 'extracted_labels' or 'response' (list or comma string).\")\n",
        "\n",
        "        for c in need_cols:\n",
        "            merged[f\"y_true_{c}\"] = merged[c].astype(int)\n",
        "            merged[f\"y_pred_{c}\"] = preds.apply(lambda L: int(c in set(map(str, L))))\n",
        "\n",
        "        # Optional per-class scores if present\n",
        "        has_scores = False\n",
        "        for c in need_cols:\n",
        "            merged[f\"y_score_{c}\"] = np.nan\n",
        "\n",
        "        for idx, r in merged.iterrows():\n",
        "            s = _try_extract_scores(r.to_dict())\n",
        "            if s:\n",
        "                has_scores = True\n",
        "                # normalize keys to capitalized class names if possible\n",
        "                norm = {k.strip().capitalize(): v for k, v in s.items()}\n",
        "                for c in need_cols:\n",
        "                    if c in norm:\n",
        "                        merged.at[idx, f\"y_score_{c}\"] = float(norm[c])\n",
        "\n",
        "        tidy_cols = [\"id\"] + \\\n",
        "                    [f\"y_true_{c}\" for c in need_cols] + \\\n",
        "                    [f\"y_pred_{c}\" for c in need_cols] + \\\n",
        "                    [f\"y_score_{c}\" for c in need_cols]\n",
        "        tidy = merged[tidy_cols].copy()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"task must be 'binary' or 'multilabel'.\")\n",
        "\n",
        "    # Cache\n",
        "    tidy.to_parquet(cache_key, index=False)\n",
        "    meta = {\"prompt_type\": prompt_type, \"has_scores\": has_scores}\n",
        "    (cache_dir / f\"{file_path.stem}_{stamp}.meta.json\").write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "    return {\n",
        "        \"file\": str(file_path),\n",
        "        \"prompt_type\": prompt_type,\n",
        "        \"task\": task,\n",
        "        \"df\": tidy.copy(),\n",
        "        \"has_scores\": has_scores,\n",
        "    }\n",
        "\n",
        "\n",
        "    # ---------- Metrics -----------------------------------------------------------\n",
        "\n",
        "def _binary_metrics(df_bin: pd.DataFrame) -> Dict[str, float]:\n",
        "    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "    y_true = df_bin[\"y_true\"].values\n",
        "    y_pred = df_bin[\"y_pred\"].values.astype(int)\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1  = float(f1_score(y_true, y_pred, zero_division=0))\n",
        "    # AUC if scores available\n",
        "    if df_bin[\"y_score\"].notna().any():\n",
        "        y_score = df_bin[\"y_score\"].fillna(0.5).values\n",
        "        try:\n",
        "            auc = float(roc_auc_score(y_true, y_score))\n",
        "        except Exception:\n",
        "            auc = float(\"nan\")\n",
        "    else:\n",
        "        auc = float(\"nan\")\n",
        "    return {\"Accuracy\": acc, \"F1\": f1, \"ROC_AUC\": auc}\n",
        "\n",
        "def _multilabel_metrics(df_ml: pd.DataFrame) -> Dict[str, float]:\n",
        "    from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "    classes = _one_vs_rest_columns()\n",
        "\n",
        "    # Macro F1 across the three abuse types (ignoring NOT_IPV)\n",
        "    y_true = np.vstack([df_ml[f\"y_true_{c}\"].values for c in classes]).T\n",
        "    y_pred = np.vstack([df_ml[f\"y_pred_{c}\"].values for c in classes]).T\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
        "    f1_micro = float(f1_score(y_true, y_pred, average=\"micro\", zero_division=0))\n",
        "\n",
        "    # \"Exact match\" accuracy: all three match\n",
        "    exact_acc = float((y_true == y_pred).all(axis=1).mean())\n",
        "\n",
        "    # Per-class AUC if scores present\n",
        "    aucs = {}\n",
        "    have_any_scores = False\n",
        "    for c in classes:\n",
        "        s = df_ml[f\"y_score_{c}\"]\n",
        "        if s.notna().any():\n",
        "            have_any_scores = True\n",
        "            try:\n",
        "                aucs[c] = float(roc_auc_score(df_ml[f\"y_true_{c}\"].values, s.fillna(0.5).values))\n",
        "            except Exception:\n",
        "                aucs[c] = float(\"nan\")\n",
        "        else:\n",
        "            aucs[c] = float(\"nan\")\n",
        "\n",
        "    # Aggregate AUC (macro) across available classes\n",
        "    if have_any_scores:\n",
        "        auc_macro = float(np.nanmean([aucs[c] for c in classes]))\n",
        "    else:\n",
        "        auc_macro = float(\"nan\")\n",
        "\n",
        "    out = {\n",
        "        \"F1_macro\": f1_macro,\n",
        "        \"F1_micro\": f1_micro,\n",
        "        \"ExactMatchAcc\": exact_acc,\n",
        "        \"ROC_AUC_macro\": auc_macro,\n",
        "    }\n",
        "    # include per-class AUC columns\n",
        "    for c in classes:\n",
        "        out[f\"AUC_{c}\"] = aucs[c]\n",
        "    return out\n",
        "\n",
        "# ---------- Prompt comparison over a directory --------------------------------\n",
        "\n",
        "def compare_prompts(\n",
        "    results_dir: str | Path,\n",
        "    df_truth_binary: pd.DataFrame,\n",
        "    df_truth_multilabel: pd.DataFrame,\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Loop test_results directory, process each file once (using cache),\n",
        "    compute metrics, and return two DataFrames:\n",
        "      - df_cmp_binary: metrics per binary prompt\n",
        "      - df_cmp_multilabel: metrics per multilabel prompt\n",
        "    Also returns best prompt names for each task.\n",
        "    \"\"\"\n",
        "    results_dir = Path(results_dir)\n",
        "\n",
        "    rows_bin, rows_ml = [], []\n",
        "\n",
        "    for fp in sorted(results_dir.glob(\"*.json\")):\n",
        "        name = fp.stem\n",
        "        if name.startswith(\"binary_\"):\n",
        "            info = process_results_file(fp, df_truth_binary, task=\"binary\")\n",
        "            metrics = _binary_metrics(info[\"df\"])\n",
        "            row = {\"prompt\": info[\"prompt_type\"], **metrics, \"file\": str(fp)}\n",
        "            rows_bin.append(row)\n",
        "\n",
        "        elif name.startswith(\"multilabel_\"):\n",
        "            info = process_results_file(fp, df_truth_multilabel, task=\"multilabel\")\n",
        "            metrics = _multilabel_metrics(info[\"df\"])\n",
        "            row = {\"prompt\": info[\"prompt_type\"], **metrics, \"file\": str(fp)}\n",
        "            rows_ml.append(row)\n",
        "\n",
        "    df_cmp_binary = pd.DataFrame(rows_bin).sort_values([\"F1\", \"ROC_AUC\", \"Accuracy\"], ascending=False)\n",
        "    df_cmp_multilabel = pd.DataFrame(rows_ml).sort_values([\"F1_macro\", \"ROC_AUC_macro\", \"ExactMatchAcc\"], ascending=False)\n",
        "\n",
        "    best_binary = df_cmp_binary.iloc[0][\"prompt\"] if not df_cmp_binary.empty else None\n",
        "    best_multilabel = df_cmp_multilabel.iloc[0][\"prompt\"] if not df_cmp_multilabel.empty else None\n",
        "\n",
        "    return {\n",
        "        \"binary\": df_cmp_binary.reset_index(drop=True),\n",
        "        \"multilabel\": df_cmp_multilabel.reset_index(drop=True),\n",
        "        \"best_binary\": best_binary,\n",
        "        \"best_multilabel\": best_multilabel,\n",
        "    }\n",
        "\n",
        "# ---------- Visualization (comparison phase) ----------------------------------\n",
        "\n",
        "def plot_prompt_comparison(\n",
        "    df_cmp_binary: pd.DataFrame,\n",
        "    df_cmp_multilabel: pd.DataFrame,\n",
        "    out_dir: str | Path = \"../1_LLM_Eval/figs\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Create simple tables and barplots comparing prompts (AUC/F1).\n",
        "    Uses matplotlib only.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Binary: bar plot F1 and AUC\n",
        "    if not df_cmp_binary.empty:\n",
        "        plt.figure()\n",
        "        x = np.arange(len(df_cmp_binary))\n",
        "        plt.bar(x - 0.2, df_cmp_binary[\"F1\"], width=0.4, label=\"F1\")\n",
        "        # Safe AUC display\n",
        "        auc_vals = df_cmp_binary[\"ROC_AUC\"].fillna(0)\n",
        "        plt.bar(x + 0.2, auc_vals, width=0.4, label=\"ROC-AUC\")\n",
        "        plt.xticks(x, df_cmp_binary[\"prompt\"], rotation=45, ha=\"right\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.title(\"Binary Prompt Comparison\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / \"binary_prompt_comparison.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # Multilabel: bar plot Macro F1 and Macro AUC\n",
        "    if not df_cmp_multilabel.empty:\n",
        "        plt.figure()\n",
        "        x = np.arange(len(df_cmp_multilabel))\n",
        "        plt.bar(x - 0.2, df_cmp_multilabel[\"F1_macro\"], width=0.4, label=\"F1_macro\")\n",
        "        auc_vals = df_cmp_multilabel[\"ROC_AUC_macro\"].fillna(0)\n",
        "        plt.bar(x + 0.2, auc_vals, width=0.4, label=\"ROC_AUC_macro\")\n",
        "        plt.xticks(x, df_cmp_multilabel[\"prompt\"], rotation=45, ha=\"right\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.title(\"Multilabel Prompt Comparison\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / \"multilabel_prompt_comparison.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "# ---------- Final Evaluation with selected prompt (no reprocessing) -----------\n",
        "\n",
        "def final_evaluation_binary(\n",
        "    best_file_path: str | Path,\n",
        "    df_truth_binary: pd.DataFrame\n",
        "):\n",
        "    \"\"\"\n",
        "    Use the cached, processed outputs for the best binary prompt to:\n",
        "      - Compute ROC curve, PR curve, Accuracy, F1\n",
        "      - Produce a waterfall plot (sorted by score if available; else by prediction)\n",
        "    Saves figures to ../1_LLM_Eval/figs.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import roc_curve, precision_recall_curve, auc, accuracy_score, f1_score\n",
        "\n",
        "    info = process_results_file(best_file_path, df_truth_binary, task=\"binary\")\n",
        "    dfb = info[\"df\"]\n",
        "    out_dir = Path(\"../1_LLM_Eval/figs\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    y_true = dfb[\"y_true\"].values\n",
        "    y_pred = dfb[\"y_pred\"].values\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1  = float(f1_score(y_true, y_pred, zero_division=0))\n",
        "\n",
        "    # ROC + PR if scores available; otherwise use 0/1 \"scores\"\n",
        "    if dfb[\"y_score\"].notna().any():\n",
        "        scores = dfb[\"y_score\"].fillna(0.5).values\n",
        "    else:\n",
        "        scores = y_pred.astype(float)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "    roc_auc = float(auc(fpr, tpr))\n",
        "\n",
        "    prec, rec, _ = precision_recall_curve(y_true, scores)\n",
        "    pr_auc = float(auc(rec, prec))\n",
        "\n",
        "    # Save ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
        "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Binary ROC\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"binary_ROC.png\", dpi=150); plt.close()\n",
        "\n",
        "    # Save PR\n",
        "    plt.figure()\n",
        "    plt.plot(rec, prec, label=f\"PR AUC={pr_auc:.3f}\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Binary PR Curve\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"binary_PR.png\", dpi=150); plt.close()\n",
        "\n",
        "    # Waterfall: sort by score desc\n",
        "    order = np.argsort(-scores)\n",
        "    plt.figure()\n",
        "    plt.bar(np.arange(len(scores)), scores[order], edgecolor=\"none\")\n",
        "    # overlay color by true class (blue=NOT_IPV 0, orange=IPV 1)\n",
        "    for idx, o in enumerate(order):\n",
        "        if y_true[o] == 1:\n",
        "            plt.plot(idx, scores[order][idx], marker=\"o\")  # simple overlay\n",
        "    plt.xlabel(\"Samples (sorted by score)\"); plt.ylabel(\"Score\")\n",
        "    plt.title(\"Binary Waterfall (dots mark true IPV=1)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"binary_waterfall.png\", dpi=150); plt.close()\n",
        "\n",
        "    return {\"Accuracy\": acc, \"F1\": f1, \"ROC_AUC\": roc_auc, \"PR_AUC\": pr_auc}\n",
        "\n",
        "def final_evaluation_multilabel(\n",
        "    best_file_path: str | Path,\n",
        "    df_truth_multilabel: pd.DataFrame\n",
        "):\n",
        "    \"\"\"\n",
        "    Use the cached, processed outputs for the best multi-label prompt to:\n",
        "      - Compute per-type ROC/AUC (Physical, Emotional, Sexual)\n",
        "      - Bar chart comparing type-specific AUCs and Macro F1\n",
        "      - Waterfall plot for each subtype (sorted by score if available)\n",
        "    Saves figures to ../1_LLM_Eval/figs.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import roc_curve, auc, f1_score\n",
        "\n",
        "    classes = _one_vs_rest_columns()\n",
        "    info = process_results_file(best_file_path, df_truth_multilabel, task=\"multilabel\")\n",
        "    dfm = info[\"df\"]\n",
        "    out_dir = Path(\"../1_LLM_Eval/figs\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Metrics\n",
        "    f1_macro = float(f1_score(\n",
        "        np.vstack([dfm[f\"y_true_{c}\"] for c in classes]).T,\n",
        "        np.vstack([dfm[f\"y_pred_{c}\"] for c in classes]).T,\n",
        "        average=\"macro\",\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    aucs = {}\n",
        "    for c in classes:\n",
        "        y_true = dfm[f\"y_true_{c}\"].values\n",
        "        s = dfm[f\"y_score_{c}\"]\n",
        "        if s.notna().any():\n",
        "            scores = s.fillna(0.5).values\n",
        "        else:\n",
        "            scores = dfm[f\"y_pred_{c}\"].values.astype(float)  # degenerate but allows plotting\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "        aucs[c] = float(auc(fpr, tpr))\n",
        "\n",
        "        # Save subtype ROC\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, label=f\"AUC={aucs[c]:.3f}\")\n",
        "        plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {c}\")\n",
        "        plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(out_dir / f\"multilabel_ROC_{c}.png\", dpi=150); plt.close()\n",
        "\n",
        "        # Waterfall per subtype\n",
        "        order = np.argsort(-scores)\n",
        "        plt.figure()\n",
        "        plt.bar(np.arange(len(scores)), scores[order], edgecolor=\"none\")\n",
        "        for idx, o in enumerate(order):\n",
        "            if y_true[o] == 1:\n",
        "                plt.plot(idx, scores[order][idx], marker=\"o\")\n",
        "        plt.xlabel(\"Samples (sorted by score)\"); plt.ylabel(\"Score\")\n",
        "        plt.title(f\"Waterfall — {c} (dots mark true {c}=1)\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / f\"multilabel_waterfall_{c}.png\", dpi=150); plt.close()\n",
        "\n",
        "    # Summary bar: AUCs\n",
        "    plt.figure()\n",
        "    xs = np.arange(len(classes))\n",
        "    plt.bar(xs, [aucs[c] for c in classes])\n",
        "    plt.xticks(xs, classes)\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.title(f\"Type-specific AUCs (Macro F1={f1_macro:.3f})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"multilabel_type_AUCs.png\", dpi=150); plt.close()\n",
        "\n",
        "    return {\"F1_macro\": f1_macro, **{f\"AUC_{c}\": aucs[c] for c in classes}}\n",
        "\n",
        "# ---------- Example driver (glue) ---------------------------------------------\n",
        "\n",
        "def run_prompt_comparison_and_final(\n",
        "    results_dir=\"../1_LLM_Eval/test_results\",\n",
        "    df_truth_binary: pd.DataFrame = None,\n",
        "    df_truth_multilabel: pd.DataFrame = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Prompt Comparison Phase: compute metrics per prompt, plot comparisons,\n",
        "       identify best prompts for binary and multilabel.\n",
        "    2) Final Evaluation with the best prompt (uses cached processing).\n",
        "    Returns a dict with DataFrames, best names, and final metrics.\n",
        "    \"\"\"\n",
        "    cmp_out = compare_prompts(results_dir, df_truth_binary, df_truth_multilabel)\n",
        "    plot_prompt_comparison(cmp_out[\"binary\"], cmp_out[\"multilabel\"])\n",
        "\n",
        "    # Find the files for best prompts without reprocessing\n",
        "    res_dir = Path(results_dir)\n",
        "    best_bin_name = cmp_out[\"best_binary\"]\n",
        "    best_ml_name  = cmp_out[\"best_multilabel\"]\n",
        "\n",
        "    best_bin_file = None\n",
        "    best_ml_file  = None\n",
        "    if best_bin_name:\n",
        "        # pick the latest file that matches binary_bestBinName_*.json\n",
        "        candidates = sorted(res_dir.glob(f\"binary_{best_bin_name}_*.json\"))\n",
        "        if candidates:\n",
        "            best_bin_file = candidates[-1]\n",
        "    if best_ml_name:\n",
        "        candidates = sorted(res_dir.glob(f\"multilabel_{best_ml_name}_*.json\"))\n",
        "        if candidates:\n",
        "            best_ml_file = candidates[-1]\n",
        "\n",
        "    final_bin = final_ml = None\n",
        "    if best_bin_file:\n",
        "        final_bin = final_evaluation_binary(best_bin_file, df_truth_binary)\n",
        "    if best_ml_file:\n",
        "        final_ml  = final_evaluation_multilabel(best_ml_file, df_truth_multilabel)\n",
        "\n",
        "    return {\n",
        "        \"comparison_binary\": cmp_out[\"binary\"],\n",
        "        \"comparison_multilabel\": cmp_out[\"multilabel\"],\n",
        "        \"best_binary\": best_bin_name,\n",
        "        \"best_multilabel\": best_ml_name,\n",
        "        \"final_binary_metrics\": final_bin,\n",
        "        \"final_multilabel_metrics\": final_ml,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p22w2ddEPKeB"
      },
      "outputs": [],
      "source": [
        "# ---------- Metrics -----------------------------------------------------------\n",
        "\n",
        "def _binary_metrics(df_bin: pd.DataFrame) -> Dict[str, float]:\n",
        "    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "    y_true = df_bin[\"y_true\"].values\n",
        "    y_pred = df_bin[\"y_pred\"].values.astype(int)\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1  = float(f1_score(y_true, y_pred, zero_division=0))\n",
        "    # AUC if scores available\n",
        "    if df_bin[\"y_score\"].notna().any():\n",
        "        y_score = df_bin[\"y_score\"].fillna(0.5).values\n",
        "        try:\n",
        "            auc = float(roc_auc_score(y_true, y_score))\n",
        "        except Exception:\n",
        "            auc = float(\"nan\")\n",
        "    else:\n",
        "        auc = float(\"nan\")\n",
        "    return {\"Accuracy\": acc, \"F1\": f1, \"ROC_AUC\": auc}\n",
        "\n",
        "def _multilabel_metrics(df_ml: pd.DataFrame) -> Dict[str, float]:\n",
        "    from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "    classes = _one_vs_rest_columns()\n",
        "\n",
        "    # Macro F1 across the three abuse types (ignoring NOT_IPV)\n",
        "    y_true = np.vstack([df_ml[f\"y_true_{c}\"].values for c in classes]).T\n",
        "    y_pred = np.vstack([df_ml[f\"y_pred_{c}\"].values for c in classes]).T\n",
        "    f1_macro = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
        "    f1_micro = float(f1_score(y_true, y_pred, average=\"micro\", zero_division=0))\n",
        "\n",
        "    # \"Exact match\" accuracy: all three match\n",
        "    exact_acc = float((y_true == y_pred).all(axis=1).mean())\n",
        "\n",
        "    # Per-class AUC if scores present\n",
        "    aucs = {}\n",
        "    have_any_scores = False\n",
        "    for c in classes:\n",
        "        s = df_ml[f\"y_score_{c}\"]\n",
        "        if s.notna().any():\n",
        "            have_any_scores = True\n",
        "            try:\n",
        "                aucs[c] = float(roc_auc_score(df_ml[f\"y_true_{c}\"].values, s.fillna(0.5).values))\n",
        "            except Exception:\n",
        "                aucs[c] = float(\"nan\")\n",
        "        else:\n",
        "            aucs[c] = float(\"nan\")\n",
        "\n",
        "    # Aggregate AUC (macro) across available classes\n",
        "    if have_any_scores:\n",
        "        auc_macro = float(np.nanmean([aucs[c] for c in classes]))\n",
        "    else:\n",
        "        auc_macro = float(\"nan\")\n",
        "\n",
        "    out = {\n",
        "        \"F1_macro\": f1_macro,\n",
        "        \"F1_micro\": f1_micro,\n",
        "        \"ExactMatchAcc\": exact_acc,\n",
        "        \"ROC_AUC_macro\": auc_macro,\n",
        "    }\n",
        "    # include per-class AUC columns\n",
        "    for c in classes:\n",
        "        out[f\"AUC_{c}\"] = aucs[c]\n",
        "    return out\n",
        "\n",
        "# ---------- Prompt comparison over a directory --------------------------------\n",
        "\n",
        "def compare_prompts(\n",
        "    results_dir: str | Path,\n",
        "    df_truth_binary: pd.DataFrame,\n",
        "    df_truth_multilabel: pd.DataFrame,\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Loop test_results directory, process each file once (using cache),\n",
        "    compute metrics, and return two DataFrames:\n",
        "      - df_cmp_binary: metrics per binary prompt\n",
        "      - df_cmp_multilabel: metrics per multilabel prompt\n",
        "    Also returns best prompt names for each task.\n",
        "    \"\"\"\n",
        "    results_dir = Path(results_dir)\n",
        "\n",
        "    rows_bin, rows_ml = [], []\n",
        "\n",
        "    for fp in sorted(results_dir.glob(\"*.json\")):\n",
        "        name = fp.stem\n",
        "        if name.startswith(\"binary_\"):\n",
        "            info = process_results_file(fp, df_truth_binary, task=\"binary\")\n",
        "            metrics = _binary_metrics(info[\"df\"])\n",
        "            row = {\"prompt\": info[\"prompt_type\"], **metrics, \"file\": str(fp)}\n",
        "            rows_bin.append(row)\n",
        "\n",
        "        elif name.startswith(\"multilabel_\"):\n",
        "            info = process_results_file(fp, df_truth_multilabel, task=\"multilabel\")\n",
        "            metrics = _multilabel_metrics(info[\"df\"])\n",
        "            row = {\"prompt\": info[\"prompt_type\"], **metrics, \"file\": str(fp)}\n",
        "            rows_ml.append(row)\n",
        "\n",
        "    df_cmp_binary = pd.DataFrame(rows_bin).sort_values([\"F1\", \"ROC_AUC\", \"Accuracy\"], ascending=False)\n",
        "    df_cmp_multilabel = pd.DataFrame(rows_ml).sort_values([\"F1_macro\", \"ROC_AUC_macro\", \"ExactMatchAcc\"], ascending=False)\n",
        "\n",
        "    best_binary = df_cmp_binary.iloc[0][\"prompt\"] if not df_cmp_binary.empty else None\n",
        "    best_multilabel = df_cmp_multilabel.iloc[0][\"prompt\"] if not df_cmp_multilabel.empty else None\n",
        "\n",
        "    return {\n",
        "        \"binary\": df_cmp_binary.reset_index(drop=True),\n",
        "        \"multilabel\": df_cmp_multilabel.reset_index(drop=True),\n",
        "        \"best_binary\": best_binary,\n",
        "        \"best_multilabel\": best_multilabel,\n",
        "    }\n",
        "\n",
        "# ---------- Visualization (comparison phase) ----------------------------------\n",
        "\n",
        "def plot_prompt_comparison(\n",
        "    df_cmp_binary: pd.DataFrame,\n",
        "    df_cmp_multilabel: pd.DataFrame,\n",
        "    out_dir: str | Path = \"../1_LLM_Eval/figs\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Create simple tables and barplots comparing prompts (AUC/F1).\n",
        "    Uses matplotlib only.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Binary: bar plot F1 and AUC\n",
        "    if not df_cmp_binary.empty:\n",
        "        plt.figure()\n",
        "        x = np.arange(len(df_cmp_binary))\n",
        "        plt.bar(x - 0.2, df_cmp_binary[\"F1\"], width=0.4, label=\"F1\")\n",
        "        # Safe AUC display\n",
        "        auc_vals = df_cmp_binary[\"ROC_AUC\"].fillna(0)\n",
        "        plt.bar(x + 0.2, auc_vals, width=0.4, label=\"ROC-AUC\")\n",
        "        plt.xticks(x, df_cmp_binary[\"prompt\"], rotation=45, ha=\"right\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.title(\"Binary Prompt Comparison\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / \"binary_prompt_comparison.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # Multilabel: bar plot Macro F1 and Macro AUC\n",
        "    if not df_cmp_multilabel.empty:\n",
        "        plt.figure()\n",
        "        x = np.arange(len(df_cmp_multilabel))\n",
        "        plt.bar(x - 0.2, df_cmp_multilabel[\"F1_macro\"], width=0.4, label=\"F1_macro\")\n",
        "        auc_vals = df_cmp_multilabel[\"ROC_AUC_macro\"].fillna(0)\n",
        "        plt.bar(x + 0.2, auc_vals, width=0.4, label=\"ROC_AUC_macro\")\n",
        "        plt.xticks(x, df_cmp_multilabel[\"prompt\"], rotation=45, ha=\"right\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.title(\"Multilabel Prompt Comparison\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / \"multilabel_prompt_comparison.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "# ---------- Final Evaluation with selected prompt (no reprocessing) -----------\n",
        "\n",
        "def final_evaluation_binary(\n",
        "    best_file_path: str | Path,\n",
        "    df_truth_binary: pd.DataFrame\n",
        "):\n",
        "    \"\"\"\n",
        "    Use the cached, processed outputs for the best binary prompt to:\n",
        "      - Compute ROC curve, PR curve, Accuracy, F1\n",
        "      - Produce a waterfall plot (sorted by score if available; else by prediction)\n",
        "    Saves figures to ../1_LLM_Eval/figs.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import roc_curve, precision_recall_curve, auc, accuracy_score, f1_score\n",
        "\n",
        "    info = process_results_file(best_file_path, df_truth_binary, task=\"binary\")\n",
        "    dfb = info[\"df\"]\n",
        "    out_dir = Path(\"../1_LLM_Eval/figs\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    y_true = dfb[\"y_true\"].values\n",
        "    y_pred = dfb[\"y_pred\"].values\n",
        "    acc = float(accuracy_score(y_true, y_pred))\n",
        "    f1  = float(f1_score(y_true, y_pred, zero_division=0))\n",
        "\n",
        "    # ROC + PR if scores available; otherwise use 0/1 \"scores\"\n",
        "    if dfb[\"y_score\"].notna().any():\n",
        "        scores = dfb[\"y_score\"].fillna(0.5).values\n",
        "    else:\n",
        "        scores = y_pred.astype(float)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "    roc_auc = float(auc(fpr, tpr))\n",
        "\n",
        "    prec, rec, _ = precision_recall_curve(y_true, scores)\n",
        "    pr_auc = float(auc(rec, prec))\n",
        "\n",
        "    # Save ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
        "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Binary ROC\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"binary_ROC.png\", dpi=150); plt.close()\n",
        "\n",
        "    # Save PR\n",
        "    plt.figure()\n",
        "    plt.plot(rec, prec, label=f\"PR AUC={pr_auc:.3f}\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Binary PR Curve\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"binary_PR.png\", dpi=150); plt.close()\n",
        "\n",
        "    # Waterfall: sort by score desc\n",
        "    order = np.argsort(-scores)\n",
        "    plt.figure()\n",
        "    plt.bar(np.arange(len(scores)), scores[order], edgecolor=\"none\")\n",
        "    # overlay color by true class (blue=NOT_IPV 0, orange=IPV 1)\n",
        "    for idx, o in enumerate(order):\n",
        "        if y_true[o] == 1:\n",
        "            plt.plot(idx, scores[order][idx], marker=\"o\")  # simple overlay\n",
        "    plt.xlabel(\"Samples (sorted by score)\"); plt.ylabel(\"Score\")\n",
        "    plt.title(\"Binary Waterfall (dots mark true IPV=1)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"binary_waterfall.png\", dpi=150); plt.close()\n",
        "\n",
        "    return {\"Accuracy\": acc, \"F1\": f1, \"ROC_AUC\": roc_auc, \"PR_AUC\": pr_auc}\n",
        "\n",
        "def final_evaluation_multilabel(\n",
        "    best_file_path: str | Path,\n",
        "    df_truth_multilabel: pd.DataFrame\n",
        "):\n",
        "    \"\"\"\n",
        "    Use the cached, processed outputs for the best multi-label prompt to:\n",
        "      - Compute per-type ROC/AUC (Physical, Emotional, Sexual)\n",
        "      - Bar chart comparing type-specific AUCs and Macro F1\n",
        "      - Waterfall plot for each subtype (sorted by score if available)\n",
        "    Saves figures to ../1_LLM_Eval/figs.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import roc_curve, auc, f1_score\n",
        "\n",
        "    classes = _one_vs_rest_columns()\n",
        "    info = process_results_file(best_file_path, df_truth_multilabel, task=\"multilabel\")\n",
        "    dfm = info[\"df\"]\n",
        "    out_dir = Path(\"../1_LLM_Eval/figs\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Metrics\n",
        "    f1_macro = float(f1_score(\n",
        "        np.vstack([dfm[f\"y_true_{c}\"] for c in classes]).T,\n",
        "        np.vstack([dfm[f\"y_pred_{c}\"] for c in classes]).T,\n",
        "        average=\"macro\",\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    aucs = {}\n",
        "    for c in classes:\n",
        "        y_true = dfm[f\"y_true_{c}\"].values\n",
        "        s = dfm[f\"y_score_{c}\"]\n",
        "        if s.notna().any():\n",
        "            scores = s.fillna(0.5).values\n",
        "        else:\n",
        "            scores = dfm[f\"y_pred_{c}\"].values.astype(float)  # degenerate but allows plotting\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "        aucs[c] = float(auc(fpr, tpr))\n",
        "\n",
        "        # Save subtype ROC\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, label=f\"AUC={aucs[c]:.3f}\")\n",
        "        plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {c}\")\n",
        "        plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(out_dir / f\"multilabel_ROC_{c}.png\", dpi=150); plt.close()\n",
        "\n",
        "        # Waterfall per subtype\n",
        "        order = np.argsort(-scores)\n",
        "        plt.figure()\n",
        "        plt.bar(np.arange(len(scores)), scores[order], edgecolor=\"none\")\n",
        "        for idx, o in enumerate(order):\n",
        "            if y_true[o] == 1:\n",
        "                plt.plot(idx, scores[order][idx], marker=\"o\")\n",
        "        plt.xlabel(\"Samples (sorted by score)\"); plt.ylabel(\"Score\")\n",
        "        plt.title(f\"Waterfall — {c} (dots mark true {c}=1)\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / f\"multilabel_waterfall_{c}.png\", dpi=150); plt.close()\n",
        "\n",
        "    # Summary bar: AUCs\n",
        "    plt.figure()\n",
        "    xs = np.arange(len(classes))\n",
        "    plt.bar(xs, [aucs[c] for c in classes])\n",
        "    plt.xticks(xs, classes)\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.title(f\"Type-specific AUCs (Macro F1={f1_macro:.3f})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_dir / \"multilabel_type_AUCs.png\", dpi=150); plt.close()\n",
        "\n",
        "    return {\"F1_macro\": f1_macro, **{f\"AUC_{c}\": aucs[c] for c in classes}}\n",
        "\n",
        "# ---------- Example driver (glue) ---------------------------------------------\n",
        "\n",
        "def run_prompt_comparison_and_final(\n",
        "    results_dir=\"../1_LLM_Eval/test_results\",\n",
        "    df_truth_binary: pd.DataFrame = None,\n",
        "    df_truth_multilabel: pd.DataFrame = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Prompt Comparison Phase: compute metrics per prompt, plot comparisons,\n",
        "       identify best prompts for binary and multilabel.\n",
        "    2) Final Evaluation with the best prompt (uses cached processing).\n",
        "    Returns a dict with DataFrames, best names, and final metrics.\n",
        "    \"\"\"\n",
        "    cmp_out = compare_prompts(results_dir, df_truth_binary, df_truth_multilabel)\n",
        "    plot_prompt_comparison(cmp_out[\"binary\"], cmp_out[\"multilabel\"])\n",
        "\n",
        "    # Find the files for best prompts without reprocessing\n",
        "    res_dir = Path(results_dir)\n",
        "    best_bin_name = cmp_out[\"best_binary\"]\n",
        "    best_ml_name  = cmp_out[\"best_multilabel\"]\n",
        "\n",
        "    best_bin_file = None\n",
        "    best_ml_file  = None\n",
        "    if best_bin_name:\n",
        "        # pick the latest file that matches binary_bestBinName_*.json\n",
        "        candidates = sorted(res_dir.glob(f\"binary_{best_bin_name}_*.json\"))\n",
        "        if candidates:\n",
        "            best_bin_file = candidates[-1]\n",
        "    if best_ml_name:\n",
        "        candidates = sorted(res_dir.glob(f\"multilabel_{best_ml_name}_*.json\"))\n",
        "        if candidates:\n",
        "            best_ml_file = candidates[-1]\n",
        "\n",
        "    final_bin = final_ml = None\n",
        "    if best_bin_file:\n",
        "        final_bin = final_evaluation_binary(best_bin_file, df_truth_binary)\n",
        "    if best_ml_file:\n",
        "        final_ml  = final_evaluation_multilabel(best_ml_file, df_truth_multilabel)\n",
        "\n",
        "    return {\n",
        "        \"comparison_binary\": cmp_out[\"binary\"],\n",
        "        \"comparison_multilabel\": cmp_out[\"multilabel\"],\n",
        "        \"best_binary\": best_bin_name,\n",
        "        \"best_multilabel\": best_ml_name,\n",
        "        \"final_binary_metrics\": final_bin,\n",
        "        \"final_multilabel_metrics\": final_ml,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av_O0INERLzg"
      },
      "source": [
        "# Actual Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtyxELITRuK9"
      },
      "source": [
        "**Loading Ground Truth CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o5p_nvL1RMzE"
      },
      "outputs": [],
      "source": [
        "# Load your full dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ECs & Work/Research/617points.csv\")\n",
        "\n",
        "# Ensure there's an 'id' column (if not, create one from index)\n",
        "if \"id\" not in df.columns:\n",
        "    df[\"id\"] = df.index\n",
        "\n",
        "# ---------------------------\n",
        "# 1️⃣ Binary ground truth\n",
        "# ---------------------------\n",
        "# Define IPV = if ANY of the 3 abuse labels is True\n",
        "df[\"IPV\"] = df[[\"Physical Abuse\", \"Emotional Abuse\", \"Sexual Abuse\"]].any(axis=1)\n",
        "\n",
        "df_truth_binary = df[[\"id\", \"IPV\"]].copy()\n",
        "df_truth_binary[\"label\"] = df_truth_binary[\"IPV\"].map({True: \"IPV\", False: \"NOT_IPV\"})\n",
        "df_truth_binary = df_truth_binary[[\"id\", \"label\"]]  # reorder columns\n",
        "\n",
        "# ---------------------------\n",
        "# 2️⃣ Multilabel ground truth\n",
        "# ---------------------------\n",
        "df_truth_multilabel = df.rename(\n",
        "    columns={\n",
        "        \"Physical Abuse\": \"Physical\",\n",
        "        \"Emotional Abuse\": \"Emotional\",\n",
        "        \"Sexual Abuse\": \"Sexual\"\n",
        "    }\n",
        ")[[\"id\", \"Physical\", \"Emotional\", \"Sexual\"]].copy()\n",
        "\n",
        "# Convert to 0/1\n",
        "df_truth_multilabel = df_truth_multilabel.astype({\n",
        "    \"Physical\": int,\n",
        "    \"Emotional\": int,\n",
        "    \"Sexual\": int\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "NP2aHXNVQhpg",
        "outputId": "b8e0b3b5-48c2-4b45-d827-9f198e2cbfe4"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'F1_macro'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4056300019.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m out = run_prompt_comparison_and_final(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mresults_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ECs/Research\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf_truth_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_truth_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf_truth_multilabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_truth_multilabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/tmp/ipython-input-4206906839.py\u001b[0m in \u001b[0;36mrun_prompt_comparison_and_final\u001b[0;34m(results_dir, df_truth_binary, df_truth_multilabel)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mDataFrames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mcmp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_truth_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_truth_multilabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0mplot_prompt_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmp_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmp_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4206906839.py\u001b[0m in \u001b[0;36mcompare_prompts\u001b[0;34m(results_dir, df_truth_binary, df_truth_multilabel)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mdf_cmp_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ROC_AUC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mdf_cmp_multilabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F1_macro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ROC_AUC_macro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ExactMatchAcc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mbest_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cmp_binary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf_cmp_binary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7170\u001b[0m             )\n\u001b[1;32m   7171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7172\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7174\u001b[0m             \u001b[0;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'F1_macro'"
          ]
        }
      ],
      "source": [
        "out = run_prompt_comparison_and_final(\n",
        "    results_dir=\"/content/drive/MyDrive/ECs/Research\",\n",
        "    df_truth_binary=df_truth_binary,\n",
        "    df_truth_multilabel=df_truth_multilabel\n",
        ")\n",
        "print(out[\"comparison_binary\"])\n",
        "print(out[\"comparison_multilabel\"])\n",
        "print(\"Best Binary Prompt:\", out[\"best_binary\"])\n",
        "print(\"Best Multilabel Prompt:\", out[\"best_multilabel\"])\n",
        "print(\"Final Binary Metrics:\", out[\"final_binary_metrics\"])\n",
        "print(\"Final Multilabel Metrics:\", out[\"final_multilabel_metrics\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu9QJwSCR1g9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
