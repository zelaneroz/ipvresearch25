{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479420e3",
   "metadata": {},
   "source": [
    "# Week 4.\n",
    "**Deliverables**\n",
    "* Refine prompts for Binary & Multitype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138d41b",
   "metadata": {},
   "source": [
    "## 1. Prompt Library\n",
    "Binary and multitype templates that feed every evaluation routine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce217a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY PROMPTS\n",
    "# (1) Zero-Shot — final (no inline JSON examples)\n",
    "prompt_binary_zeroshot = \"\"\"\n",
    "You are analyzing whether a sentence describes Intimate Partner Violence (IPV).\n",
    "\n",
    "Definition:\n",
    "Intimate Partner Violence includes any form of physical, emotional, or sexual harm\n",
    "inflicted by a partner. This may involve control, manipulation, threats, humiliation,\n",
    "or physical aggression.\n",
    "\n",
    "Decide if the following sentence involves IPV or not.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: a single key \"label\" with value \"IPV\" or \"NOT_IPV\".\n",
    "Print nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (2) Few-Shot — final (no inline JSON examples)\n",
    "prompt_binary_fewshot = \"\"\"\n",
    "You are classifying sentences as describing Intimate Partner Violence (IPV) or NOT_IPV.\n",
    "\n",
    "Definition:\n",
    "IPV occurs when one partner harms or controls another physically, emotionally, or sexually.\n",
    "\n",
    "Here are examples (for guidance only):\n",
    "1. \"He pushed me against the wall and shouted that I’m worthless.\" → IPV\n",
    "2. \"She often insults me and forbids me from seeing my friends.\" → IPV\n",
    "3. \"We argued but both apologized and moved on peacefully.\" → NOT_IPV\n",
    "4. \"He surprised me with flowers after work.\" → NOT_IPV\n",
    "\n",
    "Now analyze this sentence:\n",
    "\"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (3) Chain-of-Thought — final (no inline JSON examples)\n",
    "prompt_binary_cot = \"\"\"\n",
    "You are reasoning step-by-step to decide if a sentence describes Intimate Partner Violence (IPV).\n",
    "\n",
    "Guidelines:\n",
    "- IPV includes threats, coercion, physical harm, humiliation, or emotional manipulation.\n",
    "- NOT_IPV describes healthy, neutral, or unrelated situations.\n",
    "\n",
    "Think internally (do NOT show your reasoning) about:\n",
    "1. Does the sentence show any behavior that causes harm, fear, or control?\n",
    "2. Is there a partner/relationship context?\n",
    "3. Does it express affection or support instead of harm?\n",
    "\n",
    "After thinking silently, return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (4) Meta — final (no inline JSON examples)\n",
    "prompt_binary_meta = \"\"\"\n",
    "You are a social-behavioral analyst evaluating sentences for signs of Intimate Partner Violence (IPV).\n",
    "\n",
    "Your objective is to be accurate but cautious.\n",
    "- If the sentence clearly involves harm, coercion, or control → label as IPV.\n",
    "- If the sentence shows affection, neutrality, or uncertainty → label as NOT_IPV.\n",
    "\n",
    "Reflect internally before answering; do NOT print your reasoning.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (5) Self-Consistency — final (no inline JSON examples)\n",
    "prompt_binary_selfconsistency = \"\"\"\n",
    "You will internally evaluate the sentence for Intimate Partner Violence (IPV) multiple times\n",
    "and choose the majority label as your final answer.\n",
    "\n",
    "Guidelines:\n",
    "- IPV → signs of physical, emotional, or sexual harm, threats, or coercion.\n",
    "- NOT_IPV → supportive, neutral, or unrelated content.\n",
    "\n",
    "Do NOT reveal thoughts or votes.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210e000",
   "metadata": {},
   "source": [
    "### 1.2 Multitype Prompts\n",
    "These templates mirror the binary ones but collect subtype labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09576f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTITYPE PROMPTS\n",
    "prompt_multilabel_zeroshot = \"\"\"\n",
    "You are identifying which forms of Intimate Partner Violence (IPV) appear in a sentence.\n",
    "\n",
    "Decide independently for emotional, physical, and sexual abuse.\n",
    "Return ONLY one JSON object enclosed between <json> and </json> with the keys\n",
    "'id', 'emotional', 'physical', and 'sexual'.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_fewshot = \"\"\"\n",
    "You are labeling sentences for types of Intimate Partner Violence (IPV).\n",
    "Use the examples to stay calibrated. For each category (emotional, physical, sexual), output 1 if it is clearly present, else 0.\n",
    "\n",
    "Examples:\n",
    "1. \"He insults me daily and forbids me from leaving the house.\" -> emotional: 1, physical: 0, sexual: 0\n",
    "2. \"She slapped me when I disagreed with her.\" -> emotional: 0, physical: 1, sexual: 0\n",
    "3. \"They pressured me into intimacy when I said no.\" -> emotional: 0, physical: 0, sexual: 1\n",
    "4. \"We spent the evening cooking together peacefully.\" -> emotional: 0, physical: 0, sexual: 0\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_cot = \"\"\"\n",
    "Reason silently about whether emotional, physical, or sexual IPV occurs in the sentence.\n",
    "Use relationship context, threats, coercion, and bodily harm cues.\n",
    "After your hidden reasoning, output ONLY the JSON block specified below.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_meta = \"\"\"\n",
    "Act as a cautious social-behavioral analyst.\n",
    "Label a subtype as 1 only when the text clearly shows that form of IPV; otherwise return 0.\n",
    "Favor precision to avoid false positives.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_selfconsistency = \"\"\"\n",
    "Independently evaluate the sentence multiple times to reduce uncertainty.\n",
    "After internal self-consistency voting, output the majority decision for each subtype in the JSON schema below.\n",
    "Do not reveal the intermediate thoughts.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb692028",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. System & Model Setup\n",
    "Shared imports, paths, and model objects. Execute once per runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f65f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "DATASET_PATH = Path(\"../Dataset/617points.csv\")\n",
    "RESULTS_ROOT = Path(\"test_results_raw\")\n",
    "BINARY_RESULTS_DIR = RESULTS_ROOT / \"binary\"\n",
    "MULTITYPE_RESULTS_DIR = RESULTS_ROOT / \"multitype\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e43c8",
   "metadata": {},
   "source": [
    "## 3. Prediction Generation\n",
    "Run the Colab cloning cell first, then load the dataset and choose either binary or multitype generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ed7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone from git\n",
    "!git clone https://github.com/zelaneroz/ipvresearch25\n",
    "%cd ipvresearch25/1_LLM_Eval\n",
    "#Load dataset\n",
    "filename = \"../Dataset/617points.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3947348",
   "metadata": {},
   "source": [
    "### 3.0 Data Access & Directories\n",
    "Loads the dataset and prepares local folders for saving model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8905ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset + result folders\n",
    "BINARY_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MULTITYPE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}. Run the git clone cell above or update DATASET_PATH.\")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Loaded {len(df)} rows from {DATASET_PATH}\")\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt registries for downstream loops\n",
    "binary_prompts = {\n",
    "    \"zeroshot\": prompt_binary_zeroshot,\n",
    "    \"fewshot\": prompt_binary_fewshot,\n",
    "    \"cot\": prompt_binary_cot,\n",
    "    \"meta\": prompt_binary_meta,\n",
    "    \"selfconsistency\": prompt_binary_selfconsistency,\n",
    "}\n",
    "\n",
    "multilabel_prompts = {\n",
    "    \"zeroshot\": prompt_multilabel_zeroshot,\n",
    "    \"fewshot\": prompt_multilabel_fewshot,\n",
    "    \"cot\": prompt_multilabel_cot,\n",
    "    \"meta\": prompt_multilabel_meta,\n",
    "    \"selfconsistency\": prompt_multilabel_selfconsistency,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba5ebf",
   "metadata": {},
   "source": [
    "### 3.1 Binary Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_binary_prompts(df: pd.DataFrame, path: Path, n_samples: int = 3) -> None:\n",
    "    \"\"\"Run all binary prompt types on the first `n_samples` rows and persist outputs.\"\"\"\n",
    "    import re\n",
    "\n",
    "    df_subset = df.head(n_samples)\n",
    "    results_dir = Path(path)\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Running binary classification tests...\")\n",
    "    print(f\"Number of samples: {len(df_subset)}\")\n",
    "    print(f\"Results will be saved in: {results_dir}\n",
    "\")\n",
    "\n",
    "    for prompt_type, template in binary_prompts.items():\n",
    "        print(f\"Testing prompt type: {prompt_type}\")\n",
    "        records = []\n",
    "\n",
    "        for i, row in df_subset.iterrows():\n",
    "            text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
    "            prompt_text = template.replace(\"{text}\", text)\n",
    "\n",
    "            try:\n",
    "                inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=128,\n",
    "                    temperature=0.0,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "                gen_tokens = output[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "                result_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "            except Exception as exc:\n",
    "                result_text = f\"ERROR: {exc}\"\n",
    "\n",
    "            label = None\n",
    "            match = re.search(r\"<json[^>]*>\\s*(.*?)\\s*</json>\", result_text, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                block = match.group(1).strip()\n",
    "                try:\n",
    "                    parsed = json.loads(block)\n",
    "                    if isinstance(parsed, dict):\n",
    "                        label = parsed.get(\"label\") or parsed.get(\"labels\")\n",
    "                    elif isinstance(parsed, list) and parsed:\n",
    "                        label = parsed[0]\n",
    "                    elif isinstance(parsed, str):\n",
    "                        label = parsed.strip()\n",
    "                except json.JSONDecodeError:\n",
    "                    if \"NOT_IPV\" in block.upper():\n",
    "                        label = \"NOT_IPV\"\n",
    "                    elif \"IPV\" in block.upper():\n",
    "                        label = \"IPV\"\n",
    "            else:\n",
    "                if \"NOT_IPV\" in result_text.upper():\n",
    "                    label = \"NOT_IPV\"\n",
    "                elif \"IPV\" in result_text.upper():\n",
    "                    label = \"IPV\"\n",
    "\n",
    "            if label is None:\n",
    "                label = \"UNKNOWN\"\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"id\": int(i),\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"extracted_label\": label,\n",
    "                    \"raw_response\": result_text,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        output_path = results_dir / f\"binary_{prompt_type}.json\"\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(records, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Saved results for '{prompt_type}' to {output_path}\")\n",
    "\n",
    "    print(\"\n",
    "All binary prompt tests completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary generation runner (toggle RUN_BINARY to execute)\n",
    "RUN_BINARY = False\n",
    "BINARY_SAMPLE_COUNT = 5\n",
    "binary_run_dir = BINARY_RESULTS_DIR / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if RUN_BINARY:\n",
    "    if 'df' not in globals():\n",
    "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
    "    test_binary_prompts(df, path=binary_run_dir, n_samples=BINARY_SAMPLE_COUNT)\n",
    "else:\n",
    "    print(\"Binary generation skipped. Set RUN_BINARY = True to execute.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4db5db",
   "metadata": {},
   "source": [
    "### 3.2 Multitype Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528df897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Stage 1: Classification ----------\n",
    "def multitype_predict(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, int]:\n",
    "    template = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot)\n",
    "    prompt = template.format(text=sentence, sample_id=sample_id or 0)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    try:\n",
    "        return json.loads(decoded)\n",
    "    except Exception:\n",
    "        import re\n",
    "        match = re.search(r\"\\{.*\\}\", decoded, re.DOTALL)\n",
    "        return json.loads(match.group()) if match else {\"emotional\": 0, \"physical\": 0, \"sexual\": 0}\n",
    "\n",
    "\n",
    "# ---------- Stage 2: Confidence ----------\n",
    "def logprob_confidence(prompt: str, generated_text: str) -> float:\n",
    "    tokens = tokenizer(prompt + generated_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "    input_ids = tokens[\"input_ids\"][0]\n",
    "\n",
    "    logp_sum = 0.0\n",
    "    count = 0\n",
    "    for idx in range(1, len(input_ids)):\n",
    "        token_id = input_ids[idx]\n",
    "        logp_sum += log_probs[0, idx - 1, token_id].item()\n",
    "        count += 1\n",
    "    avg_logp = logp_sum / max(1, count)\n",
    "    confidence = math.exp(avg_logp)\n",
    "    return float(max(0.0, min(1.0, confidence)))\n",
    "\n",
    "\n",
    "# ---------- Combined Function ----------\n",
    "def multitype_classify(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, float]:\n",
    "    pred = multitype_predict(sentence, sample_id=sample_id, prompt_key=prompt_key)\n",
    "    classification_prompt = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot).format(\n",
    "        text=sentence,\n",
    "        sample_id=sample_id or 0,\n",
    "    )\n",
    "    output_str = json.dumps(pred)\n",
    "    conf = logprob_confidence(classification_prompt, output_str)\n",
    "    pred[\"confidence_score\"] = round(conf, 4)\n",
    "    pred[\"id\"] = sample_id or 0\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multitype generation runner (toggle RUN_MULTITYPE to execute)\n",
    "RUN_MULTITYPE = False\n",
    "MULTITYPE_SAMPLE_COUNT = 5\n",
    "MULTITYPE_PROMPT_KEY = \"zeroshot\"\n",
    "multitype_outputs = []\n",
    "\n",
    "if RUN_MULTITYPE:\n",
    "    if 'df' not in globals():\n",
    "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
    "    subset = df.head(MULTITYPE_SAMPLE_COUNT)\n",
    "    for idx, row in subset.iterrows():\n",
    "        text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
    "        multitype_outputs.append(\n",
    "            multitype_classify(text, sample_id=int(idx), prompt_key=MULTITYPE_PROMPT_KEY)\n",
    "        )\n",
    "    print(f\"Generated {len(multitype_outputs)} multitype predictions using '{MULTITYPE_PROMPT_KEY}'.\")\n",
    "else:\n",
    "    print(\"Multitype generation skipped. Set RUN_MULTITPE = True to execute.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de86ab",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "Summaries, metrics, and visual diagnostics. Load the JSON artifacts generated above and feed them into the eval pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: list recently generated result files\n",
    "from glob import glob\n",
    "\n",
    "def list_result_files(root: Path, pattern: str = \"*.json\", limit: int = 10):\n",
    "    files = sorted(root.rglob(pattern))[-limit:]\n",
    "    if not files:\n",
    "        print(f\"No files found under {root}\")\n",
    "        return\n",
    "    for file in files:\n",
    "        print(f\"- {file.relative_to(Path.cwd())}\")\n",
    "\n",
    "print(\"Recent binary result files:\")\n",
    "list_result_files(BINARY_RESULTS_DIR)\n",
    "print(\"\\nRecent multitype result files:\")\n",
    "list_result_files(MULTITYPE_RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3a4d6",
   "metadata": {},
   "source": [
    "### 4.1 Import Evaluation Functions\n",
    "Import the evaluation and visualization functions from eval_llm_pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from eval_llm_pipeline import (\n",
    "    compute_binary_metrics_detailed,\n",
    "    compute_multitype_metrics_per_subgroup,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve_binary,\n",
    "    plot_precision_recall_curve_binary,\n",
    "    plot_per_class_f1_bar_chart,\n",
    "    append_binary_results_to_json,\n",
    "    append_multitype_results_to_json,\n",
    ")\n",
    "\n",
    "# Paths for JSON results\n",
    "BINARY_JSON_PATH = Path(\"results/binary_results.json\")\n",
    "MULTITYPE_JSON_PATH = Path(\"results/multitype_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e5d67",
   "metadata": {},
   "source": [
    "### 4.2 Binary Evaluation\n",
    "Evaluate binary predictions with detailed metrics and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf144d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load binary predictions and ground truth\n",
    "# Replace with your actual data loading logic\n",
    "# This assumes you have:\n",
    "# - predictions_df: DataFrame with 'id', 'extracted_label' columns\n",
    "# - ground_truth_df: DataFrame with 'id', 'IPV' or 'label' column\n",
    "\n",
    "# Example setup (modify based on your actual data structure):\n",
    "# Load a binary result file\n",
    "BINARY_RESULT_FILE = None  # Set to your binary result JSON file path\n",
    "PROMPT_TYPE = \"zeroshot\"  # Set to the prompt type you want to evaluate\n",
    "MODEL_NAME = \"qwen2.5-7b\"\n",
    "PROMPT_VERSION = PROMPT_TYPE\n",
    "\n",
    "# If you have a result file, load it:\n",
    "if BINARY_RESULT_FILE and Path(BINARY_RESULT_FILE).exists():\n",
    "    with open(BINARY_RESULT_FILE, 'r') as f:\n",
    "        predictions_data = json.load(f)\n",
    "    predictions_df = pd.DataFrame(predictions_data)\n",
    "    \n",
    "    # Merge with ground truth\n",
    "    # Assuming df has 'IPV' column or can be converted to binary labels\n",
    "    merged_df = df.merge(predictions_df, on='id', how='inner')\n",
    "    \n",
    "    # Convert labels to binary (1 for IPV, 0 for NOT_IPV)\n",
    "    y_true = (merged_df['IPV'].astype(int) if 'IPV' in merged_df.columns \n",
    "              else (merged_df['label'].str.upper() == 'IPV').astype(int))\n",
    "    y_pred = (merged_df['extracted_label'].str.upper() == 'IPV').astype(int)\n",
    "    \n",
    "    # Compute detailed binary metrics\n",
    "    binary_metrics = compute_binary_metrics_detailed(y_true, y_pred)\n",
    "    \n",
    "    # Display metrics table\n",
    "    metrics_table = pd.DataFrame([binary_metrics]).T\n",
    "    metrics_table.columns = ['Value']\n",
    "    print(\"=== Binary Classification Metrics ===\")\n",
    "    print(metrics_table)\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plot_confusion_matrix(\n",
    "        y_true, y_pred,\n",
    "        ax=axes[0, 0],\n",
    "        title=\"Binary Confusion Matrix\",\n",
    "        xlabel=\"Predicted\",\n",
    "        ylabel=\"Actual\",\n",
    "        class_names=[\"NOT_IPV\", \"IPV\"]\n",
    "    )\n",
    "    \n",
    "    # ROC Curve (if you have scores, otherwise use predictions as scores)\n",
    "    y_scores = y_pred.astype(float)  # Use predictions as scores if no confidence scores available\n",
    "    plot_roc_curve_binary(\n",
    "        y_true, y_scores,\n",
    "        ax=axes[0, 1],\n",
    "        title=\"ROC Curve (Binary)\",\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        label=f\"{MODEL_NAME} ({PROMPT_TYPE})\"\n",
    "    )\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    plot_precision_recall_curve_binary(\n",
    "        y_true, y_scores,\n",
    "        ax=axes[1, 0],\n",
    "        title=\"Precision-Recall Curve (Binary)\",\n",
    "        xlabel=\"Recall\",\n",
    "        ylabel=\"Precision\",\n",
    "        label=f\"{MODEL_NAME} ({PROMPT_TYPE})\"\n",
    "    )\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Append to JSON\n",
    "    append_binary_results_to_json(\n",
    "        json_path=BINARY_JSON_PATH,\n",
    "        model_name=MODEL_NAME,\n",
    "        prompt_version=PROMPT_VERSION,\n",
    "        metrics=binary_metrics,\n",
    "        notes=f\"Evaluation of {PROMPT_TYPE} prompt\"\n",
    "    )\n",
    "    print(f\"\\nResults appended to {BINARY_JSON_PATH}\")\n",
    "else:\n",
    "    print(\"Set BINARY_RESULT_FILE to a valid path to run binary evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb680a",
   "metadata": {},
   "source": [
    "### 4.3 Multitype Evaluation\n",
    "Evaluate multitype predictions per subgroup with detailed metrics and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load multitype predictions and ground truth\n",
    "# Replace with your actual data loading logic\n",
    "# This assumes you have:\n",
    "# - predictions_df: DataFrame with 'id', 'emotional', 'physical', 'sexual' columns\n",
    "# - ground_truth_df: DataFrame with 'id', 'Physical', 'Emotional', 'Sexual' columns\n",
    "# - Optional: subgroup column (e.g., 'gender', 'age_group')\n",
    "\n",
    "# Example setup (modify based on your actual data structure):\n",
    "MULTITYPE_RESULT_FILE = None  # Set to your multitype result JSON file path\n",
    "MULTITYPE_PROMPT_TYPE = \"zeroshot\"  # Set to the prompt type you want to evaluate\n",
    "MULTITYPE_MODEL_NAME = \"qwen2.5-7b\"\n",
    "MULTITYPE_PROMPT_VERSION = MULTITYPE_PROMPT_TYPE\n",
    "\n",
    "# If you have a result file, load it:\n",
    "if MULTITYPE_RESULT_FILE and Path(MULTITYPE_RESULT_FILE).exists():\n",
    "    with open(MULTITYPE_RESULT_FILE, 'r') as f:\n",
    "        predictions_data = json.load(f)\n",
    "    predictions_df = pd.DataFrame(predictions_data)\n",
    "    \n",
    "    # Merge with ground truth\n",
    "    # Assuming df has 'Physical', 'Emotional', 'Sexual' columns (or 'Physical Abuse', etc.)\n",
    "    merged_df = df.merge(predictions_df, on='id', how='inner')\n",
    "    \n",
    "    # Map column names if needed\n",
    "    true_cols = []\n",
    "    pred_cols = []\n",
    "    for label in ['Physical', 'Emotional', 'Sexual']:\n",
    "        # Try different possible column names\n",
    "        true_col = None\n",
    "        for col in [label, f\"{label} Abuse\"]:\n",
    "            if col in merged_df.columns:\n",
    "                true_col = col\n",
    "                break\n",
    "        if true_col:\n",
    "            true_cols.append(true_col)\n",
    "            # Prediction columns might be lowercase\n",
    "            pred_col = label.lower() if label.lower() in merged_df.columns else label\n",
    "            if pred_col in merged_df.columns:\n",
    "                pred_cols.append(pred_col)\n",
    "            else:\n",
    "                pred_cols.append(None)\n",
    "    \n",
    "    # Filter out None columns\n",
    "    valid_pairs = [(t, p) for t, p in zip(true_cols, pred_cols) if p is not None]\n",
    "    if valid_pairs:\n",
    "        true_cols, pred_cols = zip(*valid_pairs)\n",
    "        true_cols, pred_cols = list(true_cols), list(pred_cols)\n",
    "        \n",
    "        # Optional: subgroup column (e.g., 'gender', 'age_group')\n",
    "        subgroup_col = None  # Set to column name if you want per-subgroup metrics\n",
    "        \n",
    "        # Compute multitype metrics per subgroup\n",
    "        multitype_metrics = compute_multitype_metrics_per_subgroup(\n",
    "            df=merged_df,\n",
    "            y_true_cols=true_cols,\n",
    "            y_pred_cols=pred_cols,\n",
    "            subgroup_col=subgroup_col\n",
    "        )\n",
    "        \n",
    "        # Display metrics table\n",
    "        print(\"=== Multitype Classification Metrics (Per Subgroup) ===\")\n",
    "        for subgroup, classes in multitype_metrics.items():\n",
    "            print(f\"\\n--- Subgroup: {subgroup} ---\")\n",
    "            metrics_list = []\n",
    "            for class_name, metrics in classes.items():\n",
    "                row = {'Class': class_name, **metrics}\n",
    "                metrics_list.append(row)\n",
    "            metrics_table = pd.DataFrame(metrics_list)\n",
    "            print(metrics_table.to_string(index=False))\n",
    "        \n",
    "        # Visualizations\n",
    "        # Get overall metrics for visualization (or first subgroup)\n",
    "        overall_key = \"overall\" if \"overall\" in multitype_metrics else list(multitype_metrics.keys())[0]\n",
    "        overall_metrics = multitype_metrics[overall_key]\n",
    "        \n",
    "        # Create figure with subplots for confusion matrices\n",
    "        n_classes = len(overall_metrics)\n",
    "        fig, axes = plt.subplots(1, n_classes, figsize=(6 * n_classes, 5))\n",
    "        if n_classes == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Confusion matrices for each class\n",
    "        for idx, (class_name, class_metrics) in enumerate(overall_metrics.items()):\n",
    "            # Get true and predicted for this class\n",
    "            y_true_class = merged_df[true_cols[idx]].astype(int).values\n",
    "            y_pred_class = merged_df[pred_cols[idx]].astype(int).values\n",
    "            \n",
    "            plot_confusion_matrix(\n",
    "                y_true_class, y_pred_class,\n",
    "                ax=axes[idx],\n",
    "                title=f\"Confusion Matrix - {class_name}\",\n",
    "                xlabel=\"Predicted\",\n",
    "                ylabel=\"Actual\",\n",
    "                class_names=[\"Not Present\", \"Present\"]\n",
    "            )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Per-Class F1 Bar Chart (separate figure)\n",
    "        fig_f1, ax_f1 = plt.subplots(figsize=(8, 5))\n",
    "        plot_per_class_f1_bar_chart(\n",
    "            overall_metrics,\n",
    "            ax=ax_f1,\n",
    "            title=\"Per-Class F1 Scores (Multitype)\",\n",
    "            xlabel=\"Class\",\n",
    "            ylabel=\"F1 Score\"\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Append to JSON\n",
    "        append_multitype_results_to_json(\n",
    "            json_path=MULTITYPE_JSON_PATH,\n",
    "            model_name=MULTITYPE_MODEL_NAME,\n",
    "            prompt_version=MULTITYPE_PROMPT_VERSION,\n",
    "            metrics_per_subgroup=multitype_metrics,\n",
    "            notes=f\"Evaluation of {MULTITYPE_PROMPT_TYPE} prompt\"\n",
    "        )\n",
    "        print(f\"\\nResults appended to {MULTITYPE_JSON_PATH}\")\n",
    "    else:\n",
    "        print(\"Could not find matching true/prediction columns. Check column names.\")\n",
    "else:\n",
    "    print(\"Set MULTITYPE_RESULT_FILE to a valid path to run multitype evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
