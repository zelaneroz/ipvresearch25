{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479420e3",
   "metadata": {},
   "source": [
    "# Week 4.\n",
    "**Deliverables**\n",
    "* Refine prompts for Binary & Multitype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138d41b",
   "metadata": {},
   "source": [
    "## 1. Prompt Library\n",
    "Binary and multitype templates that feed every evaluation routine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce217a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY PROMPTS\n",
    "# (1) Zero-Shot — final (no inline JSON examples)\n",
    "prompt_binary_zeroshot = \"\"\"\n",
    "You are analyzing whether a sentence describes Intimate Partner Violence (IPV).\n",
    "\n",
    "Definition:\n",
    "Intimate Partner Violence includes any form of physical, emotional, or sexual harm\n",
    "inflicted by a partner. This may involve control, manipulation, threats, humiliation,\n",
    "or physical aggression.\n",
    "\n",
    "Decide if the following sentence involves IPV or not.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: a single key \"label\" with value \"IPV\" or \"NOT_IPV\".\n",
    "Print nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (2) Few-Shot — final (no inline JSON examples)\n",
    "prompt_binary_fewshot = \"\"\"\n",
    "You are classifying sentences as describing Intimate Partner Violence (IPV) or NOT_IPV.\n",
    "\n",
    "Definition:\n",
    "IPV occurs when one partner harms or controls another physically, emotionally, or sexually.\n",
    "\n",
    "Here are examples (for guidance only):\n",
    "1. \"He pushed me against the wall and shouted that I’m worthless.\" → IPV\n",
    "2. \"She often insults me and forbids me from seeing my friends.\" → IPV\n",
    "3. \"We argued but both apologized and moved on peacefully.\" → NOT_IPV\n",
    "4. \"He surprised me with flowers after work.\" → NOT_IPV\n",
    "\n",
    "Now analyze this sentence:\n",
    "\"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (3) Chain-of-Thought — final (no inline JSON examples)\n",
    "prompt_binary_cot = \"\"\"\n",
    "You are reasoning step-by-step to decide if a sentence describes Intimate Partner Violence (IPV).\n",
    "\n",
    "Guidelines:\n",
    "- IPV includes threats, coercion, physical harm, humiliation, or emotional manipulation.\n",
    "- NOT_IPV describes healthy, neutral, or unrelated situations.\n",
    "\n",
    "Think internally (do NOT show your reasoning) about:\n",
    "1. Does the sentence show any behavior that causes harm, fear, or control?\n",
    "2. Is there a partner/relationship context?\n",
    "3. Does it express affection or support instead of harm?\n",
    "\n",
    "After thinking silently, return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (4) Meta — final (no inline JSON examples)\n",
    "prompt_binary_meta = \"\"\"\n",
    "You are a social-behavioral analyst evaluating sentences for signs of Intimate Partner Violence (IPV).\n",
    "\n",
    "Your objective is to be accurate but cautious.\n",
    "- If the sentence clearly involves harm, coercion, or control → label as IPV.\n",
    "- If the sentence shows affection, neutrality, or uncertainty → label as NOT_IPV.\n",
    "\n",
    "Reflect internally before answering; do NOT print your reasoning.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# (5) Self-Consistency — final (no inline JSON examples)\n",
    "prompt_binary_selfconsistency = \"\"\"\n",
    "You will internally evaluate the sentence for Intimate Partner Violence (IPV) multiple times\n",
    "and choose the majority label as your final answer.\n",
    "\n",
    "Guidelines:\n",
    "- IPV → signs of physical, emotional, or sexual harm, threats, or coercion.\n",
    "- NOT_IPV → supportive, neutral, or unrelated content.\n",
    "\n",
    "Do NOT reveal thoughts or votes.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "\n",
    "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
    "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
    "Print nothing else.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210e000",
   "metadata": {},
   "source": [
    "### 1.2 Multitype Prompts\n",
    "These templates mirror the binary ones but collect subtype labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09576f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTITYPE PROMPTS\n",
    "prompt_multilabel_zeroshot = \"\"\"\n",
    "You are identifying which forms of Intimate Partner Violence (IPV) appear in a sentence.\n",
    "\n",
    "Decide independently for emotional, physical, and sexual abuse.\n",
    "Return ONLY one JSON object enclosed between <json> and </json> with the keys\n",
    "'id', 'emotional', 'physical', and 'sexual'.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_fewshot = \"\"\"\n",
    "You are labeling sentences for types of Intimate Partner Violence (IPV).\n",
    "Use the examples to stay calibrated. For each category (emotional, physical, sexual), output 1 if it is clearly present, else 0.\n",
    "\n",
    "Examples:\n",
    "1. \"He insults me daily and forbids me from leaving the house.\" -> emotional: 1, physical: 0, sexual: 0\n",
    "2. \"She slapped me when I disagreed with her.\" -> emotional: 0, physical: 1, sexual: 0\n",
    "3. \"They pressured me into intimacy when I said no.\" -> emotional: 0, physical: 0, sexual: 1\n",
    "4. \"We spent the evening cooking together peacefully.\" -> emotional: 0, physical: 0, sexual: 0\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_cot = \"\"\"\n",
    "Reason silently about whether emotional, physical, or sexual IPV occurs in the sentence.\n",
    "Use relationship context, threats, coercion, and bodily harm cues.\n",
    "After your hidden reasoning, output ONLY the JSON block specified below.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_meta = \"\"\"\n",
    "Act as a cautious social-behavioral analyst.\n",
    "Label a subtype as 1 only when the text clearly shows that form of IPV; otherwise return 0.\n",
    "Favor precision to avoid false positives.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_selfconsistency = \"\"\"\n",
    "Independently evaluate the sentence multiple times to reduce uncertainty.\n",
    "After internal self-consistency voting, output the majority decision for each subtype in the JSON schema below.\n",
    "Do not reveal the intermediate thoughts.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb692028",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. System & Model Setup\n",
    "Shared imports, paths, and model objects. Execute once per runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f65f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "DATASET_PATH = Path(\"../Dataset/617points.csv\")\n",
    "RESULTS_ROOT = Path(\"test_results_raw\")\n",
    "BINARY_RESULTS_DIR = RESULTS_ROOT / \"binary\"\n",
    "MULTITYPE_RESULTS_DIR = RESULTS_ROOT / \"multitype\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e43c8",
   "metadata": {},
   "source": [
    "## 3. Prediction Generation\n",
    "Run the Colab cloning cell first, then load the dataset and choose either binary or multitype generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ed7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone from git\n",
    "!git clone https://github.com/zelaneroz/ipvresearch25\n",
    "%cd ipvresearch25/1_LLM_Eval\n",
    "#Load dataset\n",
    "filename = \"../Dataset/617points.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3947348",
   "metadata": {},
   "source": [
    "### 3.0 Data Access & Directories\n",
    "Loads the dataset and prepares local folders for saving model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8905ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset + result folders\n",
    "BINARY_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MULTITYPE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}. Run the git clone cell above or update DATASET_PATH.\")\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Loaded {len(df)} rows from {DATASET_PATH}\")\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt registries for downstream loops\n",
    "binary_prompts = {\n",
    "    \"zeroshot\": prompt_binary_zeroshot,\n",
    "    \"fewshot\": prompt_binary_fewshot,\n",
    "    \"cot\": prompt_binary_cot,\n",
    "    \"meta\": prompt_binary_meta,\n",
    "    \"selfconsistency\": prompt_binary_selfconsistency,\n",
    "}\n",
    "\n",
    "multilabel_prompts = {\n",
    "    \"zeroshot\": prompt_multilabel_zeroshot,\n",
    "    \"fewshot\": prompt_multilabel_fewshot,\n",
    "    \"cot\": prompt_multilabel_cot,\n",
    "    \"meta\": prompt_multilabel_meta,\n",
    "    \"selfconsistency\": prompt_multilabel_selfconsistency,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba5ebf",
   "metadata": {},
   "source": [
    "### 3.1 Binary Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_binary_prompts(df: pd.DataFrame, path: Path, n_samples: int = 3) -> None:\n",
    "    \"\"\"Run all binary prompt types on the first `n_samples` rows and persist outputs.\"\"\"\n",
    "    import re\n",
    "\n",
    "    df_subset = df.head(n_samples)\n",
    "    results_dir = Path(path)\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Running binary classification tests...\")\n",
    "    print(f\"Number of samples: {len(df_subset)}\")\n",
    "    print(f\"Results will be saved in: {results_dir}\n",
    "\")\n",
    "\n",
    "    for prompt_type, template in binary_prompts.items():\n",
    "        print(f\"Testing prompt type: {prompt_type}\")\n",
    "        records = []\n",
    "\n",
    "        for i, row in df_subset.iterrows():\n",
    "            text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
    "            prompt_text = template.replace(\"{text}\", text)\n",
    "\n",
    "            try:\n",
    "                inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=128,\n",
    "                    temperature=0.0,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "                gen_tokens = output[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "                result_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "            except Exception as exc:\n",
    "                result_text = f\"ERROR: {exc}\"\n",
    "\n",
    "            label = None\n",
    "            match = re.search(r\"<json[^>]*>\\s*(.*?)\\s*</json>\", result_text, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                block = match.group(1).strip()\n",
    "                try:\n",
    "                    parsed = json.loads(block)\n",
    "                    if isinstance(parsed, dict):\n",
    "                        label = parsed.get(\"label\") or parsed.get(\"labels\")\n",
    "                    elif isinstance(parsed, list) and parsed:\n",
    "                        label = parsed[0]\n",
    "                    elif isinstance(parsed, str):\n",
    "                        label = parsed.strip()\n",
    "                except json.JSONDecodeError:\n",
    "                    if \"NOT_IPV\" in block.upper():\n",
    "                        label = \"NOT_IPV\"\n",
    "                    elif \"IPV\" in block.upper():\n",
    "                        label = \"IPV\"\n",
    "            else:\n",
    "                if \"NOT_IPV\" in result_text.upper():\n",
    "                    label = \"NOT_IPV\"\n",
    "                elif \"IPV\" in result_text.upper():\n",
    "                    label = \"IPV\"\n",
    "\n",
    "            if label is None:\n",
    "                label = \"UNKNOWN\"\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"id\": int(i),\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"extracted_label\": label,\n",
    "                    \"raw_response\": result_text,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        output_path = results_dir / f\"binary_{prompt_type}.json\"\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(records, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Saved results for '{prompt_type}' to {output_path}\")\n",
    "\n",
    "    print(\"\n",
    "All binary prompt tests completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary generation runner (toggle RUN_BINARY to execute)\n",
    "RUN_BINARY = False\n",
    "BINARY_SAMPLE_COUNT = 5\n",
    "binary_run_dir = BINARY_RESULTS_DIR / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if RUN_BINARY:\n",
    "    if 'df' not in globals():\n",
    "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
    "    test_binary_prompts(df, path=binary_run_dir, n_samples=BINARY_SAMPLE_COUNT)\n",
    "else:\n",
    "    print(\"Binary generation skipped. Set RUN_BINARY = True to execute.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4db5db",
   "metadata": {},
   "source": [
    "### 3.2 Multitype Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528df897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Stage 1: Classification ----------\n",
    "def multitype_predict(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, int]:\n",
    "    template = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot)\n",
    "    prompt = template.format(text=sentence, sample_id=sample_id or 0)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    try:\n",
    "        return json.loads(decoded)\n",
    "    except Exception:\n",
    "        import re\n",
    "        match = re.search(r\"\\{.*\\}\", decoded, re.DOTALL)\n",
    "        return json.loads(match.group()) if match else {\"emotional\": 0, \"physical\": 0, \"sexual\": 0}\n",
    "\n",
    "\n",
    "# ---------- Stage 2: Confidence ----------\n",
    "def logprob_confidence(prompt: str, generated_text: str) -> float:\n",
    "    tokens = tokenizer(prompt + generated_text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "    input_ids = tokens[\"input_ids\"][0]\n",
    "\n",
    "    logp_sum = 0.0\n",
    "    count = 0\n",
    "    for idx in range(1, len(input_ids)):\n",
    "        token_id = input_ids[idx]\n",
    "        logp_sum += log_probs[0, idx - 1, token_id].item()\n",
    "        count += 1\n",
    "    avg_logp = logp_sum / max(1, count)\n",
    "    confidence = math.exp(avg_logp)\n",
    "    return float(max(0.0, min(1.0, confidence)))\n",
    "\n",
    "\n",
    "# ---------- Combined Function ----------\n",
    "def multitype_classify(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, float]:\n",
    "    pred = multitype_predict(sentence, sample_id=sample_id, prompt_key=prompt_key)\n",
    "    classification_prompt = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot).format(\n",
    "        text=sentence,\n",
    "        sample_id=sample_id or 0,\n",
    "    )\n",
    "    output_str = json.dumps(pred)\n",
    "    conf = logprob_confidence(classification_prompt, output_str)\n",
    "    pred[\"confidence_score\"] = round(conf, 4)\n",
    "    pred[\"id\"] = sample_id or 0\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multitype generation runner (toggle RUN_MULTITYPE to execute)\n",
    "RUN_MULTITYPE = False\n",
    "MULTITYPE_SAMPLE_COUNT = 5\n",
    "MULTITYPE_PROMPT_KEY = \"zeroshot\"\n",
    "multitype_outputs = []\n",
    "\n",
    "if RUN_MULTITYPE:\n",
    "    if 'df' not in globals():\n",
    "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
    "    subset = df.head(MULTITYPE_SAMPLE_COUNT)\n",
    "    for idx, row in subset.iterrows():\n",
    "        text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
    "        multitype_outputs.append(\n",
    "            multitype_classify(text, sample_id=int(idx), prompt_key=MULTITYPE_PROMPT_KEY)\n",
    "        )\n",
    "    print(f\"Generated {len(multitype_outputs)} multitype predictions using '{MULTITYPE_PROMPT_KEY}'.\")\n",
    "else:\n",
    "    print(\"Multitype generation skipped. Set RUN_MULTITPE = True to execute.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de86ab",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "Summaries, metrics, and visual diagnostics. Load the JSON artifacts generated above and feed them into the eval pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: list recently generated result files\n",
    "from glob import glob\n",
    "\n",
    "def list_result_files(root: Path, pattern: str = \"*.json\", limit: int = 10):\n",
    "    files = sorted(root.rglob(pattern))[-limit:]\n",
    "    if not files:\n",
    "        print(f\"No files found under {root}\")\n",
    "        return\n",
    "    for file in files:\n",
    "        print(f\"- {file.relative_to(Path.cwd())}\")\n",
    "\n",
    "print(\"Recent binary result files:\")\n",
    "list_result_files(BINARY_RESULTS_DIR)\n",
    "print(\"\n",
    "Recent multitype result files:\")\n",
    "list_result_files(MULTITYPE_RESULTS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
