{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3b0183",
   "metadata": {},
   "source": [
    "# Cross-Validated Multi-Label IPV Prediction (Qwen)\n",
    "End-to-end notebook for sampling test sets, running Qwen multilabel predictions, computing metrics, and persisting cross-validated runs to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38068045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "DATA_PATH = Path(\"../Dataset/reddit_data.csv\")\n",
    "RESULTS_PATH = Path(\"results_crossval.json\")\n",
    "PROMPT_KEY = \"zeroshot\"\n",
    "N_RUNS = 10\n",
    "MAX_NEW_TOKENS = 128\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac213676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load data and derive label arrays\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "if \"items\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'items' column in the dataset.\")\n",
    "\n",
    "physical = df[\"Physical Abuse\"].astype(bool)\n",
    "emotional = df[\"Emotional Abuse\"].astype(bool)\n",
    "sexual = df[\"Sexual Abuse\"].astype(bool)\n",
    "tag = df[\"Tag\"].astype(bool)\n",
    "notipv = (~tag) & (~physical) & (~emotional) & (~sexual)\n",
    "\n",
    "label_arrays = {\n",
    "    \"physical\": physical.astype(int).to_numpy(),\n",
    "    \"emotional\": emotional.astype(int).to_numpy(),\n",
    "    \"sexual\": sexual.astype(int).to_numpy(),\n",
    "    \"not_ipv\": notipv.astype(int).to_numpy(),\n",
    "}\n",
    "\n",
    "k = int(sexual.sum())\n",
    "print(f\"Loaded {len(df)} rows | sexual positives (k) = {k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Multilabel prompts (copied from w4_qwen.ipynb)\n",
    "\n",
    "prompt_multilabel_zeroshot = \"\"\"\n",
    "You are identifying which forms of Intimate Partner Violence (IPV) appear in a sentence.\n",
    "\n",
    "Decide independently for emotional, physical, and sexual abuse.Decide independently for emotional, physical, and sexual abuse. If it is a particular type of IPV, set emotional, physical, or sexual to 1, otherwise set it to 0. Multiple IPV types can be true or none at all.\n",
    "Return ONLY one JSON object enclosed between <json> and </json> with the keys\n",
    "'id', 'emotional', 'physical', and 'sexual'.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_fewshot = \"\"\"\n",
    "You are labeling sentences for types of Intimate Partner Violence (IPV).\n",
    "Use the examples to stay calibrated. For each category (emotional, physical, sexual), output 1 if it is clearly present, else 0.\n",
    "\n",
    "Examples:\n",
    "1. \"He insults me daily and forbids me from leaving the house.\" -> emotional: 1, physical: 0, sexual: 0\n",
    "2. \"She slapped me when I disagreed with her.\" -> emotional: 0, physical: 1, sexual: 0\n",
    "3. \"They pressured me into intimacy when I said no.\" -> emotional: 0, physical: 0, sexual: 1\n",
    "4. \"We spent the evening cooking together peacefully.\" -> emotional: 0, physical: 0, sexual: 0\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_cot = \"\"\"\n",
    "Reason silently about whether emotional, physical, or sexual IPV occurs in the sentence.\n",
    "Use relationship context, threats, coercion, and bodily harm cues.\n",
    "After your hidden reasoning, output ONLY the JSON block specified below.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_meta = \"\"\"\n",
    "Act as a cautious social-behavioral analyst.\n",
    "Label a subtype as 1 only when the text clearly shows that form of IPV; otherwise return 0.\n",
    "Favor precision to avoid false positives.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_multilabel_selfconsistency = \"\"\"\n",
    "Independently evaluate the sentence multiple times to reduce uncertainty.\n",
    "After internal self-consistency voting, output the majority decision for each subtype in the JSON schema below.\n",
    "Do not reveal the intermediate thoughts.\n",
    "\n",
    "Sentence: \"{text}\"\n",
    "Sample ID: \"{sample_id}\"\n",
    "<json>\n",
    "{{\n",
    "  \"id\": \"{sample_id}\",\n",
    "  \"emotional\": 0 or 1,\n",
    "  \"physical\": 0 or 1,\n",
    "  \"sexual\": 0 or 1\n",
    "}}\n",
    "</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "multilabel_prompts = {\n",
    "    \"zeroshot\": prompt_multilabel_zeroshot,\n",
    "    \"fewshot\": prompt_multilabel_fewshot,\n",
    "    \"cot\": prompt_multilabel_cot,\n",
    "    \"meta\": prompt_multilabel_meta,\n",
    "    \"selfconsistency\": prompt_multilabel_selfconsistency,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b23bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load Qwen model\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Prediction helpers using Qwen\n",
    "\n",
    "def format_prompt(text: str, sample_id: int, prompt_key: str = PROMPT_KEY) -> str:\n",
    "    template = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot)\n",
    "    return template.format(text=text, sample_id=sample_id)\n",
    "\n",
    "def parse_multilabel_response(raw_response: str, sample_id: int) -> Dict[str, int]:\n",
    "    match = re.search(r\"<json[^>]*>(.*?)</json>\", raw_response, re.DOTALL | re.IGNORECASE)\n",
    "    block = match.group(1).strip() if match else raw_response\n",
    "\n",
    "    payload: Dict[str, object] = {}\n",
    "    try:\n",
    "        payload = json.loads(block)\n",
    "    except Exception:\n",
    "        fallback = re.search(r\"\\{.*\\}\", block, re.DOTALL)\n",
    "        payload = json.loads(fallback.group()) if fallback else {}\n",
    "\n",
    "    def normalize_label(value: object) -> int:\n",
    "        if isinstance(value, bool):\n",
    "            return int(value)\n",
    "        try:\n",
    "            return 1 if int(value) == 1 else 0\n",
    "        except Exception:\n",
    "            if isinstance(value, str):\n",
    "                if value.strip().lower() in {\"true\", \"yes\", \"y\", \"ipv\"}:\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    if not isinstance(payload, dict):\n",
    "        payload = {}\n",
    "\n",
    "    return {\n",
    "        \"id\": int(payload.get(\"id\", sample_id)) if isinstance(payload.get(\"id\", sample_id), (int, str)) else sample_id,\n",
    "        \"emotional\": normalize_label(payload.get(\"emotional\", 0)),\n",
    "        \"physical\": normalize_label(payload.get(\"physical\", 0)),\n",
    "        \"sexual\": normalize_label(payload.get(\"sexual\", 0)),\n",
    "    }\n",
    "\n",
    "def qwen_predict(\n",
    "    items: Sequence[str],\n",
    "    sample_ids: Sequence[int] | None = None,\n",
    "    prompt_key: str = PROMPT_KEY,\n",
    "    max_new_tokens: int = MAX_NEW_TOKENS,\n",
    ") -> List[Dict[str, int]]:\n",
    "    if sample_ids is None:\n",
    "        sample_ids = list(range(len(items)))\n",
    "    if len(sample_ids) != len(items):\n",
    "        raise ValueError(\"sample_ids length must match items length\")\n",
    "\n",
    "    predictions: List[Dict[str, int]] = []\n",
    "    for text, sid in zip(items, sample_ids):\n",
    "        prompt = format_prompt(text, sample_id=int(sid), prompt_key=prompt_key)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "            )\n",
    "\n",
    "        generated = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "        decoded = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "        parsed = parse_multilabel_response(decoded, sample_id=int(sid))\n",
    "        parsed[\"raw_response\"] = decoded\n",
    "        predictions.append(parsed)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Sampling, evaluation, and aggregation utilities\n",
    "\n",
    "def compute_confusion_stats(y_true: Sequence[int], y_pred: Sequence[int]) -> Dict[str, float]:\n",
    "    y_true = np.array(y_true, dtype=int)\n",
    "    y_pred = np.array(y_pred, dtype=int)\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Mismatched shapes for ground truth and predictions.\")\n",
    "\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    accuracy = (tp + tn) / len(y_true) if len(y_true) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"TP\": tp,\n",
    "        \"FP\": fp,\n",
    "        \"TN\": tn,\n",
    "        \"FN\": fn,\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"Accuracy\": round(accuracy, 4),\n",
    "    }\n",
    "\n",
    "def sample_test_set(df: pd.DataFrame, k: int, seed: int) -> tuple[pd.DataFrame, List[int]]:\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be > 0 (requires sexual positives present in the dataset).\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    sexual_df = df[df[\"Sexual Abuse\"].astype(bool)]\n",
    "    emotional_df = df[df[\"Emotional Abuse\"].astype(bool)]\n",
    "    physical_df = df[df[\"Physical Abuse\"].astype(bool)]\n",
    "    negative_mask = (\n",
    "        (df[\"Tag\"] == False)\n",
    "        & (df[\"Physical Abuse\"] == False)\n",
    "        & (df[\"Emotional Abuse\"] == False)\n",
    "        & (df[\"Sexual Abuse\"] == False)\n",
    "    )\n",
    "    negative_df = df[negative_mask]\n",
    "\n",
    "    emotional_sample = emotional_df.sample(\n",
    "        n=min(k, len(emotional_df)),\n",
    "        random_state=seed,\n",
    "        replace=False,\n",
    "    )\n",
    "    physical_sample = physical_df.sample(\n",
    "        n=min(k, len(physical_df)),\n",
    "        random_state=seed,\n",
    "        replace=False,\n",
    "    )\n",
    "\n",
    "    neg_count = int(rng.integers(3 * k, 5 * k + 1))\n",
    "    negative_sample = negative_df.sample(\n",
    "        n=min(neg_count, len(negative_df)),\n",
    "        random_state=seed,\n",
    "        replace=False,\n",
    "    )\n",
    "\n",
    "    combined = pd.concat(\n",
    "        [sexual_df, emotional_sample, physical_sample, negative_sample],\n",
    "        axis=0,\n",
    "    )\n",
    "    combined = combined.loc[~combined[\"items\"].duplicated(keep=\"first\")]\n",
    "    return combined, combined.index.tolist()\n",
    "\n",
    "def evaluate_run(test_df: pd.DataFrame, predictions: List[Dict[str, int]]) -> Dict[str, Dict[str, float]]:\n",
    "    if len(test_df) != len(predictions):\n",
    "        raise ValueError(\"Number of predictions must match the test set size.\")\n",
    "\n",
    "    truth = {\n",
    "        \"physical\": test_df[\"Physical Abuse\"].astype(int).to_numpy(),\n",
    "        \"emotional\": test_df[\"Emotional Abuse\"].astype(int).to_numpy(),\n",
    "        \"sexual\": test_df[\"Sexual Abuse\"].astype(int).to_numpy(),\n",
    "    }\n",
    "    truth[\"not_ipv\"] = (\n",
    "        (test_df[\"Tag\"] == False)\n",
    "        & (test_df[\"Physical Abuse\"] == False)\n",
    "        & (test_df[\"Emotional Abuse\"] == False)\n",
    "        & (test_df[\"Sexual Abuse\"] == False)\n",
    "    ).astype(int).to_numpy()\n",
    "\n",
    "    pred_physical = np.array([int(bool(pred.get(\"physical\", 0))) for pred in predictions])\n",
    "    pred_emotional = np.array([int(bool(pred.get(\"emotional\", 0))) for pred in predictions])\n",
    "    pred_sexual = np.array([int(bool(pred.get(\"sexual\", 0))) for pred in predictions])\n",
    "    pred_not_ipv = ((pred_physical == 0) & (pred_emotional == 0) & (pred_sexual == 0)).astype(int)\n",
    "\n",
    "    preds = {\n",
    "        \"physical\": pred_physical,\n",
    "        \"emotional\": pred_emotional,\n",
    "        \"sexual\": pred_sexual,\n",
    "        \"not_ipv\": pred_not_ipv,\n",
    "    }\n",
    "\n",
    "    metrics: Dict[str, Dict[str, float]] = {}\n",
    "    for label in [\"physical\", \"emotional\", \"sexual\", \"not_ipv\"]:\n",
    "        metrics[label] = compute_confusion_stats(truth[label], preds[label])\n",
    "    return metrics\n",
    "\n",
    "def compute_aggregate_metrics(runs: List[Dict[str, object]]) -> Dict[str, Dict[str, float]]:\n",
    "    if not runs:\n",
    "        return {}\n",
    "\n",
    "    labels = [\"physical\", \"emotional\", \"sexual\", \"not_ipv\"]\n",
    "    aggregate: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "    for label in labels:\n",
    "        tp_vals = [int(run[\"labels\"][label][\"TP\"]) for run in runs]\n",
    "        fp_vals = [int(run[\"labels\"][label][\"FP\"]) for run in runs]\n",
    "        tn_vals = [int(run[\"labels\"][label][\"TN\"]) for run in runs]\n",
    "        fn_vals = [int(run[\"labels\"][label][\"FN\"]) for run in runs]\n",
    "        precision_vals = [float(run[\"labels\"][label][\"Precision\"]) for run in runs]\n",
    "        recall_vals = [float(run[\"labels\"][label][\"Recall\"]) for run in runs]\n",
    "        accuracy_vals = [float(run[\"labels\"][label][\"Accuracy\"]) for run in runs]\n",
    "\n",
    "        aggregate[label] = {\n",
    "            \"TP\": int(sum(tp_vals)),\n",
    "            \"FP\": int(sum(fp_vals)),\n",
    "            \"TN\": int(sum(tn_vals)),\n",
    "            \"FN\": int(sum(fn_vals)),\n",
    "            \"Precision_mean\": float(np.mean(precision_vals)),\n",
    "            \"Precision_std\": float(np.std(precision_vals)),\n",
    "            \"Recall_mean\": float(np.mean(recall_vals)),\n",
    "            \"Recall_std\": float(np.std(recall_vals)),\n",
    "            \"Accuracy_mean\": float(np.mean(accuracy_vals)),\n",
    "            \"Accuracy_std\": float(np.std(accuracy_vals)),\n",
    "        }\n",
    "\n",
    "    return aggregate\n",
    "\n",
    "def run_evaluation(\n",
    "    N: int = N_RUNS,\n",
    "    start_run_id: int = 1,\n",
    "    prompt_key: str = PROMPT_KEY,\n",
    "    base_seed: int | None = None,\n",
    ") -> List[Dict[str, object]]:\n",
    "    k_local = int(df[\"Sexual Abuse\"].astype(bool).sum())\n",
    "    rng = np.random.default_rng(base_seed or int(datetime.now().timestamp()))\n",
    "\n",
    "    results: List[Dict[str, object]] = []\n",
    "    for i in range(N):\n",
    "        seed = int(rng.integers(0, 1_000_000_000))\n",
    "        test_df, sample_indices = sample_test_set(df, k=k_local, seed=seed)\n",
    "        preds = qwen_predict(\n",
    "            test_df[\"items\"].tolist(),\n",
    "            sample_ids=test_df.index.tolist(),\n",
    "            prompt_key=prompt_key,\n",
    "        )\n",
    "        label_metrics = evaluate_run(test_df, preds)\n",
    "        results.append(\n",
    "            {\n",
    "                \"run_id\": int(start_run_id + i),\n",
    "                \"seed\": seed,\n",
    "                \"n_samples\": int(len(test_df)),\n",
    "                \"labels\": label_metrics,\n",
    "                \"samples_used_indices\": [int(idx) for idx in sample_indices],\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "def ensure_results_file(path: Path = RESULTS_PATH) -> Dict[str, object]:\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            return json.load(fp)\n",
    "    return {\"model_name\": \"qwen_ipv_classifier\", \"runs\": [], \"aggregate\": {}}\n",
    "\n",
    "def save_results_to_json(run_results: List[Dict[str, object]], path: Path = RESULTS_PATH) -> Dict[str, object]:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    data = ensure_results_file(path)\n",
    "    data.setdefault(\"model_name\", \"qwen_ipv_classifier\")\n",
    "    data.setdefault(\"runs\", [])\n",
    "    data.setdefault(\"aggregate\", {})\n",
    "\n",
    "    data[\"runs\"].extend(run_results)\n",
    "    data[\"aggregate\"] = compute_aggregate_metrics(data[\"runs\"])\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(data, fp, indent=2)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Execute cross-validated evaluation and persist results\n",
    "\n",
    "existing = ensure_results_file(RESULTS_PATH)\n",
    "start_id = len(existing.get(\"runs\", [])) + 1\n",
    "print(f\"Existing runs: {start_id - 1}. Starting new runs at id {start_id}.\")\n",
    "\n",
    "new_runs = run_evaluation(N=N_RUNS, start_run_id=start_id, prompt_key=PROMPT_KEY)\n",
    "results_json = save_results_to_json(new_runs, path=RESULTS_PATH)\n",
    "\n",
    "print(f\"Saved {len(new_runs)} runs to {RESULTS_PATH.resolve()}\")\n",
    "print(\"Aggregate metrics (across all stored runs):\")\n",
    "print(json.dumps(results_json.get(\"aggregate\", {}), indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
