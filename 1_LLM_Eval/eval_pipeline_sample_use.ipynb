{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3480a6",
   "metadata": {},
   "source": [
    "# `eval_llm_pipeline` Sample Usage\n\n",
    "This notebook shows how to wire `eval_llm_pipeline.LLMEvalPipeline` into future experiments so metrics, ROC/PR curves, and confidence visualizations stay identical across prompts/models.\n",
    "\n",
    "Key takeaways:\n",
    "- Works for both binary IPV detection and multitype runs.\n",
    "- Multitype plots now score *exact-match correctness vs confidence*, mirroring the new pipeline behavior.\n",
    "- All figures/CSVs land in `1_LLM_Eval/test_results/figs/` so you can diff results over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a841419",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "1. Import the pipeline class.\n",
    "2. Point `PROJECT_ROOT` at the repository root.\n",
    "3. Reuse the helper constants (`DATASET_PATH`, `OUTPUT_DIR`) in later cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git config --global user.name \"github username\"\n",
    "# !git config --global user.email \"Email\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ipvresearch25   # optional: clears old copy\n",
    "# !git clone https://github.com/zelaneroz/ipvresearch25.git\n",
    "# %cd ipvresearch25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db682d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATASET_PATH = PROJECT_ROOT / 'Dataset/reddit_data.csv'\n",
    "OUTPUT_DIR = PROJECT_ROOT / '1_LLM_Eval/test_results/figs'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT / '1_LLM_Eval'))\n",
    "from eval_llm_pipeline import LLMEvalPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585076e",
   "metadata": {},
   "source": [
    "## 2. Binary evaluation example\n\n",
    "Below we compare two prompting strategies (`cot` vs `fewshot`). Extend `binary_results` with additional JSON files to stack more experiments on the same plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb2adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "         accuracy  precision      f1  roc_auc\n",
      "model                                        \n",
      "cot        0.8058     0.9598  0.7818   0.8143\n",
      "fewshot    0.8932     0.9745  0.8900   0.8975\n",
      "\n",
      "Saved metrics to /Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/test_results/figs/binary_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cot</th>\n",
       "      <td>0.805825</td>\n",
       "      <td>0.959821</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.814344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fewshot</th>\n",
       "      <td>0.893204</td>\n",
       "      <td>0.974453</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.897523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy  precision        f1   roc_auc\n",
       "model                                           \n",
       "cot      0.805825   0.959821  0.781818  0.814344\n",
       "fewshot  0.893204   0.974453  0.890000  0.897523"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/test_results/figs/binary_precision_residual.png')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_results = {\n",
    "    'cot': PROJECT_ROOT / '1_LLM_Eval/test_results/binary_cot.json',\n",
    "    'fewshot': PROJECT_ROOT / '1_LLM_Eval/test_results/binary_fewshot.json',\n",
    "}\n",
    "\n",
    "binary_pipeline = LLMEvalPipeline(\n",
    "    results_map=binary_results,\n",
    "    ground_truth=DATASET_PATH,\n",
    "    task='binary',\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    positive_label='IPV'\n",
    ")\n",
    "\n",
    "binary_metrics = binary_pipeline.save_and_print_metrics(\n",
    "    OUTPUT_DIR / 'binary_metrics.csv'\n",
    ")\n",
    "display(binary_metrics)\n",
    "\n",
    "binary_pipeline.plot_roc_auc_bars('binary_roc_auc_bar.png')\n",
    "binary_pipeline.plot_roc_curves('binary_roc_curve')\n",
    "binary_pipeline.plot_precision_vs_residual('binary_precision_residual')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996c728",
   "metadata": {},
   "source": [
    "*Artifacts saved*: `binary_metrics.csv`, `binary_roc_auc_bar.png`, `binary_roc_curve.png`, and `binary_precision_residual.png` inside `1_LLM_Eval/test_results/figs/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22086c9a",
   "metadata": {},
   "source": [
    "## 3. Multitype evaluation example\n\n",
    "Multitype runs now judge success by **exact-match correctness** (all abuse subtypes correct within a row). The ROC and precision\u2013residual plots therefore compare the model's confidence against that per-row correctness, while the waterfall plot colors each sentence by the dominant subtype (or Not IPV).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5407628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary ===\n",
      "         accuracy  precision      f1  roc_auc\n",
      "model                                        \n",
      "cot          0.68     0.3333  0.0667   0.5556\n",
      "fewshot      0.64     0.0000  0.0000   0.5000\n",
      "\n",
      "Saved metrics to /Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/test_results/figs/multilabel_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cot</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fewshot</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy  precision        f1   roc_auc\n",
       "model                                           \n",
       "cot          0.68   0.333333  0.066667  0.555556\n",
       "fewshot      0.64   0.000000  0.000000  0.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1018: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1018: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1018: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1018: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:879: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:879: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:879: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:879: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/test_results/figs/multilabel_confidence_waterfall_cot.png'),\n",
       " PosixPath('/Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/test_results/figs/multilabel_confidence_waterfall_fewshot.png')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_results = {\n",
    "    'cot': PROJECT_ROOT / '1_LLM_Eval/test_results/multilabel_cot_20251029-192707.json',\n",
    "    'fewshot': PROJECT_ROOT / '1_LLM_Eval/test_results/multilabel_fewshot_20251029-192707.json',\n",
    "}\n\n",
    "multilabel_pipeline = LLMEvalPipeline(\n",
    "    results_map=multilabel_results,\n",
    "    ground_truth=DATASET_PATH,\n",
    "    task='multitype',\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")\n\n",
    "multi_metrics = multilabel_pipeline.save_and_print_metrics(\n",
    "    OUTPUT_DIR / 'multilabel_metrics.csv'\n",
    ")\n",
    "display(multi_metrics)\n\n",
    "multilabel_pipeline.plot_roc_auc_bars('multilabel_roc_auc_bar.png')\n",
    "multilabel_pipeline.plot_roc_curves('multilabel_roc_curve')\n",
    "multilabel_pipeline.plot_precision_vs_residual('multilabel_precision_residual')\n",
    "multilabel_pipeline.multi_confidence_score_plot('multilabel_confidence_waterfall')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefedfc",
   "metadata": {},
   "source": [
    "## 4. Next steps\n\n",
    "- Swap in new JSON outputs by updating the dictionaries.\n",
    "- Point `DATASET_PATH` to alternative ground-truth files if you are testing new cohorts.\n",
    "- Capture returned `Path` objects from plotting calls if you want to log them elsewhere (e.g., Weights & Biases).\n",
    "- When adding new multitype models, ensure their outputs include confidence scores (or per-class probabilities) so the exact-match ROC plot stays informative.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}