{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3480a6",
   "metadata": {},
   "source": [
    "# `eval_llm_pipeline` Sample Usage\n",
    "\n",
    "This notebook demonstrates how to reuse `eval_llm_pipeline.LLMEvalPipeline` so every LLM experiment shares identical evaluation/plotting logic. Adjust the paths in the code cells to point to whichever result JSON files and datasets you want to compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a841419",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "1. Import the pipeline class.\n",
    "2. Point `PROJECT_ROOT` at the repository root.\n",
    "3. Reuse the helper constants (`DATASET_PATH`, `OUTPUT_DIR`) in later cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db682d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from IPython.display import display\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATASET_PATH = PROJECT_ROOT / 'Dataset/reddit_data.csv'\n",
    "OUTPUT_DIR = PROJECT_ROOT / '1_LLM_Eval/test_results/figs'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT / '1_LLM_Eval'))\n",
    "from eval_llm_pipeline import LLMEvalPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585076e",
   "metadata": {},
   "source": [
    "## 2. Binary evaluation example\n",
    "\n",
    "Below we compare two binary prompting strategies (`cot` vs `fewshot`). Replace the JSON paths or extend the dictionary with additional runs as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_results = {\n",
    "    'cot': PROJECT_ROOT / '1_LLM_Eval/test_results/binary_cot.json',\n",
    "    'fewshot': PROJECT_ROOT / '1_LLM_Eval/test_results/binary_fewshot.json',\n",
    "}\n",
    "\n",
    "binary_pipeline = LLMEvalPipeline(\n",
    "    results_map=binary_results,\n",
    "    ground_truth=DATASET_PATH,\n",
    "    task='binary',\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    positive_label='IPV'\n",
    ")\n",
    "\n",
    "binary_metrics = binary_pipeline.save_and_print_metrics(\n",
    "    OUTPUT_DIR / 'binary_metrics.csv'\n",
    ")\n",
    "display(binary_metrics)\n",
    "\n",
    "binary_pipeline.plot_roc_auc_bars('binary_roc_auc_bar.png')\n",
    "binary_pipeline.plot_roc_curves('binary_roc_curve')\n",
    "binary_pipeline.plot_precision_vs_residual('binary_precision_residual')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996c728",
   "metadata": {},
   "source": [
    "*Artifacts saved*: metrics table (`binary_metrics.csv`) plus ROC/precision-residual plots under `1_LLM_Eval/test_results/figs/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22086c9a",
   "metadata": {},
   "source": [
    "## 3. Multitype evaluation example\n",
    "\n",
    "For subtype-aware prompts, pass the multilabel JSON files and reuse the same dataset. The pipeline automatically derives per-class ROC curves, macro metrics, and confidence waterfalls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5407628",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_results = {\n",
    "    'cot': PROJECT_ROOT / '1_LLM_Eval/test_results/multilabel_cot_20251029-192707.json',\n",
    "    'fewshot': PROJECT_ROOT / '1_LLM_Eval/test_results/multilabel_fewshot_20251029-192707.json',\n",
    "}\n",
    "\n",
    "multilabel_pipeline = LLMEvalPipeline(\n",
    "    results_map=multilabel_results,\n",
    "    ground_truth=DATASET_PATH,\n",
    "    task='multitype',\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "multi_metrics = multilabel_pipeline.save_and_print_metrics(\n",
    "    OUTPUT_DIR / 'multilabel_metrics.csv'\n",
    ")\n",
    "display(multi_metrics)\n",
    "\n",
    "multilabel_pipeline.plot_roc_auc_bars('multilabel_roc_auc_bar.png')\n",
    "multilabel_pipeline.plot_roc_curves('multilabel_roc_curve')\n",
    "multilabel_pipeline.plot_precision_vs_residual('multilabel_precision_residual')\n",
    "multilabel_pipeline.multi_confidence_score_plot('multilabel_confidence_waterfall')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefedfc",
   "metadata": {},
   "source": [
    "## 4. Next steps\n",
    "\n",
    "* Swap in new JSON outputs by updating the dictionaries.\n",
    "* Point `DATASET_PATH` to alternative ground-truth files if you are testing new cohorts.\n",
    "* The plotting methods return `Path` objects, so you can capture them for downstream logging if desired.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
