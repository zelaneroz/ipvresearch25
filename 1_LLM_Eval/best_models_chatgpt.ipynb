{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9e3e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top runs:\n",
      "Accuracy #1: qwen2.5-7b (fewshot) -> 0.8932\n",
      "Accuracy #2: qwen2.5-7b (selfconsistency) -> 0.8560\n",
      "Accuracy #3: qwen2.5-7b (meta) -> 0.8528\n",
      "F1 #1: qwen2.5-7b (fewshot) -> 0.8900\n",
      "F1 #2: qwen2.5-7b (selfconsistency) -> 0.8463\n",
      "F1 #3: qwen2.5-7b (meta) -> 0.8428\n",
      "Saved plot → w4/best_models.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Utility for surfacing the top binary evaluation runs by metric.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Handle both script and notebook execution\n",
    "try:\n",
    "    RESULTS_PATH = Path(__file__).resolve().parent / \"results\" / \"binary_results.json\"\n",
    "except NameError:\n",
    "    # Running in notebook - try multiple possible paths\n",
    "    current_dir = Path.cwd()\n",
    "    possible_paths = [\n",
    "        current_dir / \"results\" / \"binary_results.json\",  # If running from 1_LLM_Eval dir\n",
    "        current_dir / \"1_LLM_Eval\" / \"results\" / \"binary_results.json\",  # If running from project root\n",
    "    ]\n",
    "    RESULTS_PATH = None\n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            RESULTS_PATH = path\n",
    "            break\n",
    "    if RESULTS_PATH is None:\n",
    "        # Default to relative path from notebook location\n",
    "        RESULTS_PATH = Path(\"results\") / \"binary_results.json\"\n",
    "\n",
    "TOP_K = 3\n",
    "METRICS = (\"Accuracy\", \"F1\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalRun:\n",
    "    label: str\n",
    "    metrics: Dict[str, float]\n",
    "\n",
    "    @classmethod\n",
    "    def from_entry(cls, entry: Dict) -> \"EvalRun\":\n",
    "        # Safely read model_name + prompt_version\n",
    "        model = entry.get(\"model_name\", \"UnknownModel\")\n",
    "        version = entry.get(\"prompt_version\", \"v?\")\n",
    "\n",
    "        label = f\"{model} ({version})\"\n",
    "        metrics = entry.get(\"metrics\", {})\n",
    "\n",
    "        return cls(label=label, metrics=metrics)\n",
    "\n",
    "\n",
    "def load_runs(path: Path) -> List[EvalRun]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Could not find results file at: {path}\")\n",
    "\n",
    "    with path.open() as fp:\n",
    "        payload = json.load(fp)\n",
    "\n",
    "    # Expect file to contain a list of run entries\n",
    "    if not isinstance(payload, list):\n",
    "        raise ValueError(\"Results file must contain a list of evaluation entries.\")\n",
    "\n",
    "    return [EvalRun.from_entry(entry) for entry in payload]\n",
    "\n",
    "\n",
    "def top_k_runs(runs: List[EvalRun], metric: str, k: int) -> List[EvalRun]:\n",
    "    filtered = [run for run in runs if metric in run.metrics]\n",
    "    if not filtered:\n",
    "        raise ValueError(f\"Metric '{metric}' missing from all runs.\")\n",
    "\n",
    "    sorted_runs = sorted(\n",
    "        filtered,\n",
    "        key=lambda run: run.metrics[metric],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return sorted_runs[:k]\n",
    "\n",
    "\n",
    "def plot_top_runs(runs: List[EvalRun], metrics: List[str], top_k: int) -> None:\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 5))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        top_runs = top_k_runs(runs, metric, top_k)\n",
    "\n",
    "        scores = [run.metrics[metric] for run in top_runs]\n",
    "        labels = [run.label for run in top_runs]\n",
    "\n",
    "        bars = ax.bar(range(len(scores)), scores, \n",
    "                      tick_label=[f\"#{idx+1}\" for idx in range(len(scores))])\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_xlabel(\"Rank\")\n",
    "        ax.set_title(f\"Top {len(scores)} by {metric}\")\n",
    "\n",
    "        legend_labels = [f\"{label}: {score:.3f}\" \n",
    "                         for label, score in zip(labels, scores)]\n",
    "        ax.legend(bars, legend_labels, loc=\"lower left\", fontsize=\"small\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    PLOT_DIR = \"w4\"\n",
    "    os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(PLOT_DIR, \"best_models.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot → {save_path}\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    runs = load_runs(RESULTS_PATH)\n",
    "\n",
    "    print(\"\\nTop runs:\")\n",
    "    for metric in METRICS:\n",
    "        best = top_k_runs(runs, metric, TOP_K)\n",
    "        for idx, run in enumerate(best, start=1):\n",
    "            print(f\"{metric} #{idx}: {run.label} -> {run.metrics[metric]:.4f}\")\n",
    "\n",
    "    plot_top_runs(runs, list(METRICS), TOP_K)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
