{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e91e31",
   "metadata": {},
   "source": [
    "# IPV Detection Evaluation Pipeline\n",
    "\n",
    "This notebook evaluates LLM predictions for Intimate Partner Violence (IPV) detection.\n",
    "\n",
    "## Structure\n",
    "1. **Setup**: Import libraries and define evaluation functions\n",
    "2. **Single Evaluation**: Test one prediction file at a time\n",
    "3. **Bulk Evaluation**: Compare all prediction files and generate summary\n",
    "\n",
    "## Outputs\n",
    "All plots are saved to `./eval_figs/` directory:\n",
    "- `ROC_curve.png`: Receiver Operating Characteristic curve\n",
    "- `PR_curve.png`: Precision-Recall curve\n",
    "\n",
    "## Function Reference\n",
    "\n",
    "| Function | Input | Output |\n",
    "|----------|-------|--------|\n",
    "| `load_results(path)` | File path (JSON/CSV) | DataFrame with `id` column |\n",
    "| `load_groundtruth(csv_path)` | CSV file path | DataFrame with normalized abuse columns and binary `label` |\n",
    "| `evaluate_binary(df_pred, df_truth)` | Two DataFrames | Dict of metrics + arrays `(y_true, y_pred)` |\n",
    "| `evaluate_multilabel(df_pred, df_truth)` | Two DataFrames | Dict with F1_macro, F1_micro, ExactMatchAcc |\n",
    "| `plot_roc_curve(y_true, y_scores, model_name, out_dir, filename)` | Arrays, model name, paths | ROC AUC + saves ROC curve plot |\n",
    "| `plot_pr_curve(y_true, y_scores, out_dir)` | Arrays + directory path | PR AUC + saves PR curve plot |\n",
    "| `plot_binary(y_true, y_pred, out_dir)` | Arrays + directory path | Dict with ROC_AUC, PR_AUC + saves plots |\n",
    "| `run_evaluation(results_path, groundtruth_path, task, out_dir)` | File paths, task type, output dir | Dict of all metrics + prints results |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485d15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL EVALUATION PIPELINE (CLEAN + FLEXIBLE)\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LOADERS\n",
    "# ============================================================\n",
    "\n",
    "def load_results(path: str):\n",
    "    \"\"\"Load model output file (JSON or CSV)\"\"\"\n",
    "    path = Path(path)\n",
    "    if path.suffix == \".json\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "    elif path.suffix == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(\"Results file must be .json or .csv\")\n",
    "\n",
    "    if \"id\" not in df.columns:\n",
    "        df[\"id\"] = np.arange(len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_groundtruth(csv_path: str):\n",
    "    \"\"\"Load ground truth CSV and normalize for IPV detection.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"id\" not in df.columns:\n",
    "        df[\"id\"] = df.index\n",
    "\n",
    "    # rename\n",
    "    rename_map = {\n",
    "        \"Physical Abuse\": \"Physical\",\n",
    "        \"Emotional Abuse\": \"Emotional\",\n",
    "        \"Sexual Abuse\": \"Sexual\"\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # convert TRUE/FALSE strings → booleans\n",
    "    for c in [\"Physical\", \"Emotional\", \"Sexual\"]:\n",
    "        df[c] = (\n",
    "            df[c].astype(str)\n",
    "            .str.upper()\n",
    "            .map({\"TRUE\": True, \"FALSE\": False})\n",
    "            .fillna(False)\n",
    "        )\n",
    "\n",
    "    # create IPV label (for binary)\n",
    "    df[\"IPV\"] = df[[\"Physical\", \"Emotional\", \"Sexual\"]].any(axis=1)\n",
    "    df[\"label\"] = df[\"IPV\"].map({True: \"IPV\", False: \"NOT_IPV\"})\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_binary(df_pred: pd.DataFrame, df_truth: pd.DataFrame):\n",
    "    \"\"\"Evaluate binary IPV detection performance.\"\"\"\n",
    "    y_true = df_truth[\"label\"].map({\"IPV\": 1, \"NOT_IPV\": 0}).values\n",
    "\n",
    "    # clean predictions\n",
    "    y_raw = df_pred[\"extracted_label\"].astype(str).str.upper().str.strip()\n",
    "    y_raw = y_raw.replace({\"\": \"NOT_IPV\", \"NONE\": \"NOT_IPV\", \"N/A\": \"NOT_IPV\"})\n",
    "    y_pred = y_raw.map({\"IPV\": 1, \"NOT_IPV\": 0}).fillna(0).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # optional AUC if scores exist\n",
    "    if \"y_score\" in df_pred.columns:\n",
    "        y_score = df_pred[\"y_score\"].fillna(0.5).values\n",
    "        try:\n",
    "            rocauc = roc_auc_score(y_true, y_score)\n",
    "        except Exception:\n",
    "            rocauc = np.nan\n",
    "    else:\n",
    "        rocauc = np.nan\n",
    "\n",
    "    return {\"Accuracy\": acc, \"F1\": f1, \"ROC_AUC\": rocauc}, y_true, y_pred\n",
    "\n",
    "\n",
    "def evaluate_multilabel(df_pred: pd.DataFrame, df_truth: pd.DataFrame):\n",
    "    \"\"\"Evaluate multilabel (Physical, Emotional, Sexual) predictions.\"\"\"\n",
    "    cols = [\"Physical\", \"Emotional\", \"Sexual\"]\n",
    "\n",
    "    # build y_pred from extracted_labels if necessary\n",
    "    if \"extracted_labels\" in df_pred.columns:\n",
    "        def parse_labels(x):\n",
    "            if isinstance(x, str):\n",
    "                return {lbl.strip().capitalize() for lbl in x.split(\",\")}\n",
    "            elif isinstance(x, list):\n",
    "                return {lbl.strip().capitalize() for lbl in x}\n",
    "            return set()\n",
    "\n",
    "        for c in cols:\n",
    "            df_pred[c] = df_pred[\"extracted_labels\"].apply(lambda L: int(c in parse_labels(L)))\n",
    "\n",
    "    y_true = df_truth[cols].astype(int).values\n",
    "    y_pred = df_pred[cols].astype(int).values\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    exact_acc = (y_true == y_pred).all(axis=1).mean()\n",
    "\n",
    "    return {\"F1_macro\": f1_macro, \"F1_micro\": f1_micro, \"ExactMatchAcc\": exact_acc}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================\n",
    "\n",
    "def plot_roc_curve(y_true, y_scores, model_name=\"Model\", out_dir=\"./eval_figs\", filename=\"ROC_curve.png\"):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for binary classification.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True binary labels (0 or 1)\n",
    "        y_scores: Prediction scores or probabilities (continuous values)\n",
    "        model_name: Name of the model for title\n",
    "        out_dir: Output directory for saving plots\n",
    "        filename: Name of the output file\n",
    "        \n",
    "    Returns:\n",
    "        ROC AUC score\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray', alpha=0.5, label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'{model_name} ROC Curve', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, out_dir=\"./eval_figs\"):\n",
    "    \"\"\"Plot Precision-Recall curve for binary evaluation.\"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec, label=f\"PR AUC={pr_auc:.3f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"PR_curve.png\", dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    return pr_auc\n",
    "\n",
    "\n",
    "def plot_binary(y_true, y_pred, out_dir=\"./eval_figs\"):\n",
    "    \"\"\"Plot ROC + PR curves for binary evaluation (legacy function for compatibility).\"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # For plotting, use predictions as scores (no probs available)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Use the new ROC function (y_true, y_pred as scores)\n",
    "    plot_roc_curve(y_true, y_pred, model_name=\"Model\", out_dir=out_dir, filename=\"ROC_curve.png\")\n",
    "    \n",
    "    # Plot PR curve\n",
    "    pr_auc = plot_pr_curve(y_true, y_pred, out_dir)\n",
    "\n",
    "    return {\"ROC_AUC\": roc_auc, \"PR_AUC\": pr_auc}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DRIVER\n",
    "# ============================================================\n",
    "\n",
    "def run_evaluation(\n",
    "    results_path: str,\n",
    "    groundtruth_path: str,\n",
    "    task: str = \"binary\",\n",
    "    out_dir: str = \"./eval_figs\"\n",
    "):\n",
    "    \"\"\"Main entry point: evaluate one model output file.\"\"\"\n",
    "    df_pred = load_results(results_path)\n",
    "    df_truth = load_groundtruth(groundtruth_path)\n",
    "\n",
    "    if len(df_pred) != len(df_truth):\n",
    "        print(f\"⚠️ Warning: Mismatch in length ({len(df_pred)} vs {len(df_truth)})\")\n",
    "        n = min(len(df_pred), len(df_truth))\n",
    "        df_pred = df_pred.head(n)\n",
    "        df_truth = df_truth.head(n)\n",
    "\n",
    "    if task == \"binary\":\n",
    "        metrics, y_true, y_pred = evaluate_binary(df_pred, df_truth)\n",
    "        curves = plot_binary(y_true, y_pred, out_dir)\n",
    "        metrics.update(curves)\n",
    "    elif task == \"multilabel\":\n",
    "        metrics = evaluate_multilabel(df_pred, df_truth)\n",
    "    else:\n",
    "        raise ValueError(\"Task must be 'binary' or 'multilabel'\")\n",
    "\n",
    "    print(f\"\\n{task.upper()} Evaluation Results\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:>12}: {v:.3f}\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfa176",
   "metadata": {},
   "source": [
    "# 2. Actual Evaluation\n",
    "\n",
    "## Running Evaluations\n",
    "\n",
    "You can evaluate either:\n",
    "- **Single file**: Specify one prediction file (useful for testing/debugging)\n",
    "- **All files**: Loop through all prediction files in a directory (useful for comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba5293",
   "metadata": {},
   "source": [
    "## 2.1 Single Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91431830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BINARY Evaluation Results\n",
      "    Accuracy: 0.728\n",
      "          F1: 0.656\n",
      "     ROC_AUC: 0.742\n",
      "      PR_AUC: 0.874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.7281553398058253,\n",
       " 'F1': 0.6557377049180327,\n",
       " 'ROC_AUC': 0.7419741154718884,\n",
       " 'PR_AUC': 0.8735301406197031}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_evaluation(\n",
    "    results_path=\"../1_LLM_Eval/test_results/binary_zeroshot.json\",\n",
    "    groundtruth_path=\"../Dataset/617points.csv\",\n",
    "    task=\"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25733d85",
   "metadata": {},
   "source": [
    "## 2.2 Bulk Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d91dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BINARY Evaluation Results\n",
      "    Accuracy: 0.853\n",
      "          F1: 0.843\n",
      "     ROC_AUC: 0.859\n",
      "      PR_AUC: 0.923\n",
      "\n",
      "BINARY Evaluation Results\n",
      "    Accuracy: 0.856\n",
      "          F1: 0.846\n",
      "     ROC_AUC: 0.862\n",
      "      PR_AUC: 0.925\n",
      "\n",
      "BINARY Evaluation Results\n",
      "    Accuracy: 0.806\n",
      "          F1: 0.782\n",
      "     ROC_AUC: 0.814\n",
      "      PR_AUC: 0.899\n",
      "\n",
      "BINARY Evaluation Results\n",
      "    Accuracy: 0.728\n",
      "          F1: 0.656\n",
      "     ROC_AUC: 0.742\n",
      "      PR_AUC: 0.874\n",
      "\n",
      "BINARY Evaluation Results\n",
      "    Accuracy: 0.893\n",
      "          F1: 0.890\n",
      "     ROC_AUC: 0.898\n",
      "      PR_AUC: 0.944\n",
      "   Accuracy        F1   ROC_AUC    PR_AUC                         file    task\n",
      "0  0.852751  0.842832  0.858822  0.922790             binary_meta.json  binary\n",
      "1  0.855987  0.846287  0.862068  0.925491  binary_selfconsistency.json  binary\n",
      "2  0.805825  0.781818  0.814344  0.899471              binary_cot.json  binary\n",
      "3  0.728155  0.655738  0.741974  0.873530         binary_zeroshot.json  binary\n",
      "4  0.893204  0.890000  0.897523  0.944470          binary_fewshot.json  binary\n",
      "\n",
      "🏆 Best Binary Prompt: binary_fewshot.json\n",
      "   F1 Score: 0.890\n",
      "   Available at: ../1_LLM_Eval/test_results/binary_fewshot.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"../1_LLM_Eval/test_results\")\n",
    "gt_path = \"../Dataset/617points.csv\"\n",
    "\n",
    "records = []\n",
    "\n",
    "for fp in results_dir.glob(\"*.json\"):\n",
    "    task = \"multilabel\" if \"multilabel\" in fp.name.lower() else \"binary\"\n",
    "    # Use default ./eval_figs directory (same as single evaluation)\n",
    "    metrics = run_evaluation(\n",
    "        results_path=fp,\n",
    "        groundtruth_path=gt_path,\n",
    "        task=task\n",
    "    )\n",
    "    metrics[\"file\"] = fp.name\n",
    "    metrics[\"task\"] = task\n",
    "    records.append(metrics)\n",
    "\n",
    "# Summary table\n",
    "import pandas as pd\n",
    "df_summary = pd.DataFrame(records)\n",
    "print(df_summary)\n",
    "\n",
    "# Identify best binary prompt\n",
    "df_binary = df_summary[df_summary[\"task\"] == \"binary\"]\n",
    "if not df_binary.empty:\n",
    "    # Sort by F1 score (or ROC_AUC as tiebreaker)\n",
    "    df_binary_sorted = df_binary.sort_values([\"F1\", \"ROC_AUC\"], ascending=False)\n",
    "    best_binary_file = df_binary_sorted.iloc[0][\"file\"]\n",
    "    best_binary_f1 = df_binary_sorted.iloc[0][\"F1\"]\n",
    "    \n",
    "    print(f\"\\n🏆 Best Binary Prompt: {best_binary_file}\")\n",
    "    print(f\"   F1 Score: {best_binary_f1:.3f}\")\n",
    "    print(f\"   Available at: {Path(results_dir) / best_binary_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecae293",
   "metadata": {},
   "source": [
    "## 2.3 Plot ROC Curve for Best Binary Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab142b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ ROC curve saved with AUC = 0.898\n"
     ]
    }
   ],
   "source": [
    "# Load the best binary prompt predictions\n",
    "best_binary_path = results_dir / best_binary_file\n",
    "df_pred_best = load_results(best_binary_path)\n",
    "df_truth_best = load_groundtruth(gt_path)\n",
    "\n",
    "# Evaluate to get true labels and predictions\n",
    "_, y_true_best, y_pred_best = evaluate_binary(df_pred_best, df_truth_best)\n",
    "\n",
    "# Plot ROC curve with model name in title\n",
    "auc_score = plot_roc_curve(\n",
    "    y_true=y_true_best,\n",
    "    y_scores=y_pred_best.astype(float),  # Convert to float for plotting\n",
    "    model_name=\"Qwen2.5-7B-Instruct\",\n",
    "    out_dir=\"./eval_figs\",\n",
    "    filename=\"best_binary_roc.png\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ ROC curve saved with AUC = {auc_score:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
