[
  {
    "date_tested": "2025-11-20",
    "model": "qwen2.5-7b",
    "prompt_version": "zeroshot",
    "metrics_per_subgroup": {
      "overall": {
        "physical_true": {
          "accuracy": 0.941747572815534,
          "precision": 0.7714285714285715,
          "recall": 0.7297297297297297,
          "f1": 0.75,
          "true_positives": 54,
          "false_positives": 16,
          "true_negatives": 528,
          "false_negatives": 20
        },
        "emotional_true": {
          "accuracy": 0.8446601941747572,
          "precision": 0.7747440273037542,
          "recall": 0.8832684824902723,
          "f1": 0.8254545454545454,
          "true_positives": 227,
          "false_positives": 66,
          "true_negatives": 295,
          "false_negatives": 30
        },
        "sexual_true": {
          "accuracy": 0.9627831715210357,
          "precision": 0.5625,
          "recall": 0.36,
          "f1": 0.43902439024390244,
          "true_positives": 9,
          "false_positives": 7,
          "true_negatives": 586,
          "false_negatives": 16
        }
      }
    },
    "notes": "Evaluation of zeroshot prompt"
  },
  {
    "date_tested": "2025-11-20",
    "model": "qwen2.5-7b",
    "prompt_version": "zeroshot",
    "metrics_per_subgroup": {
      "overall": {
        "physical_true": {
          "accuracy": 0.941747572815534,
          "precision": 0.7714285714285715,
          "recall": 0.7297297297297297,
          "f1": 0.75,
          "true_positives": 54,
          "false_positives": 16,
          "true_negatives": 528,
          "false_negatives": 20
        },
        "emotional_true": {
          "accuracy": 0.8446601941747572,
          "precision": 0.7747440273037542,
          "recall": 0.8832684824902723,
          "f1": 0.8254545454545454,
          "true_positives": 227,
          "false_positives": 66,
          "true_negatives": 295,
          "false_negatives": 30
        },
        "sexual_true": {
          "accuracy": 0.9627831715210357,
          "precision": 0.5625,
          "recall": 0.36,
          "f1": 0.43902439024390244,
          "true_positives": 9,
          "false_positives": 7,
          "true_negatives": 586,
          "false_negatives": 16
        }
      }
    },
    "notes": "Evaluation of zeroshot prompt"
  },
  {
    "date_tested": "2025-11-21",
    "model": "qwen2.5-7b",
    "prompt_version": "fewshot",
    "metrics_per_subgroup": {
      "overall": {
        "physical_true": {
          "accuracy": 0.9061488673139159,
          "precision": 0.5784313725490197,
          "recall": 0.7972972972972973,
          "f1": 0.6704545454545454,
          "true_positives": 59,
          "false_positives": 43,
          "true_negatives": 501,
          "false_negatives": 15
        },
        "emotional_true": {
          "accuracy": 0.8203883495145631,
          "precision": 0.7967479674796748,
          "recall": 0.7626459143968871,
          "f1": 0.7793240556660039,
          "true_positives": 196,
          "false_positives": 50,
          "true_negatives": 311,
          "false_negatives": 61
        },
        "sexual_true": {
          "accuracy": 0.9239482200647249,
          "precision": 0.30357142857142855,
          "recall": 0.68,
          "f1": 0.4197530864197531,
          "true_positives": 17,
          "false_positives": 39,
          "true_negatives": 554,
          "false_negatives": 8
        }
      }
    },
    "notes": "Evaluation of fewshot prompt"
  },
  {
    "date_tested": "2025-11-21",
    "model": "qwen2.5-7b",
    "prompt_version": "cot",
    "metrics_per_subgroup": {
      "overall": {
        "physical_true": {
          "accuracy": 0.93042071197411,
          "precision": 0.6823529411764706,
          "recall": 0.7837837837837838,
          "f1": 0.7295597484276729,
          "true_positives": 58,
          "false_positives": 27,
          "true_negatives": 517,
          "false_negatives": 16
        },
        "emotional_true": {
          "accuracy": 0.8284789644012945,
          "precision": 0.7542087542087542,
          "recall": 0.8715953307392996,
          "f1": 0.8086642599277978,
          "true_positives": 224,
          "false_positives": 73,
          "true_negatives": 288,
          "false_negatives": 33
        },
        "sexual_true": {
          "accuracy": 0.9466019417475728,
          "precision": 0.3,
          "recall": 0.24,
          "f1": 0.2666666666666666,
          "true_positives": 6,
          "false_positives": 14,
          "true_negatives": 579,
          "false_negatives": 19
        }
      }
    },
    "notes": "Evaluation of cot prompt"
  },
  {
    "date_tested": "2025-11-21",
    "model": "qwen2.5-7b",
    "prompt_version": "cot",
    "metrics_per_subgroup": {
      "overall": {
        "physical_true": {
          "accuracy": 0.5355987055016181,
          "precision": 0.16614420062695925,
          "recall": 0.7162162162162162,
          "f1": 0.2697201017811705,
          "true_positives": 53,
          "false_positives": 266,
          "true_negatives": 278,
          "false_negatives": 21
        },
        "emotional_true": {
          "accuracy": 0.5728155339805825,
          "precision": 0.4914004914004914,
          "recall": 0.7782101167315175,
          "f1": 0.6024096385542169,
          "true_positives": 200,
          "false_positives": 207,
          "true_negatives": 154,
          "false_negatives": 57
        },
        "sexual_true": {
          "accuracy": 0.5469255663430421,
          "precision": 0.06779661016949153,
          "recall": 0.8,
          "f1": 0.125,
          "true_positives": 20,
          "false_positives": 275,
          "true_negatives": 318,
          "false_negatives": 5
        }
      }
    },
    "notes": "Evaluation of cot prompt"
  },
  {
    "date_tested": "2025-11-21",
    "model": "qwen2.5-7b",
    "prompt_version": "selfconsistency",
    "metrics_per_subgroup": {
      "overall": {
        "physical_true": {
          "accuracy": 0.5355987055016181,
          "precision": 0.16614420062695925,
          "recall": 0.7162162162162162,
          "f1": 0.2697201017811705,
          "true_positives": 53,
          "false_positives": 266,
          "true_negatives": 278,
          "false_negatives": 21
        },
        "emotional_true": {
          "accuracy": 0.5728155339805825,
          "precision": 0.4914004914004914,
          "recall": 0.7782101167315175,
          "f1": 0.6024096385542169,
          "true_positives": 200,
          "false_positives": 207,
          "true_negatives": 154,
          "false_negatives": 57
        },
        "sexual_true": {
          "accuracy": 0.5469255663430421,
          "precision": 0.06779661016949153,
          "recall": 0.8,
          "f1": 0.125,
          "true_positives": 20,
          "false_positives": 275,
          "true_negatives": 318,
          "false_negatives": 5
        }
      }
    },
    "notes": "Evaluation of selfconsistency prompt"
  },
  {
    "date_tested": "2025-11-21",
    "model": "qwen2.5-7b",
    "prompt_version": "cot",
    "metrics_per_subgroup": {
      "overall": {
        "physical_true": {
          "accuracy": 0.93042071197411,
          "precision": 0.6823529411764706,
          "recall": 0.7837837837837838,
          "f1": 0.7295597484276729,
          "true_positives": 58,
          "false_positives": 27,
          "true_negatives": 517,
          "false_negatives": 16
        },
        "emotional_true": {
          "accuracy": 0.8284789644012945,
          "precision": 0.7542087542087542,
          "recall": 0.8715953307392996,
          "f1": 0.8086642599277978,
          "true_positives": 224,
          "false_positives": 73,
          "true_negatives": 288,
          "false_negatives": 33
        },
        "sexual_true": {
          "accuracy": 0.9466019417475728,
          "precision": 0.3,
          "recall": 0.24,
          "f1": 0.2666666666666666,
          "true_positives": 6,
          "false_positives": 14,
          "true_negatives": 579,
          "false_negatives": 19
        }
      }
    },
    "notes": "Evaluation of cot prompt"
  }
]