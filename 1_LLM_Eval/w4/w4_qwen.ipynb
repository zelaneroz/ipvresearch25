{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "479420e3",
      "metadata": {
        "id": "479420e3"
      },
      "source": [
        "# Week 4\n",
        "**Deliverables**\n",
        "* Refine prompts for Binary & Multitype"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1138d41b",
      "metadata": {
        "id": "1138d41b"
      },
      "source": [
        "## 1. Prompt Library\n",
        "Binary and multitype templates that feed every evaluation routine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce217a3",
      "metadata": {
        "id": "1ce217a3"
      },
      "outputs": [],
      "source": [
        "# BINARY PROMPTS\n",
        "# (1) Zero-Shot — final (no inline JSON examples)\n",
        "prompt_binary_zeroshot = \"\"\"\n",
        "You are an expert analyzing whether a sentence describes Intimate Partner Violence (IPV).\n",
        "Intimate Partner Violence refers to any physical, emotional, psychological, financial, or sexual harm or coercion occurring within a romantic or formerly romantic relationship. Examples include physical aggression, threats, intimidation, coercion, unwanted sexual acts, emotional abuse (insults, humiliation, degradation), controlling behaviors (monitoring, isolation, restricting access to resources), manipulation, gaslighting, and persistent jealousy.\n",
        "NOT_IPV should be used when the sentence involves disagreements without abuse or coercion, non-romantic relationships (friends, coworkers, family, strangers), neutral statements, harm not involving a partner, general opinions about relationships, hypothetical or fictional statements without abuse, or consensual behaviors without coercion.\n",
        "\n",
        "Decide if the following sentence involves IPV or not.\n",
        "Sentence: \"{text}\"\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: a single key \"label\" with value \"IPV\" or \"NOT_IPV\".\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (2) Few-Shot — final (no inline JSON examples)\n",
        "prompt_binary_fewshot = \"\"\"\n",
        "You are classifying sentences as describing Intimate Partner Violence (IPV) or NOT_IPV.\n",
        "\n",
        "Definition:\n",
        "IPV occurs when one partner harms or controls another physically, emotionally, or sexually.\n",
        "\n",
        "Here are examples (for guidance only):\n",
        "1. \"He pushed me against the wall and shouted that I’m worthless.\" → IPV\n",
        "2. \"She often insults me and forbids me from seeing my friends.\" → IPV\n",
        "3. \"We argued but both apologized and moved on peacefully.\" → NOT_IPV\n",
        "4. \"He surprised me with flowers after work.\" → NOT_IPV\n",
        "\n",
        "Now analyze this sentence:\n",
        "\"{text}\"\n",
        "\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (3) Chain-of-Thought — final (no inline JSON examples)\n",
        "prompt_binary_cot = \"\"\"\n",
        "You are reasoning step-by-step to decide if a sentence describes Intimate Partner Violence (IPV).\n",
        "\n",
        "Guidelines:\n",
        "- IPV includes threats, coercion, physical harm, humiliation, or emotional manipulation.\n",
        "- NOT_IPV describes healthy, neutral, or unrelated situations.\n",
        "\n",
        "Think internally (do NOT show your reasoning) about:\n",
        "1. Does the sentence show any behavior that causes harm, fear, or control?\n",
        "2. Is there a partner/relationship context?\n",
        "3. Does it express affection or support instead of harm?\n",
        "\n",
        "After thinking silently, return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (4) Meta — final (no inline JSON examples)\n",
        "prompt_binary_meta = \"\"\"\n",
        "You are a social-behavioral analyst evaluating sentences for signs of Intimate Partner Violence (IPV).\n",
        "\n",
        "Your objective is to be accurate but cautious.\n",
        "- If the sentence clearly involves harm, coercion, or control → label as IPV.\n",
        "- If the sentence shows affection, neutrality, or uncertainty → label as NOT_IPV.\n",
        "\n",
        "Reflect internally before answering; do NOT print your reasoning.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "# (5) Self-Consistency — final (no inline JSON examples)\n",
        "prompt_binary_selfconsistency = \"\"\"\n",
        "You will internally evaluate the sentence for Intimate Partner Violence (IPV) multiple times\n",
        "and choose the majority label as your final answer.\n",
        "\n",
        "Guidelines:\n",
        "- IPV → signs of physical, emotional, or sexual harm, threats, or coercion.\n",
        "- NOT_IPV → supportive, neutral, or unrelated content.\n",
        "\n",
        "Do NOT reveal thoughts or votes.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\n",
        "Return ONLY one JSON object, enclosed between <json> and </json>.\n",
        "Format: {\"label\": \"IPV\"} or {\"label\": \"NOT_IPV\"}.\n",
        "Print nothing else.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1210e000",
      "metadata": {
        "id": "1210e000"
      },
      "source": [
        "### 1.2 Multitype Prompts\n",
        "These templates mirror the binary ones but collect subtype labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c09576f0",
      "metadata": {
        "id": "c09576f0"
      },
      "outputs": [],
      "source": [
        "# MULTITYPE PROMPTS\n",
        "prompt_multilabel_zeroshot = \"\"\"\n",
        "You are identifying which forms of Intimate Partner Violence (IPV) appear in a sentence.\n",
        "\n",
        "Decide independently for emotional, physical, and sexual abuse.Decide independently for emotional, physical, and sexual abuse. If it is a particular type of IPV, set emotional, physical, or sexual to 1, otherwise set it to 0. Multiple IPV types can be true or none at all.\n",
        "Return ONLY one JSON object enclosed between <json> and </json> with the keys\n",
        "'id', 'emotional', 'physical', and 'sexual'.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_fewshot = \"\"\"\n",
        "You are labeling sentences for types of Intimate Partner Violence (IPV).\n",
        "Use the examples to stay calibrated. For each category (emotional, physical, sexual), output 1 if it is clearly present, else 0.\n",
        "\n",
        "Examples:\n",
        "1. \"He insults me daily and forbids me from leaving the house.\" -> emotional: 1, physical: 0, sexual: 0\n",
        "2. \"She slapped me when I disagreed with her.\" -> emotional: 0, physical: 1, sexual: 0\n",
        "3. \"They pressured me into intimacy when I said no.\" -> emotional: 0, physical: 0, sexual: 1\n",
        "4. \"We spent the evening cooking together peacefully.\" -> emotional: 0, physical: 0, sexual: 0\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_cot = \"\"\"\n",
        "Reason silently about whether emotional, physical, or sexual IPV occurs in the sentence.\n",
        "Use relationship context, threats, coercion, and bodily harm cues.\n",
        "After your hidden reasoning, output ONLY the JSON block specified below.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_meta = \"\"\"\n",
        "Act as a cautious social-behavioral analyst.\n",
        "Label a subtype as 1 only when the text clearly shows that form of IPV; otherwise return 0.\n",
        "Favor precision to avoid false positives.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_multilabel_selfconsistency = \"\"\"\n",
        "Independently evaluate the sentence multiple times to reduce uncertainty.\n",
        "After internal self-consistency voting, output the majority decision for each subtype in the JSON schema below.\n",
        "Do not reveal the intermediate thoughts.\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "Sample ID: \"{sample_id}\"\n",
        "<json>\n",
        "{{\n",
        "  \"id\": \"{sample_id}\",\n",
        "  \"emotional\": 0 or 1,\n",
        "  \"physical\": 0 or 1,\n",
        "  \"sexual\": 0 or 1\n",
        "}}\n",
        "</json>\n",
        "\"\"\".strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb692028",
      "metadata": {
        "id": "cb692028",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 2. System & Model Setup\n",
        "Shared imports, paths, and model objects. Execute once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f65f46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "fd31b3a79961493f8e6de35f86a2a896",
            "774696fc0e09452aa74fd1d7ecc0e631",
            "2647645796aa41a6b0fa63dddd9a39da",
            "f937ee2499214f7592fe534d5b07900e",
            "71c2f44a3500435d93ff924e76e12b01",
            "9569bbc1c1d04d4e9666363c097a5e83",
            "c1f39be0116d4a39a6b3f1dbb34899ff",
            "35243a2b7e944b4cb96ef74b18caf2ea",
            "71a90bfa6ad640a9b08c988bb1e1b474",
            "d50b5ea7641149fea1b356e7eb0868b7",
            "4612038745274cc1a09637108b08680d"
          ]
        },
        "id": "84f65f46",
        "outputId": "0756f068-dc75-42ee-9abe-4ea87e376e01"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m     14\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m DATASET_PATH \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Dataset/reddit_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/__init__.py:958\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m    956\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mset\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _import_structure\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 958\u001b[0m import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m import_structure[\u001b[38;5;28mfrozenset\u001b[39m({})]\u001b[38;5;241m.\u001b[39mupdate(_import_structure)\n\u001b[1;32m    961\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     extra_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m: __version__},\n\u001b[1;32m    967\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/utils/import_utils.py:2867\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[0;34m(module_path, prefix)\u001b[0m\n\u001b[1;32m   2843\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[1;32m   2845\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[1;32m   2847\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;124;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[1;32m   2866\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2867\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m     spread_dict \u001b[38;5;241m=\u001b[39m spread_import_structure(import_structure)\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/utils/import_utils.py:2580\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(module_path):\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__pycache__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(module_path, f)):\n\u001b[0;32m-> 2580\u001b[0m         import_structure[f] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2582\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, f)):\n\u001b[1;32m   2583\u001b[0m         adjacent_modules\u001b[38;5;241m.\u001b[39mappend(f)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/utils/import_utils.py:2595\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_substring\u001b[39m(substring, list_):\n\u001b[1;32m   2593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(substring \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m list_)\n\u001b[0;32m-> 2595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfind_substring\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodular_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacent_modules\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m find_substring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodeling_\u001b[39m\u001b[38;5;124m\"\u001b[39m, adjacent_modules):\n\u001b[1;32m   2596\u001b[0m     adjacent_modules \u001b[38;5;241m=\u001b[39m [module \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m adjacent_modules \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodular_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module]\n\u001b[1;32m   2598\u001b[0m module_requirements \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/utils/import_utils.py:2593\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path.<locals>.find_substring\u001b[0;34m(substring, list_)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_substring\u001b[39m(substring, list_):\n\u001b[0;32m-> 2593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubstring\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlist_\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DATASET_PATH = Path(\"../Dataset/reddit_data.csv\")\n",
        "RESULTS_ROOT = Path(\"qwen\")\n",
        "BINARY_RESULTS_DIR = RESULTS_ROOT \n",
        "MULTITYPE_RESULTS_DIR = RESULTS_ROOT \n",
        "\n",
        "print(f\"Loading {MODEL_NAME} ...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Model ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251e43c8",
      "metadata": {
        "id": "251e43c8"
      },
      "source": [
        "## 3. Prediction Generation\n",
        "Run the Colab cloning cell first, then load the dataset and choose either binary or multitype generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18ed7b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b18ed7b4",
        "outputId": "8aed309e-20e2-4e46-eb55-47017eb50771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ipvresearch25'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (174/174), done.\u001b[K\n",
            "remote: Total 225 (delta 110), reused 148 (delta 51), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (225/225), 7.94 MiB | 39.07 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "/content/ipvresearch25/1_LLM_Eval\n"
          ]
        }
      ],
      "source": [
        "#Clone from git\n",
        "!git clone https://github.com/zelaneroz/ipvresearch25\n",
        "%cd ipvresearch25/1_LLM_Eval\n",
        "#Load dataset\n",
        "filename = \"../Dataset/reddit_data.csv\"\n",
        "df = pd.read_csv(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3947348",
      "metadata": {
        "id": "e3947348"
      },
      "source": [
        "### 3.0 Data Access & Directories\n",
        "Loads the dataset and prepares local folders for saving model outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8905ea",
      "metadata": {
        "id": "2e8905ea"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'BINARY_RESULTS_DIR' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Apply to your result directories\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m ensure_dir(\u001b[43mBINARY_RESULTS_DIR\u001b[49m)\n\u001b[1;32m     16\u001b[0m ensure_dir(MULTITYPE_RESULTS_DIR)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BINARY_RESULTS_DIR' is not defined"
          ]
        }
      ],
      "source": [
        "# Dataset + result folders (safe creation)\n",
        "DATASET_PATH = Path(\"../Dataset/reddit_data.csv\")\n",
        "RESULTS_ROOT = Path(\"w4/qwen\")\n",
        "BINARY_RESULTS_DIR = RESULTS_ROOT \n",
        "MULTITYPE_RESULTS_DIR = RESULTS_ROOT \n",
        "def ensure_dir(path: Path) -> None:\n",
        "    \"\"\"\n",
        "    Ensure that a directory exists.\n",
        "    - If it exists, do nothing.\n",
        "    - If it does not exist, create it (including parents).\n",
        "    \"\"\"\n",
        "    if path.exists():\n",
        "        print(f\"Directory already exists: {path}\")\n",
        "    else:\n",
        "        print(f\"Directory does NOT exist. Creating: {path}\")\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Apply to your result directories\n",
        "ensure_dir(BINARY_RESULTS_DIR)\n",
        "ensure_dir(MULTITYPE_RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c36d68b9",
      "metadata": {
        "id": "c36d68b9"
      },
      "outputs": [],
      "source": [
        "# Prompt registries for downstream loops\n",
        "binary_prompts = {\n",
        "    \"zeroshot\": prompt_binary_zeroshot,\n",
        "    \"fewshot\": prompt_binary_fewshot,\n",
        "    \"cot\": prompt_binary_cot,\n",
        "    \"meta\": prompt_binary_meta,\n",
        "    \"selfconsistency\": prompt_binary_selfconsistency,\n",
        "}\n",
        "\n",
        "multilabel_prompts = {\n",
        "    \"zeroshot\": prompt_multilabel_zeroshot,\n",
        "    \"fewshot\": prompt_multilabel_fewshot,\n",
        "    \"cot\": prompt_multilabel_cot,\n",
        "    \"meta\": prompt_multilabel_meta,\n",
        "    \"selfconsistency\": prompt_multilabel_selfconsistency,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ba5ebf",
      "metadata": {
        "id": "f4ba5ebf"
      },
      "source": [
        "### 3.1 Binary Prediction Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1700430e",
      "metadata": {
        "id": "1700430e"
      },
      "outputs": [],
      "source": [
        "def test_binary_prompts(df: pd.DataFrame, path: Path, n_samples: int = 3) -> None:\n",
        "    \"\"\"Run all binary prompt types on the first `n_samples` rows and persist outputs.\"\"\"\n",
        "    import re\n",
        "\n",
        "    df_subset = df.head(n_samples)\n",
        "    results_dir = Path(path)\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Running binary classification tests...\")\n",
        "    print(f\"Number of samples: {len(df_subset)}\")\n",
        "    print(f\"Results will be saved in: {results_dir}\")\n",
        "\n",
        "    for prompt_type, template in binary_prompts.items():\n",
        "        print(f\"Testing prompt type: {prompt_type}\")\n",
        "        records = []\n",
        "\n",
        "        for i, row in df_subset.iterrows():\n",
        "            text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
        "            prompt_text = template.replace(\"{text}\", text)\n",
        "\n",
        "            try:\n",
        "                inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "                output = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=128,\n",
        "                    temperature=0.0,\n",
        "                    do_sample=False,\n",
        "                )\n",
        "                gen_tokens = output[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "                result_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
        "            except Exception as exc:\n",
        "                result_text = f\"ERROR: {exc}\"\n",
        "\n",
        "            label = None\n",
        "            match = re.search(r\"<json[^>]*>\\s*(.*?)\\s*</json>\", result_text, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                block = match.group(1).strip()\n",
        "                try:\n",
        "                    parsed = json.loads(block)\n",
        "                    if isinstance(parsed, dict):\n",
        "                        label = parsed.get(\"label\") or parsed.get(\"labels\")\n",
        "                    elif isinstance(parsed, list) and parsed:\n",
        "                        label = parsed[0]\n",
        "                    elif isinstance(parsed, str):\n",
        "                        label = parsed.strip()\n",
        "                except json.JSONDecodeError:\n",
        "                    if \"NOT_IPV\" in block.upper():\n",
        "                        label = \"NOT_IPV\"\n",
        "                    elif \"IPV\" in block.upper():\n",
        "                        label = \"IPV\"\n",
        "            else:\n",
        "                if \"NOT_IPV\" in result_text.upper():\n",
        "                    label = \"NOT_IPV\"\n",
        "                elif \"IPV\" in result_text.upper():\n",
        "                    label = \"IPV\"\n",
        "\n",
        "            if label is None:\n",
        "                label = \"UNKNOWN\"\n",
        "\n",
        "            records.append(\n",
        "                {\n",
        "                    \"id\": int(i),\n",
        "                    \"prompt_type\": prompt_type,\n",
        "                    \"extracted_label\": label,\n",
        "                    \"raw_response\": result_text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        output_path = results_dir / f\"binary_{prompt_type}.json\"\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "            json.dump(records, fp, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved results for '{prompt_type}' to {output_path}\")\n",
        "\n",
        "    print(\"All binary prompt tests completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3f75e5e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f75e5e9",
        "outputId": "341bc90b-a58a-44d7-8072-712c0b4d27e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary generation skipped. Set RUN_BINARY = True to execute.\n"
          ]
        }
      ],
      "source": [
        "# Binary generation runner (toggle RUN_BINARY to execute)\n",
        "RUN_BINARY = False\n",
        "BINARY_SAMPLE_COUNT = 5\n",
        "binary_run_dir = BINARY_RESULTS_DIR / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "if RUN_BINARY:\n",
        "    if 'df' not in globals():\n",
        "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
        "    test_binary_prompts(df, path=binary_run_dir, n_samples=BINARY_SAMPLE_COUNT)\n",
        "else:\n",
        "    print(\"Binary generation skipped. Set RUN_BINARY = True to execute.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4db5db",
      "metadata": {
        "id": "8e4db5db"
      },
      "source": [
        "### 3.2 Multitype Prediction Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "528df897",
      "metadata": {
        "id": "528df897"
      },
      "outputs": [],
      "source": [
        "# ---------- Stage 1: Classification ----------\n",
        "def multitype_predict(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, int]:\n",
        "    template = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot)\n",
        "    prompt = template.format(text=sentence, sample_id=sample_id or 0)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
        "    try:\n",
        "        return json.loads(decoded)\n",
        "    except Exception:\n",
        "        import re\n",
        "        match = re.search(r\"\\{.*\\}\", decoded, re.DOTALL)\n",
        "        return json.loads(match.group()) if match else {\"emotional\": 0, \"physical\": 0, \"sexual\": 0}\n",
        "\n",
        "\n",
        "# ---------- Stage 2: Confidence ----------\n",
        "def logprob_confidence(prompt: str, generated_text: str) -> float:\n",
        "    tokens = tokenizer(prompt + generated_text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        log_probs = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
        "    input_ids = tokens[\"input_ids\"][0]\n",
        "\n",
        "    logp_sum = 0.0\n",
        "    count = 0\n",
        "    for idx in range(1, len(input_ids)):\n",
        "        token_id = input_ids[idx]\n",
        "        logp_sum += log_probs[0, idx - 1, token_id].item()\n",
        "        count += 1\n",
        "    avg_logp = logp_sum / max(1, count)\n",
        "    confidence = math.exp(avg_logp)\n",
        "    return float(max(0.0, min(1.0, confidence)))\n",
        "\n",
        "\n",
        "# ---------- Combined Function ----------\n",
        "def multitype_classify(sentence: str, sample_id: Optional[int] = None, prompt_key: str = \"zeroshot\") -> Dict[str, float]:\n",
        "    pred = multitype_predict(sentence, sample_id=sample_id, prompt_key=prompt_key)\n",
        "    classification_prompt = multilabel_prompts.get(prompt_key, prompt_multilabel_zeroshot).format(\n",
        "        text=sentence,\n",
        "        sample_id=sample_id or 0,\n",
        "    )\n",
        "\n",
        "    output_str = json.dumps(pred)\n",
        "    conf = logprob_confidence(classification_prompt, output_str)\n",
        "    pred[\"confidence_score\"] = round(conf, 4)\n",
        "    pred[\"id\"] = sample_id or 0\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "042f8c45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "042f8c45",
        "outputId": "cb67ee27-f5b8-4834-e8dc-e31900a6e33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multitype generation skipped. Set RUN_MULTITPE = True to execute.\n"
          ]
        }
      ],
      "source": [
        "# Multitype generation runner (toggle RUN_MULTITYPE to execute)\n",
        "RUN_MULTITYPE = False\n",
        "MULTITYPE_SAMPLE_COUNT = 5\n",
        "MULTITYPE_PROMPT_KEY = \"zeroshot\"\n",
        "multitype_outputs = []\n",
        "\n",
        "if RUN_MULTITYPE:\n",
        "    if 'df' not in globals():\n",
        "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
        "    subset = df.head(MULTITYPE_SAMPLE_COUNT)\n",
        "    for idx, row in subset.iterrows():\n",
        "        text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
        "        multitype_outputs.append(\n",
        "            multitype_classify(text, sample_id=int(idx), prompt_key=MULTITYPE_PROMPT_KEY)\n",
        "        )\n",
        "    print(f\"Generated {len(multitype_outputs)} multitype predictions using '{MULTITYPE_PROMPT_KEY}'.\")\n",
        "else:\n",
        "    print(\"Multitype generation skipped. Set RUN_MULTITPE = True to execute.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "15918832",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_multitype_prompts(df: pd.DataFrame, path: Path, n_samples: int = 3) -> None:\n",
        "    \"\"\"\n",
        "    Run all multitype (multilabel) prompt types on the first `n_samples` rows\n",
        "    and persist outputs in the same structure as binary predictions.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    df_subset = df.head(n_samples)\n",
        "    results_dir = Path(path)\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Running multitype classification tests...\")\n",
        "    print(f\"Number of samples: {len(df_subset)}\")\n",
        "    print(f\"Results will be saved in: {results_dir}\")\n",
        "\n",
        "    for prompt_type, template in multilabel_prompts.items():\n",
        "        print(f\"Testing prompt type: {prompt_type}\")\n",
        "        records = []\n",
        "\n",
        "        for i, row in df_subset.iterrows():\n",
        "            text = row[\"items\"] if \"items\" in df.columns else str(row.iloc[0])\n",
        "\n",
        "            # Use your existing classification pipeline\n",
        "            result = multitype_classify(\n",
        "                sentence=text,\n",
        "                sample_id=int(i),\n",
        "                prompt_key=prompt_type\n",
        "            )\n",
        "\n",
        "            # Normalize expected output shape\n",
        "            record = {\n",
        "                \"id\": int(i),\n",
        "                \"prompt_type\": prompt_type,\n",
        "                \"emotional\": result.get(\"emotional\", 0),\n",
        "                \"physical\": result.get(\"physical\", 0),\n",
        "                \"sexual\": result.get(\"sexual\", 0),\n",
        "                \"confidence_score\": result.get(\"confidence_score\", None),\n",
        "                \"raw_response\": json.dumps(result, indent=2),\n",
        "            }\n",
        "\n",
        "            records.append(record)\n",
        "\n",
        "        # Save per prompt type\n",
        "        output_path = results_dir / f\"multitype_{prompt_type}.json\"\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "            json.dump(records, fp, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved results for '{prompt_type}' to {output_path}\")\n",
        "\n",
        "    print(\"All multitype prompt tests completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "fa463ed5",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'MULTITYPE_RESULTS_DIR' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m RUN_MULTITYPE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m MULTITYPE_SAMPLE_COUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m618\u001b[39m\n\u001b[0;32m----> 3\u001b[0m multitype_run_dir \u001b[38;5;241m=\u001b[39m \u001b[43mMULTITYPE_RESULTS_DIR\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RUN_MULTITYPE:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MULTITYPE_RESULTS_DIR' is not defined"
          ]
        }
      ],
      "source": [
        "RUN_MULTITYPE = True\n",
        "MULTITYPE_SAMPLE_COUNT = 618\n",
        "multitype_run_dir = MULTITYPE_RESULTS_DIR\n",
        "\n",
        "if RUN_MULTITYPE:\n",
        "    if 'df' not in globals():\n",
        "        raise RuntimeError(\"Dataset `df` not loaded. Run the data prep cell above.\")\n",
        "    test_multitype_prompts(df, path=multitype_run_dir, n_samples=MULTITYPE_SAMPLE_COUNT)\n",
        "else:\n",
        "    print(\"Multitype generation skipped. Set RUN_MULTITYPE = True to execute.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61de86ab",
      "metadata": {
        "id": "61de86ab"
      },
      "source": [
        "## 4. Results\n",
        "Summaries, metrics, and visual diagnostics. Load the JSON artifacts generated above and feed them into the eval pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6f821d43",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/w4\n"
          ]
        }
      ],
      "source": [
        "print(Path.cwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5b2a519a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recent binary result files:\n",
            "- /Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/w4/qwen/binary_zeroshot_v2.json\n"
          ]
        }
      ],
      "source": [
        "# Utility: list recently generated result files\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "def list_result_files(root: Path, pattern: str = \"*.json\", limit: int = 10):\n",
        "    files = sorted(root.rglob(pattern))[-limit:]\n",
        "    if not files:\n",
        "        print(f\"No files found under {root}\")\n",
        "        return \n",
        "    for file in files:\n",
        "        # print(f\"- {file.relative_to(Path.cwd())}\")\n",
        "        print(f\"- {file.resolve()}\")\n",
        "\n",
        "RESULTS_ROOT = Path(\"qwen\")\n",
        "print(\"Recent binary result files:\")\n",
        "list_result_files(RESULTS_ROOT)\n",
        "# print(\"\\nRecent multitype result files:\")\n",
        "# list_result_files(MULTITYPE_RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84c3a4d6",
      "metadata": {
        "id": "84c3a4d6"
      },
      "source": [
        "### 4.1 Import Evaluation Functions\n",
        "Import the evaluation and visualization functions from eval_llm_pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "77bfedea",
      "metadata": {
        "id": "77bfedea"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from eval_llm_pipeline import (\n",
        "    compute_binary_metrics_detailed,\n",
        "    compute_multitype_metrics_per_subgroup,\n",
        "    plot_confusion_matrix,\n",
        "    plot_roc_curve_binary,\n",
        "    plot_precision_recall_curve_binary,\n",
        "    plot_per_class_f1_bar_chart,\n",
        "    append_binary_results_to_json,\n",
        "    append_multitype_results_to_json,\n",
        ")\n",
        "\n",
        "# Paths for JSON results\n",
        "BINARY_JSON_PATH = Path(\"results/binary_results.json\")\n",
        "MULTITYPE_JSON_PATH = Path(\"results/multitype_results.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "368e5d67",
      "metadata": {
        "id": "368e5d67"
      },
      "source": [
        "### 4.2 Binary Evaluation\n",
        "Evaluate binary predictions with detailed metrics and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "814af1a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Paths\n",
        "# =====================================\n",
        "GT_PATH = \"../Dataset/reddit_data.csv\"\n",
        "PLOT_DIR=RESULTS_DIR = RESULTS_ROOT\n",
        "BINARY_RESULTS_JSON = \"results/binary_results.json\"\n",
        "os.makedirs(PLOT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1303e5bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/zeespanto/DevProj/ipvresearch25/1_LLM_Eval/w4\n"
          ]
        }
      ],
      "source": [
        "print(Path.cwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5cK5Z-cnY4sN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cK5Z-cnY4sN",
        "outputId": "7b843cb8-6720-4e0b-d8d7-ce5e5caa7ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found files: ['binary_zeroshot_v2.json']\n",
            "\n",
            "=== Evaluating binary_zeroshot_v2.json (zeroshot_v2) ===\n",
            "{'Accuracy': 0.5275080906148867, 'F1': 0.6906779661016949, 'ROC_AUC': 0.5, 'PR_AUC': 0.5275080906148867}\n",
            "Saved plot → qwen/binary_zeroshot_v2.png\n",
            "\n",
            "All results saved to: ../results/binary_results.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Paths\n",
        "# =====================================\n",
        "GT_PATH = \"../../Dataset/reddit_data.csv\"\n",
        "PLOT_DIR=RESULTS_DIR = \"qwen\"\n",
        "BINARY_RESULTS_JSON = \"../results/binary_results.json\"\n",
        "os.makedirs(PLOT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"qwen2.5-7b\"\n",
        "\n",
        "# =====================================\n",
        "# Helper Functions\n",
        "# =====================================\n",
        "def compute_binary_metrics_detailed(y_true, y_pred):\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"ROC_AUC\": roc_auc_score(y_true, y_pred),\n",
        "        \"PR_AUC\": average_precision_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def append_binary_results_to_json(json_path, model_name, prompt_version, metrics, notes=\"\"):\n",
        "    now_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    entry = {\n",
        "        \"model_name\": model_name,\n",
        "        \"prompt_version\": prompt_version,\n",
        "        \"metrics\": metrics,\n",
        "        \"notes\": notes,\n",
        "        \"date_tested\": now_str,\n",
        "    }\n",
        "\n",
        "    if Path(json_path).exists():\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    else:\n",
        "        data = []\n",
        "\n",
        "    data.append(entry)\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, ax, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    im = ax.imshow(cm, cmap=\"Blues\")\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "\n",
        "    ax.set_xticks([0,1])\n",
        "    ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels([\"NOT_IPV\", \"IPV\"])\n",
        "    ax.set_yticklabels([\"NOT_IPV\", \"IPV\"])\n",
        "\n",
        "    for (i,j), val in np.ndenumerate(cm):\n",
        "        ax.text(j, i, val, ha=\"center\", va=\"center\")\n",
        "\n",
        "\n",
        "def plot_roc_curve_binary(y_true, y_pred, ax, label):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "    ax.plot(fpr, tpr, lw=2, label=label)\n",
        "    ax.plot([0,1], [0,1], \"--\", color=\"gray\")\n",
        "    ax.set_title(\"ROC Curve\")\n",
        "    ax.set_xlabel(\"False Positive Rate\")\n",
        "    ax.set_ylabel(\"True Positive Rate\")\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "def plot_precision_recall_curve_binary(y_true, y_pred, ax, label):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "    ax.plot(recall, precision, lw=2, label=label)\n",
        "    ax.set_title(\"Precision-Recall Curve\")\n",
        "    ax.set_xlabel(\"Recall\")\n",
        "    ax.set_ylabel(\"Precision\")\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Load Ground Truth\n",
        "# =====================================\n",
        "df_gt = pd.read_csv(GT_PATH)\n",
        "df_gt[\"label_true\"] = (\n",
        "    df_gt[[\"Physical Abuse\", \"Emotional Abuse\", \"Sexual Abuse\"]].any(axis=1)\n",
        ").astype(int)\n",
        "df_gt = df_gt.reset_index().rename(columns={\"index\": \"id\"})\n",
        "\n",
        "# =====================================\n",
        "# Process ALL binary JSON files\n",
        "# =====================================\n",
        "binary_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.startswith(\"binary_\") and f.endswith(\".json\")])\n",
        "print(\"Found files:\", binary_files)\n",
        "\n",
        "for file in binary_files:\n",
        "    json_path = os.path.join(RESULTS_DIR, file)\n",
        "\n",
        "    # Extract prompt_type properly\n",
        "    prompt_type = file.replace(\"binary_\", \"\").replace(\".json\", \"\")\n",
        "\n",
        "    print(f\"\\n=== Evaluating {file} ({prompt_type}) ===\")\n",
        "\n",
        "    # Load predictions\n",
        "    with open(json_path, \"r\") as f:\n",
        "        preds = json.load(f)\n",
        "    df_pred = pd.DataFrame(preds)\n",
        "\n",
        "    if \"id\" not in df_pred:\n",
        "        df_pred = df_pred.reset_index().rename(columns={\"index\": \"id\"})\n",
        "\n",
        "    merged = df_gt.merge(df_pred, on=\"id\", how=\"inner\")\n",
        "\n",
        "    merged[\"y_true\"] = merged[\"label_true\"]\n",
        "    merged[\"y_pred\"] = merged[\"extracted_label\"].str.upper().eq(\"IPV\").astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_binary_metrics_detailed(merged[\"y_true\"], merged[\"y_pred\"])\n",
        "    print(metrics)\n",
        "\n",
        "    # Save results (append)\n",
        "    append_binary_results_to_json(\n",
        "        json_path=BINARY_RESULTS_JSON,\n",
        "        model_name=MODEL_NAME,\n",
        "        prompt_version=prompt_type,\n",
        "        metrics=metrics,\n",
        "        notes=f\"Evaluation of {prompt_type} prompt\"\n",
        "    )\n",
        "\n",
        "    # ===== Plotting =====\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    plot_confusion_matrix(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[0],\n",
        "                          title=f\"{prompt_type} — Confusion Matrix\")\n",
        "\n",
        "    plot_roc_curve_binary(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[1],\n",
        "                          label=prompt_type)\n",
        "\n",
        "    plot_precision_recall_curve_binary(merged[\"y_true\"], merged[\"y_pred\"], ax=axes[2],\n",
        "                                       label=prompt_type)\n",
        "\n",
        "    plt.suptitle(f\"Binary — {prompt_type}\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    save_path = os.path.join(PLOT_DIR, f\"binary_{prompt_type}.png\")\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Saved plot → {save_path}\")\n",
        "\n",
        "\n",
        "print(\"\\nAll results saved to:\", BINARY_RESULTS_JSON)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dddb680a",
      "metadata": {
        "id": "dddb680a"
      },
      "source": [
        "### 4.3 Multitype Evaluation\n",
        "Evaluate multitype predictions per subgroup with detailed metrics and visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2e4302",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d2e4302",
        "outputId": "75d88f1f-eede-4689-a8e4-fbc2a21c3547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set MULTITYPE_RESULT_FILE to a valid path to run multitype evaluation.\n"
          ]
        }
      ],
      "source": [
        "# Example: Load multitype predictions and ground truth\n",
        "# Replace with your actual data loading logic\n",
        "# This assumes you have:\n",
        "# - predictions_df: DataFrame with 'id', 'emotional', 'physical', 'sexual' columns\n",
        "# - ground_truth_df: DataFrame with 'id', 'Physical', 'Emotional', 'Sexual' columns\n",
        "# - Optional: subgroup column (e.g., 'gender', 'age_group')\n",
        "from pathlib import Path\n",
        "# Example setup (modify based on your actual data structure):\n",
        "MULTITYPE_RESULT_FILE = \"results/binary_results.json\"  # Set to your multitype result JSON file path\n",
        "MULTITYPE_PROMPT_TYPE = \"zeroshot\"  # Set to the prompt type you want to evaluate\n",
        "MULTITYPE_MODEL_NAME = \"qwen2.5-7b\"\n",
        "MULTITYPE_PROMPT_VERSION = MULTITYPE_PROMPT_TYPE\n",
        "\n",
        "# If you have a result file, load it:\n",
        "if MULTITYPE_RESULT_FILE and Path(MULTITYPE_RESULT_FILE).exists():\n",
        "    with open(MULTITYPE_RESULT_FILE, 'r') as f:\n",
        "        predictions_data = json.load(f)\n",
        "    predictions_df = pd.DataFrame(predictions_data)\n",
        "\n",
        "    # Merge with ground truth\n",
        "    # Assuming df has 'Physical', 'Emotional', 'Sexual' columns (or 'Physical Abuse', etc.)\n",
        "    merged_df = df.merge(predictions_df, on='id', how='inner')\n",
        "\n",
        "    # Map column names if needed\n",
        "    true_cols = []\n",
        "    pred_cols = []\n",
        "    for label in ['Physical', 'Emotional', 'Sexual']:\n",
        "        # Try different possible column names\n",
        "        true_col = None\n",
        "        for col in [label, f\"{label} Abuse\"]:\n",
        "            if col in merged_df.columns:\n",
        "                true_col = col\n",
        "                break\n",
        "        if true_col:\n",
        "            true_cols.append(true_col)\n",
        "            # Prediction columns might be lowercase\n",
        "            pred_col = label.lower() if label.lower() in merged_df.columns else label\n",
        "            if pred_col in merged_df.columns:\n",
        "                pred_cols.append(pred_col)\n",
        "            else:\n",
        "                pred_cols.append(None)\n",
        "\n",
        "    # Filter out None columns\n",
        "    valid_pairs = [(t, p) for t, p in zip(true_cols, pred_cols) if p is not None]\n",
        "    if valid_pairs:\n",
        "        true_cols, pred_cols = zip(*valid_pairs)\n",
        "        true_cols, pred_cols = list(true_cols), list(pred_cols)\n",
        "\n",
        "        # Optional: subgroup column (e.g., 'gender', 'age_group')\n",
        "        subgroup_col = None  # Set to column name if you want per-subgroup metrics\n",
        "\n",
        "        # Compute multitype metrics per subgroup\n",
        "        multitype_metrics = compute_multitype_metrics_per_subgroup(\n",
        "            df=merged_df,\n",
        "            y_true_cols=true_cols,\n",
        "            y_pred_cols=pred_cols,\n",
        "            subgroup_col=subgroup_col\n",
        "        )\n",
        "\n",
        "        # Display metrics table\n",
        "        print(\"=== Multitype Classification Metrics (Per Subgroup) ===\")\n",
        "        for subgroup, classes in multitype_metrics.items():\n",
        "            print(f\"\\n--- Subgroup: {subgroup} ---\")\n",
        "            metrics_list = []\n",
        "            for class_name, metrics in classes.items():\n",
        "                row = {'Class': class_name, **metrics}\n",
        "                metrics_list.append(row)\n",
        "            metrics_table = pd.DataFrame(metrics_list)\n",
        "            print(metrics_table.to_string(index=False))\n",
        "\n",
        "        # Visualizations\n",
        "        # Get overall metrics for visualization (or first subgroup)\n",
        "        overall_key = \"overall\" if \"overall\" in multitype_metrics else list(multitype_metrics.keys())[0]\n",
        "        overall_metrics = multitype_metrics[overall_key]\n",
        "\n",
        "        # Create figure with subplots for confusion matrices\n",
        "        n_classes = len(overall_metrics)\n",
        "        fig, axes = plt.subplots(1, n_classes, figsize=(6 * n_classes, 5))\n",
        "        if n_classes == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        # Confusion matrices for each class\n",
        "        for idx, (class_name, class_metrics) in enumerate(overall_metrics.items()):\n",
        "            # Get true and predicted for this class\n",
        "            y_true_class = merged_df[true_cols[idx]].astype(int).values\n",
        "            y_pred_class = merged_df[pred_cols[idx]].astype(int).values\n",
        "\n",
        "            plot_confusion_matrix(\n",
        "                y_true_class, y_pred_class,\n",
        "                ax=axes[idx],\n",
        "                title=f\"Confusion Matrix - {class_name}\",\n",
        "                xlabel=\"Predicted\",\n",
        "                ylabel=\"Actual\",\n",
        "                class_names=[\"Not Present\", \"Present\"]\n",
        "            )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Per-Class F1 Bar Chart (separate figure)\n",
        "        fig_f1, ax_f1 = plt.subplots(figsize=(8, 5))\n",
        "        plot_per_class_f1_bar_chart(\n",
        "            overall_metrics,\n",
        "            ax=ax_f1,\n",
        "            title=\"Per-Class F1 Scores (Multitype)\",\n",
        "            xlabel=\"Class\",\n",
        "            ylabel=\"F1 Score\"\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Append to JSON\n",
        "        append_multitype_results_to_json(\n",
        "            json_path=MULTITYPE_JSON_PATH,\n",
        "            model_name=MULTITYPE_MODEL_NAME,\n",
        "            prompt_version=MULTITYPE_PROMPT_VERSION,\n",
        "            metrics_per_subgroup=multitype_metrics,\n",
        "            notes=f\"Evaluation of {MULTITYPE_PROMPT_TYPE} prompt\"\n",
        "        )\n",
        "        print(f\"\\nResults appended to {MULTITYPE_JSON_PATH}\")\n",
        "    else:\n",
        "        print(\"Could not find matching true/prediction columns. Check column names.\")\n",
        "else:\n",
        "    print(\"Set MULTITYPE_RESULT_FILE to a valid path to run multitype evaluation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bz4AnhrMeVoC",
      "metadata": {
        "id": "bz4AnhrMeVoC"
      },
      "source": [
        "# SAVE TO GITHUB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pTYsmVH-f2IK",
      "metadata": {
        "id": "pTYsmVH-f2IK"
      },
      "outputs": [],
      "source": [
        "%cd /content/ipvresearch25/\n",
        "!ls -a\n",
        "!git config --global user.email \"zeespanto@gmail.com\"\n",
        "!git config --global user.name \"Zelan Eroz Espanto\"\n",
        "# add authenhtication since I cloned using \"https://github.com/zelaneroz/ipvresearch25\"\n",
        "# need this since git will reject pushes (403) since it's unauthenticated\n",
        "\n",
        "import getpass, os\n",
        "token = getpass.getpass(\"GitHub Token: \")\n",
        "os.environ[\"GITHUB_TOKEN\"] = token\n",
        "!git remote set-url origin https://$GITHUB_TOKEN@github.com/zelaneroz/ipvresearch25.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "defb31a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Verify\n",
        "!git remote -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a40c30",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Stage changes\n",
        "!git add -A\n",
        "#Commit\n",
        "!git commit -m \"w4 qwen zeroshot experiment ran\"\n",
        "#Push\n",
        "!git push origin main"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2647645796aa41a6b0fa63dddd9a39da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35243a2b7e944b4cb96ef74b18caf2ea",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71a90bfa6ad640a9b08c988bb1e1b474",
            "value": 4
          }
        },
        "35243a2b7e944b4cb96ef74b18caf2ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4612038745274cc1a09637108b08680d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a90bfa6ad640a9b08c988bb1e1b474": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71c2f44a3500435d93ff924e76e12b01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774696fc0e09452aa74fd1d7ecc0e631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9569bbc1c1d04d4e9666363c097a5e83",
            "placeholder": "​",
            "style": "IPY_MODEL_c1f39be0116d4a39a6b3f1dbb34899ff",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9569bbc1c1d04d4e9666363c097a5e83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1f39be0116d4a39a6b3f1dbb34899ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d50b5ea7641149fea1b356e7eb0868b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f937ee2499214f7592fe534d5b07900e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50b5ea7641149fea1b356e7eb0868b7",
            "placeholder": "​",
            "style": "IPY_MODEL_4612038745274cc1a09637108b08680d",
            "value": " 4/4 [00:05&lt;00:00,  1.25s/it]"
          }
        },
        "fd31b3a79961493f8e6de35f86a2a896": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_774696fc0e09452aa74fd1d7ecc0e631",
              "IPY_MODEL_2647645796aa41a6b0fa63dddd9a39da",
              "IPY_MODEL_f937ee2499214f7592fe534d5b07900e"
            ],
            "layout": "IPY_MODEL_71c2f44a3500435d93ff924e76e12b01"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
